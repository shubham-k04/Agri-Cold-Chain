{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3313339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c244519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor 1 Temp</th>\n",
       "      <th>Sensor 2 Temp</th>\n",
       "      <th>Timestep</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>308.003540</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>1.48</td>\n",
       "      <td>308.009094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>308.003540</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>1.48</td>\n",
       "      <td>308.009094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>308.003540</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>1.72</td>\n",
       "      <td>308.009094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>308.003540</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.39</td>\n",
       "      <td>308.008850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>308.003540</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>1.25</td>\n",
       "      <td>308.009064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189373</th>\n",
       "      <td>285.617218</td>\n",
       "      <td>289.185272</td>\n",
       "      <td>6720</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>285.797241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189374</th>\n",
       "      <td>285.617218</td>\n",
       "      <td>289.185272</td>\n",
       "      <td>6720</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-4.23</td>\n",
       "      <td>1.87</td>\n",
       "      <td>290.172668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189375</th>\n",
       "      <td>285.617218</td>\n",
       "      <td>289.185272</td>\n",
       "      <td>6720</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>1.79</td>\n",
       "      <td>289.185272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189376</th>\n",
       "      <td>285.617218</td>\n",
       "      <td>289.185272</td>\n",
       "      <td>6720</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-3.80</td>\n",
       "      <td>1.58</td>\n",
       "      <td>289.218384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189377</th>\n",
       "      <td>285.617218</td>\n",
       "      <td>289.185272</td>\n",
       "      <td>6720</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-3.80</td>\n",
       "      <td>1.30</td>\n",
       "      <td>282.999908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189378 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sensor 1 Temp  Sensor 2 Temp  Timestep      X     Y     Z        Temp\n",
       "0          308.003540     308.003479         1 -0.620 -1.32  1.48  308.009094\n",
       "1          308.003540     308.003479         1 -0.930 -0.92  1.48  308.009094\n",
       "2          308.003540     308.003479         1 -0.300 -1.55  1.72  308.009094\n",
       "3          308.003540     308.003479         1 -0.001 -0.70  1.39  308.008850\n",
       "4          308.003540     308.003479         1 -0.720 -0.76  1.25  308.009064\n",
       "...               ...            ...       ...    ...   ...   ...         ...\n",
       "189373     285.617218     289.185272      6720 -0.730 -3.35  1.31  285.797241\n",
       "189374     285.617218     289.185272      6720 -0.620 -4.23  1.87  290.172668\n",
       "189375     285.617218     289.185272      6720 -1.100 -4.17  1.79  289.185272\n",
       "189376     285.617218     289.185272      6720 -1.300 -3.80  1.58  289.218384\n",
       "189377     285.617218     289.185272      6720  0.130 -3.80  1.30  282.999908\n",
       "\n",
       "[189378 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\varun\\\\Dropbox\\\\My PC (DESKTOP-MBMVSID)\\\\Downloads\\\\data_28-10.csv\")\n",
    "len(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "355748f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor 1 Temp</th>\n",
       "      <th>Sensor 2 Temp</th>\n",
       "      <th>Timestep</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>308.00354</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>1.48</td>\n",
       "      <td>308.009094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>308.00354</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>1.48</td>\n",
       "      <td>308.009094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>308.00354</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>1.72</td>\n",
       "      <td>308.009094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>308.00354</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.39</td>\n",
       "      <td>308.008850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>308.00354</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>1.25</td>\n",
       "      <td>308.009064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sensor 1 Temp  Sensor 2 Temp  Timestep      X     Y     Z        Temp\n",
       "0      308.00354     308.003479         1 -0.620 -1.32  1.48  308.009094\n",
       "1      308.00354     308.003479         1 -0.930 -0.92  1.48  308.009094\n",
       "2      308.00354     308.003479         1 -0.300 -1.55  1.72  308.009094\n",
       "3      308.00354     308.003479         1 -0.001 -0.70  1.39  308.008850\n",
       "4      308.00354     308.003479         1 -0.720 -0.76  1.25  308.009064"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.iloc[:,0:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fec0cce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor 1 Temp</th>\n",
       "      <th>Sensor 2 Temp</th>\n",
       "      <th>Timestep</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>308.00354</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>1.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>308.00354</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>1.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>308.00354</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>308.00354</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>308.00354</td>\n",
       "      <td>308.003479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sensor 1 Temp  Sensor 2 Temp  Timestep      X     Y     Z\n",
       "0      308.00354     308.003479         1 -0.620 -1.32  1.48\n",
       "1      308.00354     308.003479         1 -0.930 -0.92  1.48\n",
       "2      308.00354     308.003479         1 -0.300 -1.55  1.72\n",
       "3      308.00354     308.003479         1 -0.001 -0.70  1.39\n",
       "4      308.00354     308.003479         1 -0.720 -0.76  1.25"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "Y = df.iloc[:,-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4c0f6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151502, 6)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_in, X_test_in, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee565ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(X_train_in)\n",
    "\n",
    "X_train = scaler.transform(X_train_in)\n",
    "X_test = scaler.transform(X_test_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fc0131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "scalerfile = 'scaler.sav'                            \n",
    "pickle.dump(scaler, open(scalerfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec0ba2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 128)               896       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,121\n",
      "Trainable params: 17,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=6, activation= \"relu\"))\n",
    "model.add(Dense(64, activation= \"relu\"))\n",
    "model.add(Dense(64, activation= \"relu\"))\n",
    "model.add(Dense(32, activation= \"relu\"))\n",
    "model.add(Dense(32, activation= \"relu\"))\n",
    "model.add(Dense(16, activation= \"relu\"))\n",
    "model.add(Dense(8, activation= \"relu\"))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "opt = Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model.compile(loss='mean_absolute_error', optimizer=opt,metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa0612f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "592/592 [==============================] - 4s 5ms/step - loss: 53.3310 - mae: 53.3310 - val_loss: 5.6180 - val_mae: 5.6180\n",
      "Epoch 2/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 3.9446 - mae: 3.9446 - val_loss: 3.3393 - val_mae: 3.3393\n",
      "Epoch 3/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 3.3155 - mae: 3.3155 - val_loss: 3.0822 - val_mae: 3.0822\n",
      "Epoch 4/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 3.1527 - mae: 3.1527 - val_loss: 3.1693 - val_mae: 3.1693\n",
      "Epoch 5/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.9941 - mae: 2.9941 - val_loss: 2.8860 - val_mae: 2.8860\n",
      "Epoch 6/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 2.8727 - mae: 2.8727 - val_loss: 2.8188 - val_mae: 2.8188\n",
      "Epoch 7/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.6720 - mae: 2.6720 - val_loss: 2.5968 - val_mae: 2.5968\n",
      "Epoch 8/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 2.5828 - mae: 2.5828 - val_loss: 2.5093 - val_mae: 2.5093\n",
      "Epoch 9/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 2.4749 - mae: 2.4749 - val_loss: 2.3723 - val_mae: 2.3723\n",
      "Epoch 10/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.4095 - mae: 2.4095 - val_loss: 2.6198 - val_mae: 2.6198\n",
      "Epoch 11/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.3516 - mae: 2.3516 - val_loss: 2.5567 - val_mae: 2.5567\n",
      "Epoch 12/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 2.3322 - mae: 2.3322 - val_loss: 2.2018 - val_mae: 2.2018\n",
      "Epoch 13/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.2767 - mae: 2.2767 - val_loss: 2.2141 - val_mae: 2.2141\n",
      "Epoch 14/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.2530 - mae: 2.2530 - val_loss: 2.1032 - val_mae: 2.1032\n",
      "Epoch 15/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.2439 - mae: 2.2439 - val_loss: 2.2282 - val_mae: 2.2282\n",
      "Epoch 16/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.2037 - mae: 2.2037 - val_loss: 2.2797 - val_mae: 2.2797\n",
      "Epoch 17/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.1889 - mae: 2.1889 - val_loss: 2.0715 - val_mae: 2.0715\n",
      "Epoch 18/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.1552 - mae: 2.1552 - val_loss: 2.1338 - val_mae: 2.1338\n",
      "Epoch 19/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.1050 - mae: 2.1050 - val_loss: 2.0104 - val_mae: 2.0104\n",
      "Epoch 20/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.1216 - mae: 2.1216 - val_loss: 1.9380 - val_mae: 1.9380\n",
      "Epoch 21/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.0851 - mae: 2.0851 - val_loss: 1.9298 - val_mae: 1.9298\n",
      "Epoch 22/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 2.0573 - mae: 2.0573 - val_loss: 1.8953 - val_mae: 1.8953\n",
      "Epoch 23/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.0334 - mae: 2.0334 - val_loss: 1.9292 - val_mae: 1.9292\n",
      "Epoch 24/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.9852 - mae: 1.9852 - val_loss: 2.0143 - val_mae: 2.0143\n",
      "Epoch 25/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.9106 - mae: 1.9106 - val_loss: 1.7296 - val_mae: 1.7296\n",
      "Epoch 26/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.8726 - mae: 1.8726 - val_loss: 2.0869 - val_mae: 2.0869\n",
      "Epoch 27/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.8339 - mae: 1.8339 - val_loss: 1.9189 - val_mae: 1.9189\n",
      "Epoch 28/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.8311 - mae: 1.8311 - val_loss: 1.7793 - val_mae: 1.7793\n",
      "Epoch 29/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.8369 - mae: 1.8369 - val_loss: 1.6777 - val_mae: 1.6777\n",
      "Epoch 30/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.8146 - mae: 1.8146 - val_loss: 1.6609 - val_mae: 1.6609\n",
      "Epoch 31/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.8238 - mae: 1.8238 - val_loss: 1.6735 - val_mae: 1.6735\n",
      "Epoch 32/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.8081 - mae: 1.8081 - val_loss: 1.7374 - val_mae: 1.7374\n",
      "Epoch 33/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.7960 - mae: 1.7960 - val_loss: 1.8378 - val_mae: 1.8378\n",
      "Epoch 34/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.7642 - mae: 1.7642 - val_loss: 1.8500 - val_mae: 1.8500\n",
      "Epoch 35/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.7368 - mae: 1.7368 - val_loss: 1.5871 - val_mae: 1.5871\n",
      "Epoch 36/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.7532 - mae: 1.7532 - val_loss: 1.8792 - val_mae: 1.8792\n",
      "Epoch 37/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.7503 - mae: 1.7503 - val_loss: 2.0252 - val_mae: 2.0252\n",
      "Epoch 38/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.7487 - mae: 1.7487 - val_loss: 2.0628 - val_mae: 2.0628\n",
      "Epoch 39/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.7543 - mae: 1.7543 - val_loss: 1.6093 - val_mae: 1.6093\n",
      "Epoch 40/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.7243 - mae: 1.7243 - val_loss: 1.5438 - val_mae: 1.5438\n",
      "Epoch 41/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.6894 - mae: 1.6894 - val_loss: 1.7075 - val_mae: 1.7075\n",
      "Epoch 42/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.7186 - mae: 1.7186 - val_loss: 2.0667 - val_mae: 2.0667\n",
      "Epoch 43/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.7051 - mae: 1.7051 - val_loss: 1.5337 - val_mae: 1.5337\n",
      "Epoch 44/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.7306 - mae: 1.7306 - val_loss: 1.5303 - val_mae: 1.5303\n",
      "Epoch 45/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.6825 - mae: 1.6825 - val_loss: 1.8013 - val_mae: 1.8013\n",
      "Epoch 46/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.6867 - mae: 1.6867 - val_loss: 2.0043 - val_mae: 2.0043\n",
      "Epoch 47/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.7226 - mae: 1.7226 - val_loss: 1.7118 - val_mae: 1.7118\n",
      "Epoch 48/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.6839 - mae: 1.6839 - val_loss: 1.7253 - val_mae: 1.7253\n",
      "Epoch 49/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.6537 - mae: 1.6537 - val_loss: 1.7842 - val_mae: 1.7842\n",
      "Epoch 50/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.6481 - mae: 1.6481 - val_loss: 1.9051 - val_mae: 1.9051\n",
      "Epoch 51/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.6705 - mae: 1.6705 - val_loss: 1.4745 - val_mae: 1.4745\n",
      "Epoch 52/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.6336 - mae: 1.6336 - val_loss: 1.4854 - val_mae: 1.4854\n",
      "Epoch 53/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.6572 - mae: 1.6572 - val_loss: 1.5944 - val_mae: 1.5944\n",
      "Epoch 54/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.6146 - mae: 1.6146 - val_loss: 1.9412 - val_mae: 1.9412\n",
      "Epoch 55/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.6424 - mae: 1.6424 - val_loss: 1.4682 - val_mae: 1.4682\n",
      "Epoch 56/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.6060 - mae: 1.6060 - val_loss: 1.8980 - val_mae: 1.8980\n",
      "Epoch 57/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.6204 - mae: 1.6204 - val_loss: 1.5173 - val_mae: 1.5173\n",
      "Epoch 58/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.6476 - mae: 1.6476 - val_loss: 1.5748 - val_mae: 1.5748\n",
      "Epoch 59/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.6072 - mae: 1.6072 - val_loss: 1.7412 - val_mae: 1.7412\n",
      "Epoch 60/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.6150 - mae: 1.6150 - val_loss: 1.6015 - val_mae: 1.6015\n",
      "Epoch 61/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5556 - mae: 1.5556 - val_loss: 1.4111 - val_mae: 1.4111\n",
      "Epoch 62/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.5725 - mae: 1.5725 - val_loss: 1.4237 - val_mae: 1.4237\n",
      "Epoch 63/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.6016 - mae: 1.6016 - val_loss: 1.7260 - val_mae: 1.7260\n",
      "Epoch 64/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.5664 - mae: 1.5664 - val_loss: 1.4888 - val_mae: 1.4888\n",
      "Epoch 65/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5841 - mae: 1.5841 - val_loss: 1.4441 - val_mae: 1.4441\n",
      "Epoch 66/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5627 - mae: 1.5627 - val_loss: 1.9893 - val_mae: 1.9893\n",
      "Epoch 67/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.5382 - mae: 1.5382 - val_loss: 1.4742 - val_mae: 1.4742\n",
      "Epoch 68/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5771 - mae: 1.5771 - val_loss: 1.4167 - val_mae: 1.4167\n",
      "Epoch 69/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5848 - mae: 1.5848 - val_loss: 1.6812 - val_mae: 1.6812\n",
      "Epoch 70/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5016 - mae: 1.5016 - val_loss: 1.4455 - val_mae: 1.4455\n",
      "Epoch 71/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.5255 - mae: 1.5255 - val_loss: 1.7818 - val_mae: 1.7818\n",
      "Epoch 72/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5664 - mae: 1.5664 - val_loss: 1.8689 - val_mae: 1.8689\n",
      "Epoch 73/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.5635 - mae: 1.5635 - val_loss: 1.4012 - val_mae: 1.4012\n",
      "Epoch 74/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.5027 - mae: 1.5027 - val_loss: 1.3769 - val_mae: 1.3769\n",
      "Epoch 75/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.4999 - mae: 1.4999 - val_loss: 1.6339 - val_mae: 1.6339\n",
      "Epoch 76/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.5199 - mae: 1.5199 - val_loss: 1.8041 - val_mae: 1.8041\n",
      "Epoch 77/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.5501 - mae: 1.5501 - val_loss: 1.4689 - val_mae: 1.4689\n",
      "Epoch 78/1500\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 1.4495 - mae: 1.4495 - val_loss: 1.5672 - val_mae: 1.5672\n",
      "Epoch 79/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.4910 - mae: 1.4910 - val_loss: 1.6861 - val_mae: 1.6861\n",
      "Epoch 80/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.5322 - mae: 1.5322 - val_loss: 1.5693 - val_mae: 1.5693\n",
      "Epoch 81/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.4707 - mae: 1.4707 - val_loss: 1.4208 - val_mae: 1.4208\n",
      "Epoch 82/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4928 - mae: 1.4928 - val_loss: 1.4746 - val_mae: 1.4746\n",
      "Epoch 83/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.5115 - mae: 1.5115 - val_loss: 1.3761 - val_mae: 1.3761\n",
      "Epoch 84/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4614 - mae: 1.4614 - val_loss: 1.8380 - val_mae: 1.8380\n",
      "Epoch 85/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4376 - mae: 1.4376 - val_loss: 1.5301 - val_mae: 1.5301\n",
      "Epoch 86/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.5055 - mae: 1.5055 - val_loss: 1.6328 - val_mae: 1.6328\n",
      "Epoch 87/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4452 - mae: 1.4452 - val_loss: 1.8285 - val_mae: 1.8285\n",
      "Epoch 88/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4642 - mae: 1.4642 - val_loss: 1.3008 - val_mae: 1.3008\n",
      "Epoch 89/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.4379 - mae: 1.4379 - val_loss: 1.6814 - val_mae: 1.6814\n",
      "Epoch 90/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4861 - mae: 1.4861 - val_loss: 1.2904 - val_mae: 1.2904\n",
      "Epoch 91/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4539 - mae: 1.4539 - val_loss: 1.4085 - val_mae: 1.4085\n",
      "Epoch 92/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4458 - mae: 1.4458 - val_loss: 1.2817 - val_mae: 1.2817\n",
      "Epoch 93/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4237 - mae: 1.4237 - val_loss: 1.9526 - val_mae: 1.9526\n",
      "Epoch 94/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4161 - mae: 1.4161 - val_loss: 1.7234 - val_mae: 1.7234\n",
      "Epoch 95/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4599 - mae: 1.4599 - val_loss: 1.3377 - val_mae: 1.3377\n",
      "Epoch 96/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4526 - mae: 1.4526 - val_loss: 1.3279 - val_mae: 1.3279\n",
      "Epoch 97/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4359 - mae: 1.4359 - val_loss: 1.6313 - val_mae: 1.6313\n",
      "Epoch 98/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4672 - mae: 1.4672 - val_loss: 1.7593 - val_mae: 1.7593\n",
      "Epoch 99/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4269 - mae: 1.4269 - val_loss: 1.2642 - val_mae: 1.2642\n",
      "Epoch 100/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4271 - mae: 1.4271 - val_loss: 1.4401 - val_mae: 1.4401\n",
      "Epoch 101/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.4122 - mae: 1.4122 - val_loss: 1.6857 - val_mae: 1.6857\n",
      "Epoch 102/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.5445 - mae: 1.5445 - val_loss: 1.2559 - val_mae: 1.2559\n",
      "Epoch 103/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.4446 - mae: 1.4446 - val_loss: 1.6740 - val_mae: 1.6740\n",
      "Epoch 104/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4123 - mae: 1.4123 - val_loss: 1.2677 - val_mae: 1.2677\n",
      "Epoch 105/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.4092 - mae: 1.4092 - val_loss: 1.8747 - val_mae: 1.8747\n",
      "Epoch 106/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3834 - mae: 1.3834 - val_loss: 1.2804 - val_mae: 1.2804\n",
      "Epoch 107/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.4297 - mae: 1.4297 - val_loss: 1.2790 - val_mae: 1.2790\n",
      "Epoch 108/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.3463 - mae: 1.3463 - val_loss: 1.3121 - val_mae: 1.3121\n",
      "Epoch 109/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.4072 - mae: 1.4072 - val_loss: 1.2252 - val_mae: 1.2252\n",
      "Epoch 110/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4019 - mae: 1.4019 - val_loss: 1.2113 - val_mae: 1.2113\n",
      "Epoch 111/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3775 - mae: 1.3775 - val_loss: 1.3870 - val_mae: 1.3870\n",
      "Epoch 112/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.4294 - mae: 1.4294 - val_loss: 1.2598 - val_mae: 1.2598\n",
      "Epoch 113/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3762 - mae: 1.3762 - val_loss: 1.3092 - val_mae: 1.3092\n",
      "Epoch 114/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.4289 - mae: 1.4289 - val_loss: 1.3361 - val_mae: 1.3361\n",
      "Epoch 115/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3685 - mae: 1.3685 - val_loss: 1.3082 - val_mae: 1.3082\n",
      "Epoch 116/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3526 - mae: 1.3526 - val_loss: 1.5313 - val_mae: 1.5313\n",
      "Epoch 117/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3704 - mae: 1.3704 - val_loss: 1.1846 - val_mae: 1.1846\n",
      "Epoch 118/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3860 - mae: 1.3860 - val_loss: 1.1908 - val_mae: 1.1908\n",
      "Epoch 119/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3571 - mae: 1.3571 - val_loss: 1.2183 - val_mae: 1.2183\n",
      "Epoch 120/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.3411 - mae: 1.3411 - val_loss: 1.1832 - val_mae: 1.1832\n",
      "Epoch 121/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.4369 - mae: 1.4369 - val_loss: 1.7187 - val_mae: 1.7187\n",
      "Epoch 122/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3361 - mae: 1.3361 - val_loss: 1.1589 - val_mae: 1.1589\n",
      "Epoch 123/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.3062 - mae: 1.3062 - val_loss: 1.3228 - val_mae: 1.3228\n",
      "Epoch 124/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.3483 - mae: 1.3483 - val_loss: 1.4500 - val_mae: 1.4500\n",
      "Epoch 125/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.3496 - mae: 1.3496 - val_loss: 1.3688 - val_mae: 1.3688\n",
      "Epoch 126/1500\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 1.2961 - mae: 1.2961 - val_loss: 1.1457 - val_mae: 1.1457\n",
      "Epoch 127/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2516 - mae: 1.2516 - val_loss: 1.1487 - val_mae: 1.1487\n",
      "Epoch 128/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3162 - mae: 1.3162 - val_loss: 1.0890 - val_mae: 1.0890\n",
      "Epoch 129/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3103 - mae: 1.3103 - val_loss: 1.1554 - val_mae: 1.1554\n",
      "Epoch 130/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2844 - mae: 1.2844 - val_loss: 1.6142 - val_mae: 1.6142\n",
      "Epoch 131/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.3925 - mae: 1.3925 - val_loss: 1.1079 - val_mae: 1.1079\n",
      "Epoch 132/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2914 - mae: 1.2914 - val_loss: 1.4031 - val_mae: 1.4031\n",
      "Epoch 133/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 1.2276 - mae: 1.2276 - val_loss: 1.2686 - val_mae: 1.2686\n",
      "Epoch 134/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2368 - mae: 1.2368 - val_loss: 1.4022 - val_mae: 1.4022\n",
      "Epoch 135/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 1.2580 - mae: 1.2580 - val_loss: 1.2647 - val_mae: 1.2647\n",
      "Epoch 136/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2689 - mae: 1.2689 - val_loss: 1.0621 - val_mae: 1.0621\n",
      "Epoch 137/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2876 - mae: 1.2876 - val_loss: 1.0709 - val_mae: 1.0709\n",
      "Epoch 138/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2304 - mae: 1.2304 - val_loss: 1.0554 - val_mae: 1.0554\n",
      "Epoch 139/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2468 - mae: 1.2468 - val_loss: 1.3107 - val_mae: 1.3107\n",
      "Epoch 140/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2439 - mae: 1.2439 - val_loss: 1.1497 - val_mae: 1.1497\n",
      "Epoch 141/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2563 - mae: 1.2563 - val_loss: 1.5775 - val_mae: 1.5775\n",
      "Epoch 142/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2244 - mae: 1.2244 - val_loss: 1.0873 - val_mae: 1.0873\n",
      "Epoch 143/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2933 - mae: 1.2933 - val_loss: 1.0801 - val_mae: 1.0801\n",
      "Epoch 144/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2626 - mae: 1.2626 - val_loss: 1.2067 - val_mae: 1.2067\n",
      "Epoch 145/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1497 - mae: 1.1497 - val_loss: 1.1073 - val_mae: 1.1073\n",
      "Epoch 146/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2565 - mae: 1.2565 - val_loss: 1.0314 - val_mae: 1.0314\n",
      "Epoch 147/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1950 - mae: 1.1950 - val_loss: 1.0237 - val_mae: 1.0237\n",
      "Epoch 148/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2383 - mae: 1.2383 - val_loss: 1.9379 - val_mae: 1.9379\n",
      "Epoch 149/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1791 - mae: 1.1791 - val_loss: 1.3869 - val_mae: 1.3869\n",
      "Epoch 150/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1895 - mae: 1.1895 - val_loss: 1.1890 - val_mae: 1.1890\n",
      "Epoch 151/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2202 - mae: 1.2202 - val_loss: 1.4378 - val_mae: 1.4378\n",
      "Epoch 152/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1590 - mae: 1.1590 - val_loss: 1.0987 - val_mae: 1.0987\n",
      "Epoch 153/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.2409 - mae: 1.2409 - val_loss: 1.6943 - val_mae: 1.6943\n",
      "Epoch 154/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1887 - mae: 1.1887 - val_loss: 1.6427 - val_mae: 1.6427\n",
      "Epoch 155/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1455 - mae: 1.1455 - val_loss: 0.9508 - val_mae: 0.9508\n",
      "Epoch 156/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1342 - mae: 1.1342 - val_loss: 1.2873 - val_mae: 1.2873\n",
      "Epoch 157/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1510 - mae: 1.1510 - val_loss: 1.0981 - val_mae: 1.0981\n",
      "Epoch 158/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1530 - mae: 1.1530 - val_loss: 1.2281 - val_mae: 1.2281\n",
      "Epoch 159/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1089 - mae: 1.1089 - val_loss: 1.0781 - val_mae: 1.0781\n",
      "Epoch 160/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1375 - mae: 1.1375 - val_loss: 1.4516 - val_mae: 1.4516\n",
      "Epoch 161/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1539 - mae: 1.1539 - val_loss: 1.2274 - val_mae: 1.2274\n",
      "Epoch 162/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0927 - mae: 1.0927 - val_loss: 1.2097 - val_mae: 1.2097\n",
      "Epoch 163/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1492 - mae: 1.1492 - val_loss: 0.9200 - val_mae: 0.9200\n",
      "Epoch 164/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1282 - mae: 1.1282 - val_loss: 0.9717 - val_mae: 0.9717\n",
      "Epoch 165/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0765 - mae: 1.0765 - val_loss: 1.1354 - val_mae: 1.1354\n",
      "Epoch 166/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1038 - mae: 1.1038 - val_loss: 1.0710 - val_mae: 1.0710\n",
      "Epoch 167/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1110 - mae: 1.1110 - val_loss: 0.9836 - val_mae: 0.9836\n",
      "Epoch 168/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1367 - mae: 1.1367 - val_loss: 0.9611 - val_mae: 0.9611\n",
      "Epoch 169/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0989 - mae: 1.0989 - val_loss: 1.3027 - val_mae: 1.3027\n",
      "Epoch 170/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1166 - mae: 1.1166 - val_loss: 1.5762 - val_mae: 1.5762\n",
      "Epoch 171/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1122 - mae: 1.1122 - val_loss: 1.0478 - val_mae: 1.0478\n",
      "Epoch 172/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0707 - mae: 1.0707 - val_loss: 0.8629 - val_mae: 0.8629\n",
      "Epoch 173/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0678 - mae: 1.0678 - val_loss: 1.2120 - val_mae: 1.2120\n",
      "Epoch 174/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1204 - mae: 1.1204 - val_loss: 0.8724 - val_mae: 0.8724\n",
      "Epoch 175/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1126 - mae: 1.1126 - val_loss: 1.1404 - val_mae: 1.1404\n",
      "Epoch 176/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0492 - mae: 1.0492 - val_loss: 1.0182 - val_mae: 1.0182\n",
      "Epoch 177/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1146 - mae: 1.1146 - val_loss: 1.2906 - val_mae: 1.2906\n",
      "Epoch 178/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0772 - mae: 1.0772 - val_loss: 1.1754 - val_mae: 1.1754\n",
      "Epoch 179/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1751 - mae: 1.1751 - val_loss: 1.4107 - val_mae: 1.4107\n",
      "Epoch 180/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0657 - mae: 1.0657 - val_loss: 1.0001 - val_mae: 1.0001\n",
      "Epoch 181/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0453 - mae: 1.0453 - val_loss: 1.0073 - val_mae: 1.0073\n",
      "Epoch 182/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0664 - mae: 1.0664 - val_loss: 1.3452 - val_mae: 1.3452\n",
      "Epoch 183/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0172 - mae: 1.0172 - val_loss: 0.9429 - val_mae: 0.9429\n",
      "Epoch 184/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1862 - mae: 1.1862 - val_loss: 1.2950 - val_mae: 1.2950\n",
      "Epoch 185/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0507 - mae: 1.0507 - val_loss: 0.8813 - val_mae: 0.8813\n",
      "Epoch 186/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 1.1334 - mae: 1.1334 - val_loss: 1.0082 - val_mae: 1.0082\n",
      "Epoch 187/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0809 - mae: 1.0809 - val_loss: 1.0002 - val_mae: 1.0002\n",
      "Epoch 188/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1631 - mae: 1.1631 - val_loss: 0.8980 - val_mae: 0.8980\n",
      "Epoch 189/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0756 - mae: 1.0756 - val_loss: 0.8687 - val_mae: 0.8687\n",
      "Epoch 190/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0323 - mae: 1.0323 - val_loss: 0.8651 - val_mae: 0.8651\n",
      "Epoch 191/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1213 - mae: 1.1213 - val_loss: 0.8470 - val_mae: 0.8470\n",
      "Epoch 192/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0699 - mae: 1.0699 - val_loss: 0.8290 - val_mae: 0.8290\n",
      "Epoch 193/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.9908 - mae: 0.9908 - val_loss: 0.8781 - val_mae: 0.8781\n",
      "Epoch 194/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1081 - mae: 1.1081 - val_loss: 0.9614 - val_mae: 0.9614\n",
      "Epoch 195/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1772 - mae: 1.1772 - val_loss: 0.8727 - val_mae: 0.8727\n",
      "Epoch 196/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0019 - mae: 1.0019 - val_loss: 1.0472 - val_mae: 1.0472\n",
      "Epoch 197/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0652 - mae: 1.0652 - val_loss: 1.0609 - val_mae: 1.0609\n",
      "Epoch 198/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0469 - mae: 1.0469 - val_loss: 0.8398 - val_mae: 0.8398\n",
      "Epoch 199/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0823 - mae: 1.0823 - val_loss: 1.4480 - val_mae: 1.4480\n",
      "Epoch 200/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0995 - mae: 1.0995 - val_loss: 0.8975 - val_mae: 0.8975\n",
      "Epoch 201/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0279 - mae: 1.0279 - val_loss: 0.8962 - val_mae: 0.8962\n",
      "Epoch 202/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0029 - mae: 1.0029 - val_loss: 0.8786 - val_mae: 0.8786\n",
      "Epoch 203/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1611 - mae: 1.1611 - val_loss: 0.8596 - val_mae: 0.8596\n",
      "Epoch 204/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0411 - mae: 1.0411 - val_loss: 0.8154 - val_mae: 0.8154\n",
      "Epoch 205/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0755 - mae: 1.0755 - val_loss: 0.8261 - val_mae: 0.8261\n",
      "Epoch 206/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.9904 - mae: 0.9904 - val_loss: 0.9128 - val_mae: 0.9128\n",
      "Epoch 207/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0851 - mae: 1.0851 - val_loss: 1.2838 - val_mae: 1.2838\n",
      "Epoch 208/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0763 - mae: 1.0763 - val_loss: 1.1738 - val_mae: 1.1738\n",
      "Epoch 209/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0438 - mae: 1.0438 - val_loss: 1.3164 - val_mae: 1.3164\n",
      "Epoch 210/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0510 - mae: 1.0510 - val_loss: 1.0731 - val_mae: 1.0731\n",
      "Epoch 211/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0116 - mae: 1.0116 - val_loss: 1.1109 - val_mae: 1.1109\n",
      "Epoch 212/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0850 - mae: 1.0850 - val_loss: 0.8121 - val_mae: 0.8121\n",
      "Epoch 213/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0905 - mae: 1.0905 - val_loss: 2.4681 - val_mae: 2.4681\n",
      "Epoch 214/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 1.1833 - mae: 1.1833 - val_loss: 0.8190 - val_mae: 0.8190\n",
      "Epoch 215/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0090 - mae: 1.0090 - val_loss: 0.9018 - val_mae: 0.9018\n",
      "Epoch 216/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0214 - mae: 1.0214 - val_loss: 1.0089 - val_mae: 1.0089\n",
      "Epoch 217/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 1.0021 - mae: 1.0021 - val_loss: 1.2653 - val_mae: 1.2653\n",
      "Epoch 218/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0284 - mae: 1.0284 - val_loss: 1.0619 - val_mae: 1.0619\n",
      "Epoch 219/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0050 - mae: 1.0050 - val_loss: 0.9216 - val_mae: 0.9216\n",
      "Epoch 220/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.1832 - mae: 1.1832 - val_loss: 0.8494 - val_mae: 0.8494\n",
      "Epoch 221/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0291 - mae: 1.0291 - val_loss: 0.8819 - val_mae: 0.8819\n",
      "Epoch 222/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0218 - mae: 1.0218 - val_loss: 1.3407 - val_mae: 1.3407\n",
      "Epoch 223/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0340 - mae: 1.0340 - val_loss: 0.8611 - val_mae: 0.8611\n",
      "Epoch 224/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0346 - mae: 1.0346 - val_loss: 1.3326 - val_mae: 1.3326\n",
      "Epoch 225/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 1.0494 - mae: 1.0494 - val_loss: 0.9027 - val_mae: 0.9027\n",
      "Epoch 226/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0669 - mae: 1.0669 - val_loss: 1.2901 - val_mae: 1.2901\n",
      "Epoch 227/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.9777 - mae: 0.9777 - val_loss: 1.1246 - val_mae: 1.1246\n",
      "Epoch 228/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0212 - mae: 1.0212 - val_loss: 0.8888 - val_mae: 0.8888\n",
      "Epoch 229/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.9791 - mae: 0.9791 - val_loss: 0.9680 - val_mae: 0.9680\n",
      "Epoch 230/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.9821 - mae: 0.9821 - val_loss: 0.9497 - val_mae: 0.9497\n",
      "Epoch 231/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0608 - mae: 1.0608 - val_loss: 0.9106 - val_mae: 0.9106\n",
      "Epoch 232/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0019 - mae: 1.0019 - val_loss: 0.8189 - val_mae: 0.8189\n",
      "Epoch 233/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0683 - mae: 1.0683 - val_loss: 0.9192 - val_mae: 0.9192\n",
      "Epoch 234/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0488 - mae: 1.0488 - val_loss: 0.8321 - val_mae: 0.8321\n",
      "Epoch 235/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.9642 - mae: 0.9642 - val_loss: 0.9947 - val_mae: 0.9947\n",
      "Epoch 236/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0242 - mae: 1.0242 - val_loss: 0.8733 - val_mae: 0.8733\n",
      "Epoch 237/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0273 - mae: 1.0273 - val_loss: 0.8902 - val_mae: 0.8902\n",
      "Epoch 238/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.9815 - mae: 0.9815 - val_loss: 1.4303 - val_mae: 1.4303\n",
      "Epoch 239/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0330 - mae: 1.0330 - val_loss: 1.1640 - val_mae: 1.1640\n",
      "Epoch 240/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0123 - mae: 1.0123 - val_loss: 1.0167 - val_mae: 1.0167\n",
      "Epoch 241/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.9721 - mae: 0.9721 - val_loss: 1.5544 - val_mae: 1.5544\n",
      "Epoch 242/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0404 - mae: 1.0404 - val_loss: 1.0158 - val_mae: 1.0158\n",
      "Epoch 243/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0514 - mae: 1.0514 - val_loss: 0.8946 - val_mae: 0.8946\n",
      "Epoch 244/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.9619 - mae: 0.9619 - val_loss: 0.8447 - val_mae: 0.8447\n",
      "Epoch 245/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.9857 - mae: 0.9857 - val_loss: 1.2066 - val_mae: 1.2066\n",
      "Epoch 246/1500\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 1.0203 - mae: 1.0203 - val_loss: 0.8817 - val_mae: 0.8817\n",
      "Epoch 247/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 0.9786 - mae: 0.9786 - val_loss: 1.4899 - val_mae: 1.4899\n",
      "Epoch 248/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 1.0658 - mae: 1.0658 - val_loss: 1.4802 - val_mae: 1.4802\n",
      "Epoch 249/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 0.9998 - mae: 0.9998 - val_loss: 1.0688 - val_mae: 1.0688\n",
      "Epoch 250/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 1.0138 - mae: 1.0138 - val_loss: 1.5763 - val_mae: 1.5763\n",
      "Epoch 251/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.9878 - mae: 0.9878 - val_loss: 0.8815 - val_mae: 0.8815\n",
      "Epoch 252/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 1.0066 - mae: 1.0066 - val_loss: 1.4890 - val_mae: 1.4890\n",
      "Epoch 253/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0320 - mae: 1.0320 - val_loss: 1.0991 - val_mae: 1.0991\n",
      "Epoch 254/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 0.9883 - mae: 0.9883 - val_loss: 0.7396 - val_mae: 0.7396\n",
      "Epoch 255/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 0.9984 - mae: 0.9984 - val_loss: 0.9575 - val_mae: 0.9575\n",
      "Epoch 256/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 1.1471 - mae: 1.1471 - val_loss: 0.7941 - val_mae: 0.7941\n",
      "Epoch 257/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 0.9388 - mae: 0.9388 - val_loss: 0.9004 - val_mae: 0.9004\n",
      "Epoch 258/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 0.9758 - mae: 0.9758 - val_loss: 0.8117 - val_mae: 0.8117\n",
      "Epoch 259/1500\n",
      "592/592 [==============================] - 3s 6ms/step - loss: 0.9926 - mae: 0.9926 - val_loss: 1.2034 - val_mae: 1.2034\n",
      "Epoch 260/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.9426 - mae: 0.9426 - val_loss: 0.7561 - val_mae: 0.7561\n",
      "Epoch 261/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0038 - mae: 1.0038 - val_loss: 0.8553 - val_mae: 0.8553\n",
      "Epoch 262/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0120 - mae: 1.0120 - val_loss: 1.1446 - val_mae: 1.1446\n",
      "Epoch 263/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0212 - mae: 1.0212 - val_loss: 0.9883 - val_mae: 0.9883\n",
      "Epoch 264/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0024 - mae: 1.0024 - val_loss: 1.1809 - val_mae: 1.1809\n",
      "Epoch 265/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.9630 - mae: 0.9630 - val_loss: 1.4946 - val_mae: 1.4946\n",
      "Epoch 266/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9963 - mae: 0.9963 - val_loss: 1.1489 - val_mae: 1.1489\n",
      "Epoch 267/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0151 - mae: 1.0151 - val_loss: 0.7772 - val_mae: 0.7772\n",
      "Epoch 268/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.9440 - mae: 0.9440 - val_loss: 0.7548 - val_mae: 0.7548\n",
      "Epoch 269/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9532 - mae: 0.9532 - val_loss: 0.7445 - val_mae: 0.7445\n",
      "Epoch 270/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 1.0912 - mae: 1.0912 - val_loss: 0.7277 - val_mae: 0.7277\n",
      "Epoch 271/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 1.0014 - mae: 1.0014 - val_loss: 1.3782 - val_mae: 1.3782\n",
      "Epoch 272/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9566 - mae: 0.9566 - val_loss: 1.0819 - val_mae: 1.0819\n",
      "Epoch 273/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9792 - mae: 0.9792 - val_loss: 0.9457 - val_mae: 0.9457\n",
      "Epoch 274/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0046 - mae: 1.0046 - val_loss: 0.8043 - val_mae: 0.8043\n",
      "Epoch 275/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.9951 - mae: 0.9951 - val_loss: 0.9174 - val_mae: 0.9174\n",
      "Epoch 276/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 1.0434 - mae: 1.0434 - val_loss: 1.0509 - val_mae: 1.0509\n",
      "Epoch 277/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9628 - mae: 0.9628 - val_loss: 0.9878 - val_mae: 0.9878\n",
      "Epoch 278/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9669 - mae: 0.9669 - val_loss: 0.8751 - val_mae: 0.8751\n",
      "Epoch 279/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9738 - mae: 0.9738 - val_loss: 0.7106 - val_mae: 0.7106\n",
      "Epoch 280/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9764 - mae: 0.9764 - val_loss: 1.6052 - val_mae: 1.6052\n",
      "Epoch 281/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9507 - mae: 0.9507 - val_loss: 0.7309 - val_mae: 0.7309\n",
      "Epoch 282/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9559 - mae: 0.9559 - val_loss: 0.7992 - val_mae: 0.7992\n",
      "Epoch 283/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9814 - mae: 0.9814 - val_loss: 0.7565 - val_mae: 0.7565\n",
      "Epoch 284/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 1.0444 - mae: 1.0444 - val_loss: 1.0365 - val_mae: 1.0365\n",
      "Epoch 285/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9541 - mae: 0.9541 - val_loss: 0.7922 - val_mae: 0.7922\n",
      "Epoch 286/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9827 - mae: 0.9827 - val_loss: 1.0472 - val_mae: 1.0472\n",
      "Epoch 287/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9829 - mae: 0.9829 - val_loss: 1.0545 - val_mae: 1.0545\n",
      "Epoch 288/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9705 - mae: 0.9705 - val_loss: 0.7195 - val_mae: 0.7195\n",
      "Epoch 289/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9550 - mae: 0.9550 - val_loss: 0.6878 - val_mae: 0.6878\n",
      "Epoch 290/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.9815 - mae: 0.9815 - val_loss: 1.3952 - val_mae: 1.3952\n",
      "Epoch 291/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9328 - mae: 0.9328 - val_loss: 1.1201 - val_mae: 1.1201\n",
      "Epoch 292/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9575 - mae: 0.9575 - val_loss: 1.2482 - val_mae: 1.2482\n",
      "Epoch 293/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9682 - mae: 0.9682 - val_loss: 0.8941 - val_mae: 0.8941\n",
      "Epoch 294/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 1.0626 - mae: 1.0626 - val_loss: 1.2885 - val_mae: 1.2885\n",
      "Epoch 295/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9369 - mae: 0.9369 - val_loss: 1.1577 - val_mae: 1.1577\n",
      "Epoch 296/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9917 - mae: 0.9917 - val_loss: 0.8911 - val_mae: 0.8911\n",
      "Epoch 297/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9733 - mae: 0.9733 - val_loss: 1.3862 - val_mae: 1.3862\n",
      "Epoch 298/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9336 - mae: 0.9336 - val_loss: 0.7076 - val_mae: 0.7076\n",
      "Epoch 299/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9802 - mae: 0.9802 - val_loss: 1.0007 - val_mae: 1.0007\n",
      "Epoch 300/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9746 - mae: 0.9746 - val_loss: 0.9420 - val_mae: 0.9420\n",
      "Epoch 301/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9640 - mae: 0.9640 - val_loss: 0.7109 - val_mae: 0.7109\n",
      "Epoch 302/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 1.0012 - mae: 1.0012 - val_loss: 0.7688 - val_mae: 0.7688\n",
      "Epoch 303/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9734 - mae: 0.9734 - val_loss: 1.2575 - val_mae: 1.2575\n",
      "Epoch 304/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9351 - mae: 0.9351 - val_loss: 1.0814 - val_mae: 1.0814\n",
      "Epoch 305/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9782 - mae: 0.9782 - val_loss: 0.7796 - val_mae: 0.7796\n",
      "Epoch 306/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9448 - mae: 0.9448 - val_loss: 0.9391 - val_mae: 0.9391\n",
      "Epoch 307/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9386 - mae: 0.9386 - val_loss: 0.6808 - val_mae: 0.6808\n",
      "Epoch 308/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 1.0343 - mae: 1.0343 - val_loss: 0.9767 - val_mae: 0.9767\n",
      "Epoch 309/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9841 - mae: 0.9841 - val_loss: 0.7017 - val_mae: 0.7017\n",
      "Epoch 310/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9848 - mae: 0.9848 - val_loss: 0.7517 - val_mae: 0.7517\n",
      "Epoch 311/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9647 - mae: 0.9647 - val_loss: 1.0395 - val_mae: 1.0395\n",
      "Epoch 312/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9142 - mae: 0.9142 - val_loss: 1.2525 - val_mae: 1.2525\n",
      "Epoch 313/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9565 - mae: 0.9565 - val_loss: 0.7665 - val_mae: 0.7665\n",
      "Epoch 314/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9045 - mae: 0.9045 - val_loss: 0.8950 - val_mae: 0.8950\n",
      "Epoch 315/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9507 - mae: 0.9507 - val_loss: 0.9485 - val_mae: 0.9485\n",
      "Epoch 316/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9889 - mae: 0.9889 - val_loss: 0.7496 - val_mae: 0.7496\n",
      "Epoch 317/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9712 - mae: 0.9712 - val_loss: 0.8847 - val_mae: 0.8847\n",
      "Epoch 318/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9211 - mae: 0.9211 - val_loss: 0.8358 - val_mae: 0.8358\n",
      "Epoch 319/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9729 - mae: 0.9729 - val_loss: 0.9472 - val_mae: 0.9472\n",
      "Epoch 320/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9332 - mae: 0.9332 - val_loss: 1.1669 - val_mae: 1.1669\n",
      "Epoch 321/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9083 - mae: 0.9083 - val_loss: 0.7301 - val_mae: 0.7301\n",
      "Epoch 322/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9829 - mae: 0.9829 - val_loss: 0.8933 - val_mae: 0.8933\n",
      "Epoch 323/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9433 - mae: 0.9433 - val_loss: 0.8766 - val_mae: 0.8766\n",
      "Epoch 324/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9791 - mae: 0.9791 - val_loss: 1.3974 - val_mae: 1.3974\n",
      "Epoch 325/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.8901 - mae: 0.8901 - val_loss: 0.7042 - val_mae: 0.7042\n",
      "Epoch 326/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9698 - mae: 0.9698 - val_loss: 1.0924 - val_mae: 1.0924\n",
      "Epoch 327/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9898 - mae: 0.9898 - val_loss: 0.9179 - val_mae: 0.9179\n",
      "Epoch 328/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9188 - mae: 0.9188 - val_loss: 0.7168 - val_mae: 0.7168\n",
      "Epoch 329/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9005 - mae: 0.9005 - val_loss: 1.0091 - val_mae: 1.0091\n",
      "Epoch 330/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9443 - mae: 0.9443 - val_loss: 0.8770 - val_mae: 0.8770\n",
      "Epoch 331/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9712 - mae: 0.9712 - val_loss: 1.0957 - val_mae: 1.0957\n",
      "Epoch 332/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.8809 - mae: 0.8809 - val_loss: 1.1162 - val_mae: 1.1162\n",
      "Epoch 333/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9770 - mae: 0.9770 - val_loss: 0.7877 - val_mae: 0.7877\n",
      "Epoch 334/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9684 - mae: 0.9684 - val_loss: 1.3146 - val_mae: 1.3146\n",
      "Epoch 335/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9146 - mae: 0.9146 - val_loss: 0.6563 - val_mae: 0.6563\n",
      "Epoch 336/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9914 - mae: 0.9914 - val_loss: 0.8935 - val_mae: 0.8935\n",
      "Epoch 337/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.9883 - mae: 0.9883 - val_loss: 1.0710 - val_mae: 1.0710\n",
      "Epoch 338/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.9108 - mae: 0.9108 - val_loss: 0.8889 - val_mae: 0.8889\n",
      "Epoch 339/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9349 - mae: 0.9349 - val_loss: 1.0862 - val_mae: 1.0862\n",
      "Epoch 340/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.9556 - mae: 0.9556 - val_loss: 0.7080 - val_mae: 0.7080\n",
      "Epoch 341/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9813 - mae: 0.9813 - val_loss: 1.1361 - val_mae: 1.1361\n",
      "Epoch 342/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9293 - mae: 0.9293 - val_loss: 0.9391 - val_mae: 0.9391\n",
      "Epoch 343/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9176 - mae: 0.9176 - val_loss: 0.6588 - val_mae: 0.6588\n",
      "Epoch 344/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9051 - mae: 0.9051 - val_loss: 0.9433 - val_mae: 0.9433\n",
      "Epoch 345/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8841 - mae: 0.8841 - val_loss: 0.8838 - val_mae: 0.8838\n",
      "Epoch 346/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9760 - mae: 0.9760 - val_loss: 1.0271 - val_mae: 1.0271\n",
      "Epoch 347/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9566 - mae: 0.9566 - val_loss: 1.1618 - val_mae: 1.1618\n",
      "Epoch 348/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9338 - mae: 0.9338 - val_loss: 0.7621 - val_mae: 0.7621\n",
      "Epoch 349/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9228 - mae: 0.9228 - val_loss: 1.1763 - val_mae: 1.1763\n",
      "Epoch 350/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.9341 - mae: 0.9341 - val_loss: 0.7686 - val_mae: 0.7686\n",
      "Epoch 351/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.8708 - mae: 0.8708 - val_loss: 0.8374 - val_mae: 0.8374\n",
      "Epoch 352/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9870 - mae: 0.9870 - val_loss: 1.4524 - val_mae: 1.4524\n",
      "Epoch 353/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9605 - mae: 0.9605 - val_loss: 0.6453 - val_mae: 0.6453\n",
      "Epoch 354/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9463 - mae: 0.9463 - val_loss: 0.6592 - val_mae: 0.6592\n",
      "Epoch 355/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8749 - mae: 0.8749 - val_loss: 0.8687 - val_mae: 0.8687\n",
      "Epoch 356/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9150 - mae: 0.9150 - val_loss: 1.6717 - val_mae: 1.6717\n",
      "Epoch 357/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9586 - mae: 0.9586 - val_loss: 0.7951 - val_mae: 0.7951\n",
      "Epoch 358/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9621 - mae: 0.9621 - val_loss: 0.6421 - val_mae: 0.6421\n",
      "Epoch 359/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.8603 - mae: 0.8603 - val_loss: 0.7128 - val_mae: 0.7128\n",
      "Epoch 360/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.9848 - mae: 0.9848 - val_loss: 2.3566 - val_mae: 2.3566\n",
      "Epoch 361/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9839 - mae: 0.9839 - val_loss: 0.9932 - val_mae: 0.9932\n",
      "Epoch 362/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9037 - mae: 0.9037 - val_loss: 1.2448 - val_mae: 1.2448\n",
      "Epoch 363/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9826 - mae: 0.9826 - val_loss: 0.8998 - val_mae: 0.8998\n",
      "Epoch 364/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8572 - mae: 0.8572 - val_loss: 0.6888 - val_mae: 0.6888\n",
      "Epoch 365/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9413 - mae: 0.9413 - val_loss: 1.2611 - val_mae: 1.2611\n",
      "Epoch 366/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9463 - mae: 0.9463 - val_loss: 0.7293 - val_mae: 0.7293\n",
      "Epoch 367/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8340 - mae: 0.8340 - val_loss: 0.7318 - val_mae: 0.7318\n",
      "Epoch 368/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9364 - mae: 0.9364 - val_loss: 0.9348 - val_mae: 0.9348\n",
      "Epoch 369/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8750 - mae: 0.8750 - val_loss: 0.6964 - val_mae: 0.6964\n",
      "Epoch 370/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9080 - mae: 0.9080 - val_loss: 0.6151 - val_mae: 0.6151\n",
      "Epoch 371/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9493 - mae: 0.9493 - val_loss: 1.0772 - val_mae: 1.0772\n",
      "Epoch 372/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9631 - mae: 0.9631 - val_loss: 1.1543 - val_mae: 1.1543\n",
      "Epoch 373/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8660 - mae: 0.8660 - val_loss: 0.8488 - val_mae: 0.8488\n",
      "Epoch 374/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.9472 - mae: 0.9472 - val_loss: 0.9151 - val_mae: 0.9151\n",
      "Epoch 375/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9979 - mae: 0.9979 - val_loss: 0.6434 - val_mae: 0.6434\n",
      "Epoch 376/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9038 - mae: 0.9038 - val_loss: 1.1578 - val_mae: 1.1578\n",
      "Epoch 377/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9356 - mae: 0.9356 - val_loss: 1.2837 - val_mae: 1.2837\n",
      "Epoch 378/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9403 - mae: 0.9403 - val_loss: 0.6638 - val_mae: 0.6638\n",
      "Epoch 379/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8922 - mae: 0.8922 - val_loss: 1.5475 - val_mae: 1.5475\n",
      "Epoch 380/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.9479 - mae: 0.9479 - val_loss: 1.3711 - val_mae: 1.3711\n",
      "Epoch 381/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8809 - mae: 0.8809 - val_loss: 1.0702 - val_mae: 1.0702\n",
      "Epoch 382/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9177 - mae: 0.9177 - val_loss: 0.6406 - val_mae: 0.6406\n",
      "Epoch 383/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9222 - mae: 0.9222 - val_loss: 0.6815 - val_mae: 0.6815\n",
      "Epoch 384/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8872 - mae: 0.8872 - val_loss: 0.6567 - val_mae: 0.6567\n",
      "Epoch 385/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9245 - mae: 0.9245 - val_loss: 1.1403 - val_mae: 1.1403\n",
      "Epoch 386/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9250 - mae: 0.9250 - val_loss: 0.6581 - val_mae: 0.6581\n",
      "Epoch 387/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9438 - mae: 0.9438 - val_loss: 0.7078 - val_mae: 0.7078\n",
      "Epoch 388/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8806 - mae: 0.8806 - val_loss: 0.7318 - val_mae: 0.7318\n",
      "Epoch 389/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8685 - mae: 0.8685 - val_loss: 2.2200 - val_mae: 2.2200\n",
      "Epoch 390/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9330 - mae: 0.9330 - val_loss: 1.1069 - val_mae: 1.1069\n",
      "Epoch 391/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9425 - mae: 0.9425 - val_loss: 1.3397 - val_mae: 1.3397\n",
      "Epoch 392/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9234 - mae: 0.9234 - val_loss: 0.8916 - val_mae: 0.8916\n",
      "Epoch 393/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8819 - mae: 0.8819 - val_loss: 1.2201 - val_mae: 1.2201\n",
      "Epoch 394/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9714 - mae: 0.9714 - val_loss: 0.8298 - val_mae: 0.8298\n",
      "Epoch 395/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8670 - mae: 0.8670 - val_loss: 1.1080 - val_mae: 1.1080\n",
      "Epoch 396/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8860 - mae: 0.8860 - val_loss: 0.9682 - val_mae: 0.9682\n",
      "Epoch 397/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9215 - mae: 0.9215 - val_loss: 1.0566 - val_mae: 1.0566\n",
      "Epoch 398/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8448 - mae: 0.8448 - val_loss: 1.6101 - val_mae: 1.6101\n",
      "Epoch 399/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.8670 - mae: 0.8670 - val_loss: 0.7356 - val_mae: 0.7356\n",
      "Epoch 400/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9431 - mae: 0.9431 - val_loss: 1.0560 - val_mae: 1.0560\n",
      "Epoch 401/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9164 - mae: 0.9164 - val_loss: 1.1392 - val_mae: 1.1392\n",
      "Epoch 402/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9948 - mae: 0.9948 - val_loss: 0.6994 - val_mae: 0.6994\n",
      "Epoch 403/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9219 - mae: 0.9219 - val_loss: 0.6556 - val_mae: 0.6556\n",
      "Epoch 404/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9126 - mae: 0.9126 - val_loss: 1.0059 - val_mae: 1.0059\n",
      "Epoch 405/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9254 - mae: 0.9254 - val_loss: 0.6699 - val_mae: 0.6699\n",
      "Epoch 406/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9914 - mae: 0.9914 - val_loss: 0.9484 - val_mae: 0.9484\n",
      "Epoch 407/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9726 - mae: 0.9726 - val_loss: 1.1391 - val_mae: 1.1391\n",
      "Epoch 408/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9308 - mae: 0.9308 - val_loss: 0.8536 - val_mae: 0.8536\n",
      "Epoch 409/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8747 - mae: 0.8747 - val_loss: 0.9855 - val_mae: 0.9855\n",
      "Epoch 410/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9708 - mae: 0.9708 - val_loss: 0.6215 - val_mae: 0.6215\n",
      "Epoch 411/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8358 - mae: 0.8358 - val_loss: 0.8457 - val_mae: 0.8457\n",
      "Epoch 412/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9376 - mae: 0.9376 - val_loss: 0.7093 - val_mae: 0.7093\n",
      "Epoch 413/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 1.0277 - mae: 1.0277 - val_loss: 0.6177 - val_mae: 0.6177\n",
      "Epoch 414/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9168 - mae: 0.9168 - val_loss: 0.8842 - val_mae: 0.8842\n",
      "Epoch 415/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8934 - mae: 0.8934 - val_loss: 0.9445 - val_mae: 0.9445\n",
      "Epoch 416/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8459 - mae: 0.8459 - val_loss: 0.9118 - val_mae: 0.9118\n",
      "Epoch 417/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9234 - mae: 0.9234 - val_loss: 0.8438 - val_mae: 0.8438\n",
      "Epoch 418/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9241 - mae: 0.9241 - val_loss: 0.9899 - val_mae: 0.9899\n",
      "Epoch 419/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8736 - mae: 0.8736 - val_loss: 1.1371 - val_mae: 1.1371\n",
      "Epoch 420/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8909 - mae: 0.8909 - val_loss: 0.6100 - val_mae: 0.6100\n",
      "Epoch 421/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9038 - mae: 0.9038 - val_loss: 1.1247 - val_mae: 1.1247\n",
      "Epoch 422/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9019 - mae: 0.9019 - val_loss: 1.2368 - val_mae: 1.2368\n",
      "Epoch 423/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9527 - mae: 0.9527 - val_loss: 0.8919 - val_mae: 0.8919\n",
      "Epoch 424/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.9133 - mae: 0.9133 - val_loss: 0.8771 - val_mae: 0.8771\n",
      "Epoch 425/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8841 - mae: 0.8841 - val_loss: 0.9646 - val_mae: 0.9646\n",
      "Epoch 426/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.8289 - mae: 0.8289 - val_loss: 0.9955 - val_mae: 0.9955\n",
      "Epoch 427/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8627 - mae: 0.8627 - val_loss: 1.0078 - val_mae: 1.0078\n",
      "Epoch 428/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9744 - mae: 0.9744 - val_loss: 1.0353 - val_mae: 1.0353\n",
      "Epoch 429/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.8210 - mae: 0.8210 - val_loss: 0.6435 - val_mae: 0.6435\n",
      "Epoch 430/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9013 - mae: 0.9013 - val_loss: 0.6593 - val_mae: 0.6593\n",
      "Epoch 431/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8473 - mae: 0.8473 - val_loss: 1.2799 - val_mae: 1.2799\n",
      "Epoch 432/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9066 - mae: 0.9066 - val_loss: 0.9724 - val_mae: 0.9724\n",
      "Epoch 433/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9595 - mae: 0.9595 - val_loss: 1.0778 - val_mae: 1.0778\n",
      "Epoch 434/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.9179 - mae: 0.9179 - val_loss: 0.6254 - val_mae: 0.6254\n",
      "Epoch 435/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9139 - mae: 0.9139 - val_loss: 0.8965 - val_mae: 0.8965\n",
      "Epoch 436/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.8195 - mae: 0.8195 - val_loss: 0.6233 - val_mae: 0.6233\n",
      "Epoch 437/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 1.0384 - mae: 1.0384 - val_loss: 1.1812 - val_mae: 1.1812\n",
      "Epoch 438/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8636 - mae: 0.8636 - val_loss: 0.6026 - val_mae: 0.6026\n",
      "Epoch 439/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.8625 - mae: 0.8625 - val_loss: 0.7015 - val_mae: 0.7015\n",
      "Epoch 440/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.8785 - mae: 0.8785 - val_loss: 1.1290 - val_mae: 1.1290\n",
      "Epoch 441/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.8762 - mae: 0.8762 - val_loss: 1.7734 - val_mae: 1.7734\n",
      "Epoch 442/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9109 - mae: 0.9109 - val_loss: 0.9696 - val_mae: 0.9696\n",
      "Epoch 443/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8860 - mae: 0.8860 - val_loss: 1.6792 - val_mae: 1.6792\n",
      "Epoch 444/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9388 - mae: 0.9388 - val_loss: 0.5759 - val_mae: 0.5759\n",
      "Epoch 445/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7893 - mae: 0.7893 - val_loss: 0.8724 - val_mae: 0.8724\n",
      "Epoch 446/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.8435 - mae: 0.8435 - val_loss: 0.8027 - val_mae: 0.8027\n",
      "Epoch 447/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.9472 - mae: 0.9472 - val_loss: 1.0534 - val_mae: 1.0534\n",
      "Epoch 448/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8174 - mae: 0.8174 - val_loss: 0.7063 - val_mae: 0.7063\n",
      "Epoch 449/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8721 - mae: 0.8721 - val_loss: 0.6616 - val_mae: 0.6616\n",
      "Epoch 450/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9008 - mae: 0.9008 - val_loss: 1.1666 - val_mae: 1.1666\n",
      "Epoch 451/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9047 - mae: 0.9047 - val_loss: 1.1182 - val_mae: 1.1182\n",
      "Epoch 452/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.8430 - mae: 0.8430 - val_loss: 0.6975 - val_mae: 0.6975\n",
      "Epoch 453/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8722 - mae: 0.8722 - val_loss: 0.5863 - val_mae: 0.5863\n",
      "Epoch 454/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8659 - mae: 0.8659 - val_loss: 0.7942 - val_mae: 0.7942\n",
      "Epoch 455/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8454 - mae: 0.8454 - val_loss: 1.1275 - val_mae: 1.1275\n",
      "Epoch 456/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9775 - mae: 0.9775 - val_loss: 0.6104 - val_mae: 0.6104\n",
      "Epoch 457/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8944 - mae: 0.8944 - val_loss: 0.8240 - val_mae: 0.8240\n",
      "Epoch 458/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8014 - mae: 0.8014 - val_loss: 0.9462 - val_mae: 0.9462\n",
      "Epoch 459/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9109 - mae: 0.9109 - val_loss: 0.7446 - val_mae: 0.7446\n",
      "Epoch 460/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8391 - mae: 0.8391 - val_loss: 0.7305 - val_mae: 0.7305\n",
      "Epoch 461/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8934 - mae: 0.8934 - val_loss: 1.2165 - val_mae: 1.2165\n",
      "Epoch 462/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 1.0010 - mae: 1.0010 - val_loss: 0.5891 - val_mae: 0.5891\n",
      "Epoch 463/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8610 - mae: 0.8610 - val_loss: 0.6233 - val_mae: 0.6233\n",
      "Epoch 464/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8008 - mae: 0.8008 - val_loss: 1.0198 - val_mae: 1.0198\n",
      "Epoch 465/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9030 - mae: 0.9030 - val_loss: 0.8552 - val_mae: 0.8552\n",
      "Epoch 466/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8669 - mae: 0.8669 - val_loss: 0.8083 - val_mae: 0.8083\n",
      "Epoch 467/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8471 - mae: 0.8471 - val_loss: 0.6799 - val_mae: 0.6799\n",
      "Epoch 468/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8145 - mae: 0.8145 - val_loss: 0.5862 - val_mae: 0.5862\n",
      "Epoch 469/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9337 - mae: 0.9337 - val_loss: 0.5844 - val_mae: 0.5844\n",
      "Epoch 470/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8136 - mae: 0.8136 - val_loss: 0.8595 - val_mae: 0.8595\n",
      "Epoch 471/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 1.0131 - mae: 1.0131 - val_loss: 1.2625 - val_mae: 1.2625\n",
      "Epoch 472/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7970 - mae: 0.7970 - val_loss: 1.3952 - val_mae: 1.3952\n",
      "Epoch 473/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8716 - mae: 0.8716 - val_loss: 0.5993 - val_mae: 0.5993\n",
      "Epoch 474/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8027 - mae: 0.8027 - val_loss: 0.5900 - val_mae: 0.5900\n",
      "Epoch 475/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8724 - mae: 0.8724 - val_loss: 0.8096 - val_mae: 0.8096\n",
      "Epoch 476/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9124 - mae: 0.9124 - val_loss: 1.0374 - val_mae: 1.0374\n",
      "Epoch 477/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8318 - mae: 0.8318 - val_loss: 0.6300 - val_mae: 0.6300\n",
      "Epoch 478/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8294 - mae: 0.8294 - val_loss: 0.6016 - val_mae: 0.6016\n",
      "Epoch 479/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8728 - mae: 0.8728 - val_loss: 0.6701 - val_mae: 0.6701\n",
      "Epoch 480/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8510 - mae: 0.8510 - val_loss: 0.8730 - val_mae: 0.8730\n",
      "Epoch 481/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7682 - mae: 0.7682 - val_loss: 0.6126 - val_mae: 0.6126\n",
      "Epoch 482/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.8256 - mae: 0.8256 - val_loss: 0.5658 - val_mae: 0.5658\n",
      "Epoch 483/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.8589 - mae: 0.8589 - val_loss: 0.9044 - val_mae: 0.9044\n",
      "Epoch 484/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8740 - mae: 0.8740 - val_loss: 0.6909 - val_mae: 0.6909\n",
      "Epoch 485/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.8166 - mae: 0.8166 - val_loss: 0.9308 - val_mae: 0.9308\n",
      "Epoch 486/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.8745 - mae: 0.8745 - val_loss: 0.6762 - val_mae: 0.6762\n",
      "Epoch 487/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.8316 - mae: 0.8316 - val_loss: 0.5336 - val_mae: 0.5336\n",
      "Epoch 488/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9562 - mae: 0.9562 - val_loss: 0.8883 - val_mae: 0.8883\n",
      "Epoch 489/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.8391 - mae: 0.8391 - val_loss: 0.6714 - val_mae: 0.6714\n",
      "Epoch 490/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8235 - mae: 0.8235 - val_loss: 1.0808 - val_mae: 1.0808\n",
      "Epoch 491/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8107 - mae: 0.8107 - val_loss: 0.5478 - val_mae: 0.5478\n",
      "Epoch 492/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9109 - mae: 0.9109 - val_loss: 0.6844 - val_mae: 0.6844\n",
      "Epoch 493/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9551 - mae: 0.9551 - val_loss: 0.7831 - val_mae: 0.7831\n",
      "Epoch 494/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8812 - mae: 0.8812 - val_loss: 0.7695 - val_mae: 0.7695\n",
      "Epoch 495/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.8272 - mae: 0.8272 - val_loss: 0.5451 - val_mae: 0.5451\n",
      "Epoch 496/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.9067 - mae: 0.9067 - val_loss: 0.5947 - val_mae: 0.5947\n",
      "Epoch 497/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8644 - mae: 0.8644 - val_loss: 0.9263 - val_mae: 0.9263\n",
      "Epoch 498/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8862 - mae: 0.8862 - val_loss: 0.8070 - val_mae: 0.8070\n",
      "Epoch 499/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8221 - mae: 0.8221 - val_loss: 0.5943 - val_mae: 0.5943\n",
      "Epoch 500/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.7986 - mae: 0.7986 - val_loss: 0.5593 - val_mae: 0.5593\n",
      "Epoch 501/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7786 - mae: 0.7786 - val_loss: 1.1288 - val_mae: 1.1288\n",
      "Epoch 502/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9292 - mae: 0.9292 - val_loss: 0.5506 - val_mae: 0.5506\n",
      "Epoch 503/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8277 - mae: 0.8277 - val_loss: 0.5320 - val_mae: 0.5320\n",
      "Epoch 504/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9453 - mae: 0.9453 - val_loss: 0.6298 - val_mae: 0.6298\n",
      "Epoch 505/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8039 - mae: 0.8039 - val_loss: 0.7252 - val_mae: 0.7252\n",
      "Epoch 506/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8330 - mae: 0.8330 - val_loss: 0.5977 - val_mae: 0.5977\n",
      "Epoch 507/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9418 - mae: 0.9418 - val_loss: 0.6883 - val_mae: 0.6883\n",
      "Epoch 508/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.8658 - mae: 0.8658 - val_loss: 0.7093 - val_mae: 0.7093\n",
      "Epoch 509/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8666 - mae: 0.8666 - val_loss: 2.3665 - val_mae: 2.3665\n",
      "Epoch 510/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.8258 - mae: 0.8258 - val_loss: 0.5431 - val_mae: 0.5431\n",
      "Epoch 511/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.8620 - mae: 0.8620 - val_loss: 0.8641 - val_mae: 0.8641\n",
      "Epoch 512/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7927 - mae: 0.7927 - val_loss: 0.5452 - val_mae: 0.5452\n",
      "Epoch 513/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.8212 - mae: 0.8212 - val_loss: 0.8384 - val_mae: 0.8384\n",
      "Epoch 514/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8385 - mae: 0.8385 - val_loss: 0.5477 - val_mae: 0.5477\n",
      "Epoch 515/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8233 - mae: 0.8233 - val_loss: 1.1830 - val_mae: 1.1830\n",
      "Epoch 516/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7930 - mae: 0.7930 - val_loss: 0.6910 - val_mae: 0.6910\n",
      "Epoch 517/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8487 - mae: 0.8487 - val_loss: 0.5708 - val_mae: 0.5708\n",
      "Epoch 518/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8201 - mae: 0.8201 - val_loss: 0.5549 - val_mae: 0.5549\n",
      "Epoch 519/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8684 - mae: 0.8684 - val_loss: 0.5626 - val_mae: 0.5626\n",
      "Epoch 520/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7624 - mae: 0.7624 - val_loss: 0.8630 - val_mae: 0.8630\n",
      "Epoch 521/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9383 - mae: 0.9383 - val_loss: 0.9960 - val_mae: 0.9960\n",
      "Epoch 522/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7694 - mae: 0.7694 - val_loss: 0.9300 - val_mae: 0.9300\n",
      "Epoch 523/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7968 - mae: 0.7968 - val_loss: 0.7407 - val_mae: 0.7407\n",
      "Epoch 524/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8163 - mae: 0.8163 - val_loss: 0.5641 - val_mae: 0.5641\n",
      "Epoch 525/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.9277 - mae: 0.9277 - val_loss: 1.2443 - val_mae: 1.2443\n",
      "Epoch 526/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8281 - mae: 0.8281 - val_loss: 1.2687 - val_mae: 1.2687\n",
      "Epoch 527/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9140 - mae: 0.9140 - val_loss: 0.5323 - val_mae: 0.5323\n",
      "Epoch 528/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7949 - mae: 0.7949 - val_loss: 0.5509 - val_mae: 0.5509\n",
      "Epoch 529/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8239 - mae: 0.8239 - val_loss: 0.8977 - val_mae: 0.8977\n",
      "Epoch 530/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.8515 - mae: 0.8515 - val_loss: 1.3907 - val_mae: 1.3907\n",
      "Epoch 531/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.8387 - mae: 0.8387 - val_loss: 0.8550 - val_mae: 0.8550\n",
      "Epoch 532/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8068 - mae: 0.8068 - val_loss: 1.3060 - val_mae: 1.3060\n",
      "Epoch 533/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8117 - mae: 0.8117 - val_loss: 0.5803 - val_mae: 0.5803\n",
      "Epoch 534/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.7972 - mae: 0.7972 - val_loss: 0.7068 - val_mae: 0.7068\n",
      "Epoch 535/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7798 - mae: 0.7798 - val_loss: 0.5413 - val_mae: 0.5413\n",
      "Epoch 536/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9566 - mae: 0.9566 - val_loss: 1.4373 - val_mae: 1.4373\n",
      "Epoch 537/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7853 - mae: 0.7853 - val_loss: 0.9210 - val_mae: 0.9210\n",
      "Epoch 538/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.7685 - mae: 0.7685 - val_loss: 0.7817 - val_mae: 0.7817\n",
      "Epoch 539/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.8637 - mae: 0.8637 - val_loss: 0.6951 - val_mae: 0.6951\n",
      "Epoch 540/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.8499 - mae: 0.8499 - val_loss: 0.8005 - val_mae: 0.8005\n",
      "Epoch 541/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7954 - mae: 0.7954 - val_loss: 1.0713 - val_mae: 1.0713\n",
      "Epoch 542/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7764 - mae: 0.7764 - val_loss: 0.7155 - val_mae: 0.7155\n",
      "Epoch 543/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8321 - mae: 0.8321 - val_loss: 0.7714 - val_mae: 0.7714\n",
      "Epoch 544/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7900 - mae: 0.7900 - val_loss: 0.7667 - val_mae: 0.7667\n",
      "Epoch 545/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9683 - mae: 0.9683 - val_loss: 1.7996 - val_mae: 1.7996\n",
      "Epoch 546/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8015 - mae: 0.8015 - val_loss: 0.6284 - val_mae: 0.6284\n",
      "Epoch 547/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8710 - mae: 0.8710 - val_loss: 0.5057 - val_mae: 0.5057\n",
      "Epoch 548/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.8931 - mae: 0.8931 - val_loss: 0.6693 - val_mae: 0.6693\n",
      "Epoch 549/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7882 - mae: 0.7882 - val_loss: 0.9280 - val_mae: 0.9280\n",
      "Epoch 550/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7939 - mae: 0.7939 - val_loss: 1.3803 - val_mae: 1.3803\n",
      "Epoch 551/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7441 - mae: 0.7441 - val_loss: 0.6680 - val_mae: 0.6680\n",
      "Epoch 552/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8245 - mae: 0.8245 - val_loss: 0.9435 - val_mae: 0.9435\n",
      "Epoch 553/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7664 - mae: 0.7664 - val_loss: 0.5911 - val_mae: 0.5911\n",
      "Epoch 554/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9568 - mae: 0.9568 - val_loss: 0.5356 - val_mae: 0.5356\n",
      "Epoch 555/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8277 - mae: 0.8277 - val_loss: 0.6418 - val_mae: 0.6418\n",
      "Epoch 556/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9767 - mae: 0.9767 - val_loss: 0.6780 - val_mae: 0.6780\n",
      "Epoch 557/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7398 - mae: 0.7398 - val_loss: 0.5646 - val_mae: 0.5646\n",
      "Epoch 558/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9175 - mae: 0.9175 - val_loss: 0.5094 - val_mae: 0.5094\n",
      "Epoch 559/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7666 - mae: 0.7666 - val_loss: 0.8850 - val_mae: 0.8850\n",
      "Epoch 560/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.7956 - mae: 0.7956 - val_loss: 0.5506 - val_mae: 0.5506\n",
      "Epoch 561/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8202 - mae: 0.8202 - val_loss: 0.9419 - val_mae: 0.9419\n",
      "Epoch 562/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7999 - mae: 0.7999 - val_loss: 0.5268 - val_mae: 0.5268\n",
      "Epoch 563/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8723 - mae: 0.8723 - val_loss: 0.7323 - val_mae: 0.7323\n",
      "Epoch 564/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7447 - mae: 0.7447 - val_loss: 0.5041 - val_mae: 0.5041\n",
      "Epoch 565/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7992 - mae: 0.7992 - val_loss: 0.9337 - val_mae: 0.9337\n",
      "Epoch 566/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8230 - mae: 0.8230 - val_loss: 0.5570 - val_mae: 0.5570\n",
      "Epoch 567/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8032 - mae: 0.8032 - val_loss: 0.6158 - val_mae: 0.6158\n",
      "Epoch 568/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8123 - mae: 0.8123 - val_loss: 0.6061 - val_mae: 0.6061\n",
      "Epoch 569/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7492 - mae: 0.7492 - val_loss: 0.9138 - val_mae: 0.9138\n",
      "Epoch 570/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9041 - mae: 0.9041 - val_loss: 0.8059 - val_mae: 0.8059\n",
      "Epoch 571/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7742 - mae: 0.7742 - val_loss: 0.6021 - val_mae: 0.6021\n",
      "Epoch 572/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.8612 - mae: 0.8612 - val_loss: 0.8782 - val_mae: 0.8782\n",
      "Epoch 573/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8061 - mae: 0.8061 - val_loss: 1.1671 - val_mae: 1.1671\n",
      "Epoch 574/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8004 - mae: 0.8004 - val_loss: 0.9729 - val_mae: 0.9729\n",
      "Epoch 575/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7857 - mae: 0.7857 - val_loss: 0.5076 - val_mae: 0.5076\n",
      "Epoch 576/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7967 - mae: 0.7967 - val_loss: 0.7997 - val_mae: 0.7997\n",
      "Epoch 577/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7970 - mae: 0.7970 - val_loss: 0.9265 - val_mae: 0.9265\n",
      "Epoch 578/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8727 - mae: 0.8727 - val_loss: 0.9677 - val_mae: 0.9677\n",
      "Epoch 579/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8016 - mae: 0.8016 - val_loss: 0.6296 - val_mae: 0.6296\n",
      "Epoch 580/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7747 - mae: 0.7747 - val_loss: 0.6738 - val_mae: 0.6738\n",
      "Epoch 581/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7428 - mae: 0.7428 - val_loss: 0.7277 - val_mae: 0.7277\n",
      "Epoch 582/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8034 - mae: 0.8034 - val_loss: 0.6965 - val_mae: 0.6965\n",
      "Epoch 583/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8338 - mae: 0.8338 - val_loss: 0.8553 - val_mae: 0.8553\n",
      "Epoch 584/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7720 - mae: 0.7720 - val_loss: 0.9332 - val_mae: 0.9332\n",
      "Epoch 585/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8333 - mae: 0.8333 - val_loss: 1.1556 - val_mae: 1.1556\n",
      "Epoch 586/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8442 - mae: 0.8442 - val_loss: 1.3032 - val_mae: 1.3032\n",
      "Epoch 587/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7257 - mae: 0.7257 - val_loss: 0.7772 - val_mae: 0.7772\n",
      "Epoch 588/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7318 - mae: 0.7318 - val_loss: 0.5324 - val_mae: 0.5324\n",
      "Epoch 589/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8244 - mae: 0.8244 - val_loss: 0.7248 - val_mae: 0.7248\n",
      "Epoch 590/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7797 - mae: 0.7797 - val_loss: 1.2588 - val_mae: 1.2588\n",
      "Epoch 591/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7997 - mae: 0.7997 - val_loss: 0.5869 - val_mae: 0.5869\n",
      "Epoch 592/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8249 - mae: 0.8249 - val_loss: 0.7260 - val_mae: 0.7260\n",
      "Epoch 593/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8597 - mae: 0.8597 - val_loss: 0.7399 - val_mae: 0.7399\n",
      "Epoch 594/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7863 - mae: 0.7863 - val_loss: 0.7243 - val_mae: 0.7243\n",
      "Epoch 595/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7785 - mae: 0.7785 - val_loss: 1.2835 - val_mae: 1.2835\n",
      "Epoch 596/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.8207 - mae: 0.8207 - val_loss: 0.6382 - val_mae: 0.6382\n",
      "Epoch 597/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.8449 - mae: 0.8449 - val_loss: 0.8930 - val_mae: 0.8930\n",
      "Epoch 598/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7911 - mae: 0.7911 - val_loss: 0.5534 - val_mae: 0.5534\n",
      "Epoch 599/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7043 - mae: 0.7043 - val_loss: 1.5930 - val_mae: 1.5930\n",
      "Epoch 600/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8437 - mae: 0.8437 - val_loss: 0.8745 - val_mae: 0.8745\n",
      "Epoch 601/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8183 - mae: 0.8183 - val_loss: 0.6692 - val_mae: 0.6692\n",
      "Epoch 602/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8074 - mae: 0.8074 - val_loss: 1.0603 - val_mae: 1.0603\n",
      "Epoch 603/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8875 - mae: 0.8875 - val_loss: 0.9148 - val_mae: 0.9148\n",
      "Epoch 604/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7986 - mae: 0.7986 - val_loss: 0.8297 - val_mae: 0.8297\n",
      "Epoch 605/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7189 - mae: 0.7189 - val_loss: 0.6098 - val_mae: 0.6098\n",
      "Epoch 606/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7592 - mae: 0.7592 - val_loss: 0.5931 - val_mae: 0.5931\n",
      "Epoch 607/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7558 - mae: 0.7558 - val_loss: 0.4680 - val_mae: 0.4680\n",
      "Epoch 608/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8246 - mae: 0.8246 - val_loss: 1.2097 - val_mae: 1.2097\n",
      "Epoch 609/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8134 - mae: 0.8134 - val_loss: 0.5592 - val_mae: 0.5592\n",
      "Epoch 610/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7597 - mae: 0.7597 - val_loss: 1.0876 - val_mae: 1.0876\n",
      "Epoch 611/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8380 - mae: 0.8380 - val_loss: 0.8347 - val_mae: 0.8347\n",
      "Epoch 612/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.7048 - mae: 0.7048 - val_loss: 0.5528 - val_mae: 0.5528\n",
      "Epoch 613/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7644 - mae: 0.7644 - val_loss: 0.6792 - val_mae: 0.6792\n",
      "Epoch 614/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7681 - mae: 0.7681 - val_loss: 0.7952 - val_mae: 0.7952\n",
      "Epoch 615/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7588 - mae: 0.7588 - val_loss: 0.5910 - val_mae: 0.5910\n",
      "Epoch 616/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7663 - mae: 0.7663 - val_loss: 0.5413 - val_mae: 0.5413\n",
      "Epoch 617/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8119 - mae: 0.8119 - val_loss: 0.4980 - val_mae: 0.4980\n",
      "Epoch 618/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7554 - mae: 0.7554 - val_loss: 1.0691 - val_mae: 1.0691\n",
      "Epoch 619/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7578 - mae: 0.7578 - val_loss: 0.9058 - val_mae: 0.9058\n",
      "Epoch 620/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8118 - mae: 0.8118 - val_loss: 0.9181 - val_mae: 0.9181\n",
      "Epoch 621/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.8339 - mae: 0.8339 - val_loss: 0.5626 - val_mae: 0.5626\n",
      "Epoch 622/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7810 - mae: 0.7810 - val_loss: 0.5636 - val_mae: 0.5636\n",
      "Epoch 623/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7532 - mae: 0.7532 - val_loss: 0.5082 - val_mae: 0.5082\n",
      "Epoch 624/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7530 - mae: 0.7530 - val_loss: 0.6765 - val_mae: 0.6765\n",
      "Epoch 625/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7640 - mae: 0.7640 - val_loss: 1.6890 - val_mae: 1.6890\n",
      "Epoch 626/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8722 - mae: 0.8722 - val_loss: 0.9784 - val_mae: 0.9784\n",
      "Epoch 627/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7795 - mae: 0.7795 - val_loss: 0.4922 - val_mae: 0.4922\n",
      "Epoch 628/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8759 - mae: 0.8759 - val_loss: 0.5043 - val_mae: 0.5043\n",
      "Epoch 629/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8157 - mae: 0.8157 - val_loss: 0.4755 - val_mae: 0.4755\n",
      "Epoch 630/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7868 - mae: 0.7868 - val_loss: 1.1679 - val_mae: 1.1679\n",
      "Epoch 631/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7733 - mae: 0.7733 - val_loss: 0.8682 - val_mae: 0.8682\n",
      "Epoch 632/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8798 - mae: 0.8798 - val_loss: 0.7120 - val_mae: 0.7120\n",
      "Epoch 633/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.9121 - mae: 0.9121 - val_loss: 0.5401 - val_mae: 0.5401\n",
      "Epoch 634/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7758 - mae: 0.7758 - val_loss: 0.4738 - val_mae: 0.4738\n",
      "Epoch 635/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7509 - mae: 0.7509 - val_loss: 0.6311 - val_mae: 0.6311\n",
      "Epoch 636/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7350 - mae: 0.7350 - val_loss: 0.6905 - val_mae: 0.6905\n",
      "Epoch 637/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9296 - mae: 0.9296 - val_loss: 0.8773 - val_mae: 0.8773\n",
      "Epoch 638/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7767 - mae: 0.7767 - val_loss: 0.5903 - val_mae: 0.5903\n",
      "Epoch 639/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7159 - mae: 0.7159 - val_loss: 1.0496 - val_mae: 1.0496\n",
      "Epoch 640/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7116 - mae: 0.7116 - val_loss: 0.4972 - val_mae: 0.4972\n",
      "Epoch 641/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8821 - mae: 0.8821 - val_loss: 0.9559 - val_mae: 0.9559\n",
      "Epoch 642/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7686 - mae: 0.7686 - val_loss: 0.4853 - val_mae: 0.4853\n",
      "Epoch 643/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7294 - mae: 0.7294 - val_loss: 0.4472 - val_mae: 0.4472\n",
      "Epoch 644/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7183 - mae: 0.7183 - val_loss: 1.0497 - val_mae: 1.0497\n",
      "Epoch 645/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8680 - mae: 0.8680 - val_loss: 0.9218 - val_mae: 0.9218\n",
      "Epoch 646/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7058 - mae: 0.7058 - val_loss: 0.5290 - val_mae: 0.5290\n",
      "Epoch 647/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6969 - mae: 0.6969 - val_loss: 0.4307 - val_mae: 0.4307\n",
      "Epoch 648/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7516 - mae: 0.7516 - val_loss: 1.1701 - val_mae: 1.1701\n",
      "Epoch 649/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7331 - mae: 0.7331 - val_loss: 0.5561 - val_mae: 0.5561\n",
      "Epoch 650/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.7366 - mae: 0.7366 - val_loss: 0.4867 - val_mae: 0.4867\n",
      "Epoch 651/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7106 - mae: 0.7106 - val_loss: 0.4409 - val_mae: 0.4409\n",
      "Epoch 652/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7262 - mae: 0.7262 - val_loss: 0.5788 - val_mae: 0.5788\n",
      "Epoch 653/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6999 - mae: 0.6999 - val_loss: 0.4488 - val_mae: 0.4488\n",
      "Epoch 654/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7817 - mae: 0.7817 - val_loss: 0.7203 - val_mae: 0.7203\n",
      "Epoch 655/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7019 - mae: 0.7019 - val_loss: 0.4875 - val_mae: 0.4875\n",
      "Epoch 656/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6982 - mae: 0.6982 - val_loss: 0.4556 - val_mae: 0.4556\n",
      "Epoch 657/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6711 - mae: 0.6711 - val_loss: 0.7245 - val_mae: 0.7245\n",
      "Epoch 658/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.8284 - mae: 0.8284 - val_loss: 0.6264 - val_mae: 0.6264\n",
      "Epoch 659/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7511 - mae: 0.7511 - val_loss: 0.4260 - val_mae: 0.4260\n",
      "Epoch 660/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7913 - mae: 0.7913 - val_loss: 0.5229 - val_mae: 0.5229\n",
      "Epoch 661/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.8065 - mae: 0.8065 - val_loss: 1.7502 - val_mae: 1.7502\n",
      "Epoch 662/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7234 - mae: 0.7234 - val_loss: 0.4945 - val_mae: 0.4945\n",
      "Epoch 663/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7314 - mae: 0.7314 - val_loss: 0.7287 - val_mae: 0.7287\n",
      "Epoch 664/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6748 - mae: 0.6748 - val_loss: 0.5164 - val_mae: 0.5164\n",
      "Epoch 665/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7447 - mae: 0.7447 - val_loss: 0.4014 - val_mae: 0.4014\n",
      "Epoch 666/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7089 - mae: 0.7089 - val_loss: 1.0673 - val_mae: 1.0673\n",
      "Epoch 667/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.9240 - mae: 0.9240 - val_loss: 1.4608 - val_mae: 1.4608\n",
      "Epoch 668/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7079 - mae: 0.7079 - val_loss: 0.8601 - val_mae: 0.8601\n",
      "Epoch 669/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7253 - mae: 0.7253 - val_loss: 0.5854 - val_mae: 0.5854\n",
      "Epoch 670/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7357 - mae: 0.7357 - val_loss: 0.6847 - val_mae: 0.6847\n",
      "Epoch 671/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8035 - mae: 0.8035 - val_loss: 0.7026 - val_mae: 0.7026\n",
      "Epoch 672/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7208 - mae: 0.7208 - val_loss: 1.3199 - val_mae: 1.3199\n",
      "Epoch 673/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7963 - mae: 0.7963 - val_loss: 0.9515 - val_mae: 0.9515\n",
      "Epoch 674/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7385 - mae: 0.7385 - val_loss: 0.5151 - val_mae: 0.5151\n",
      "Epoch 675/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6933 - mae: 0.6933 - val_loss: 0.8786 - val_mae: 0.8786\n",
      "Epoch 676/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7645 - mae: 0.7645 - val_loss: 0.8278 - val_mae: 0.8278\n",
      "Epoch 677/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.6896 - mae: 0.6896 - val_loss: 0.7834 - val_mae: 0.7834\n",
      "Epoch 678/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.8378 - mae: 0.8378 - val_loss: 0.6720 - val_mae: 0.6720\n",
      "Epoch 679/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6727 - mae: 0.6727 - val_loss: 1.0274 - val_mae: 1.0274\n",
      "Epoch 680/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7975 - mae: 0.7975 - val_loss: 0.4502 - val_mae: 0.4502\n",
      "Epoch 681/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6434 - mae: 0.6434 - val_loss: 0.4127 - val_mae: 0.4127\n",
      "Epoch 682/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7278 - mae: 0.7278 - val_loss: 0.6536 - val_mae: 0.6536\n",
      "Epoch 683/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6780 - mae: 0.6780 - val_loss: 0.5071 - val_mae: 0.5071\n",
      "Epoch 684/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.7836 - mae: 0.7836 - val_loss: 1.1638 - val_mae: 1.1638\n",
      "Epoch 685/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7254 - mae: 0.7254 - val_loss: 0.4499 - val_mae: 0.4499\n",
      "Epoch 686/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7863 - mae: 0.7863 - val_loss: 0.4985 - val_mae: 0.4985\n",
      "Epoch 687/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.7184 - mae: 0.7184 - val_loss: 0.7846 - val_mae: 0.7846\n",
      "Epoch 688/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7236 - mae: 0.7236 - val_loss: 0.9709 - val_mae: 0.9709\n",
      "Epoch 689/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7183 - mae: 0.7183 - val_loss: 0.5444 - val_mae: 0.5444\n",
      "Epoch 690/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7225 - mae: 0.7225 - val_loss: 0.5901 - val_mae: 0.5901\n",
      "Epoch 691/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7253 - mae: 0.7253 - val_loss: 0.6245 - val_mae: 0.6245\n",
      "Epoch 692/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7209 - mae: 0.7209 - val_loss: 0.7595 - val_mae: 0.7595\n",
      "Epoch 693/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7235 - mae: 0.7235 - val_loss: 0.7106 - val_mae: 0.7106\n",
      "Epoch 694/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7559 - mae: 0.7559 - val_loss: 0.7261 - val_mae: 0.7261\n",
      "Epoch 695/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7250 - mae: 0.7250 - val_loss: 0.4961 - val_mae: 0.4961\n",
      "Epoch 696/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7164 - mae: 0.7164 - val_loss: 0.6313 - val_mae: 0.6313\n",
      "Epoch 697/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7625 - mae: 0.7625 - val_loss: 0.7177 - val_mae: 0.7177\n",
      "Epoch 698/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6249 - mae: 0.6249 - val_loss: 0.8523 - val_mae: 0.8523\n",
      "Epoch 699/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7806 - mae: 0.7806 - val_loss: 0.5784 - val_mae: 0.5784\n",
      "Epoch 700/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7004 - mae: 0.7004 - val_loss: 0.4961 - val_mae: 0.4961\n",
      "Epoch 701/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7541 - mae: 0.7541 - val_loss: 0.5014 - val_mae: 0.5014\n",
      "Epoch 702/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7807 - mae: 0.7807 - val_loss: 0.6132 - val_mae: 0.6132\n",
      "Epoch 703/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6782 - mae: 0.6782 - val_loss: 0.4762 - val_mae: 0.4762\n",
      "Epoch 704/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7056 - mae: 0.7056 - val_loss: 1.0041 - val_mae: 1.0041\n",
      "Epoch 705/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7038 - mae: 0.7038 - val_loss: 0.5302 - val_mae: 0.5302\n",
      "Epoch 706/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6704 - mae: 0.6704 - val_loss: 0.8780 - val_mae: 0.8780\n",
      "Epoch 707/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6833 - mae: 0.6833 - val_loss: 0.6374 - val_mae: 0.6374\n",
      "Epoch 708/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6743 - mae: 0.6743 - val_loss: 0.7435 - val_mae: 0.7435\n",
      "Epoch 709/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7270 - mae: 0.7270 - val_loss: 0.7769 - val_mae: 0.7769\n",
      "Epoch 710/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6641 - mae: 0.6641 - val_loss: 1.0422 - val_mae: 1.0422\n",
      "Epoch 711/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.8240 - mae: 0.8240 - val_loss: 0.6671 - val_mae: 0.6671\n",
      "Epoch 712/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6228 - mae: 0.6228 - val_loss: 0.9295 - val_mae: 0.9295\n",
      "Epoch 713/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7136 - mae: 0.7136 - val_loss: 0.8125 - val_mae: 0.8125\n",
      "Epoch 714/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7171 - mae: 0.7171 - val_loss: 0.6641 - val_mae: 0.6641\n",
      "Epoch 715/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7164 - mae: 0.7164 - val_loss: 0.4622 - val_mae: 0.4622\n",
      "Epoch 716/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7538 - mae: 0.7538 - val_loss: 0.4945 - val_mae: 0.4945\n",
      "Epoch 717/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7210 - mae: 0.7210 - val_loss: 0.7877 - val_mae: 0.7877\n",
      "Epoch 718/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6788 - mae: 0.6788 - val_loss: 0.6208 - val_mae: 0.6208\n",
      "Epoch 719/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7017 - mae: 0.7017 - val_loss: 0.4472 - val_mae: 0.4472\n",
      "Epoch 720/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7560 - mae: 0.7560 - val_loss: 0.8880 - val_mae: 0.8880\n",
      "Epoch 721/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7200 - mae: 0.7200 - val_loss: 0.6687 - val_mae: 0.6687\n",
      "Epoch 722/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6994 - mae: 0.6994 - val_loss: 0.7345 - val_mae: 0.7345\n",
      "Epoch 723/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6494 - mae: 0.6494 - val_loss: 0.4103 - val_mae: 0.4103\n",
      "Epoch 724/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6924 - mae: 0.6924 - val_loss: 0.8408 - val_mae: 0.8408\n",
      "Epoch 725/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7017 - mae: 0.7017 - val_loss: 0.4921 - val_mae: 0.4921\n",
      "Epoch 726/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7490 - mae: 0.7490 - val_loss: 1.4204 - val_mae: 1.4204\n",
      "Epoch 727/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7857 - mae: 0.7857 - val_loss: 0.4277 - val_mae: 0.4277\n",
      "Epoch 728/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6336 - mae: 0.6336 - val_loss: 0.7156 - val_mae: 0.7156\n",
      "Epoch 729/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6806 - mae: 0.6806 - val_loss: 0.6007 - val_mae: 0.6007\n",
      "Epoch 730/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6926 - mae: 0.6926 - val_loss: 0.9893 - val_mae: 0.9893\n",
      "Epoch 731/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7192 - mae: 0.7192 - val_loss: 1.0796 - val_mae: 1.0796\n",
      "Epoch 732/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7002 - mae: 0.7002 - val_loss: 0.5836 - val_mae: 0.5836\n",
      "Epoch 733/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6545 - mae: 0.6545 - val_loss: 0.7514 - val_mae: 0.7514\n",
      "Epoch 734/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7268 - mae: 0.7268 - val_loss: 0.4229 - val_mae: 0.4229\n",
      "Epoch 735/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7040 - mae: 0.7040 - val_loss: 0.5241 - val_mae: 0.5241\n",
      "Epoch 736/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6914 - mae: 0.6914 - val_loss: 0.8294 - val_mae: 0.8294\n",
      "Epoch 737/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6847 - mae: 0.6847 - val_loss: 0.3791 - val_mae: 0.3791\n",
      "Epoch 738/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7913 - mae: 0.7913 - val_loss: 0.4632 - val_mae: 0.4632\n",
      "Epoch 739/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7226 - mae: 0.7226 - val_loss: 0.3966 - val_mae: 0.3966\n",
      "Epoch 740/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6940 - mae: 0.6940 - val_loss: 0.8970 - val_mae: 0.8970\n",
      "Epoch 741/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7509 - mae: 0.7509 - val_loss: 0.8907 - val_mae: 0.8907\n",
      "Epoch 742/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7649 - mae: 0.7649 - val_loss: 0.7096 - val_mae: 0.7096\n",
      "Epoch 743/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6629 - mae: 0.6629 - val_loss: 0.6377 - val_mae: 0.6377\n",
      "Epoch 744/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6704 - mae: 0.6704 - val_loss: 0.5474 - val_mae: 0.5474\n",
      "Epoch 745/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6422 - mae: 0.6422 - val_loss: 0.5166 - val_mae: 0.5166\n",
      "Epoch 746/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6599 - mae: 0.6599 - val_loss: 0.7886 - val_mae: 0.7886\n",
      "Epoch 747/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7498 - mae: 0.7498 - val_loss: 0.5311 - val_mae: 0.5311\n",
      "Epoch 748/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6414 - mae: 0.6414 - val_loss: 0.4761 - val_mae: 0.4761\n",
      "Epoch 749/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6574 - mae: 0.6574 - val_loss: 0.5594 - val_mae: 0.5594\n",
      "Epoch 750/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6498 - mae: 0.6498 - val_loss: 0.8100 - val_mae: 0.8100\n",
      "Epoch 751/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7711 - mae: 0.7711 - val_loss: 0.8280 - val_mae: 0.8280\n",
      "Epoch 752/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6717 - mae: 0.6717 - val_loss: 0.4346 - val_mae: 0.4346\n",
      "Epoch 753/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7140 - mae: 0.7140 - val_loss: 0.7714 - val_mae: 0.7714\n",
      "Epoch 754/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7816 - mae: 0.7816 - val_loss: 0.9949 - val_mae: 0.9949\n",
      "Epoch 755/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6751 - mae: 0.6751 - val_loss: 0.5344 - val_mae: 0.5344\n",
      "Epoch 756/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6947 - mae: 0.6947 - val_loss: 0.8180 - val_mae: 0.8180\n",
      "Epoch 757/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6690 - mae: 0.6690 - val_loss: 0.9711 - val_mae: 0.9711\n",
      "Epoch 758/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7598 - mae: 0.7598 - val_loss: 0.5351 - val_mae: 0.5351\n",
      "Epoch 759/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7161 - mae: 0.7161 - val_loss: 0.4134 - val_mae: 0.4134\n",
      "Epoch 760/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7068 - mae: 0.7068 - val_loss: 0.4341 - val_mae: 0.4341\n",
      "Epoch 761/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6769 - mae: 0.6769 - val_loss: 0.5518 - val_mae: 0.5518\n",
      "Epoch 762/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7525 - mae: 0.7525 - val_loss: 0.6230 - val_mae: 0.6230\n",
      "Epoch 763/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6111 - mae: 0.6111 - val_loss: 0.5314 - val_mae: 0.5314\n",
      "Epoch 764/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7234 - mae: 0.7234 - val_loss: 0.6334 - val_mae: 0.6334\n",
      "Epoch 765/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7034 - mae: 0.7034 - val_loss: 0.9817 - val_mae: 0.9817\n",
      "Epoch 766/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6166 - mae: 0.6166 - val_loss: 0.3626 - val_mae: 0.3626\n",
      "Epoch 767/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7215 - mae: 0.7215 - val_loss: 0.8097 - val_mae: 0.8097\n",
      "Epoch 768/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6721 - mae: 0.6721 - val_loss: 0.7065 - val_mae: 0.7065\n",
      "Epoch 769/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7255 - mae: 0.7255 - val_loss: 0.3736 - val_mae: 0.3736\n",
      "Epoch 770/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6498 - mae: 0.6498 - val_loss: 1.5441 - val_mae: 1.5441\n",
      "Epoch 771/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6226 - mae: 0.6226 - val_loss: 0.6604 - val_mae: 0.6604\n",
      "Epoch 772/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6532 - mae: 0.6532 - val_loss: 0.4661 - val_mae: 0.4661\n",
      "Epoch 773/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6649 - mae: 0.6649 - val_loss: 0.9451 - val_mae: 0.9451\n",
      "Epoch 774/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6991 - mae: 0.6991 - val_loss: 0.7959 - val_mae: 0.7959\n",
      "Epoch 775/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7185 - mae: 0.7185 - val_loss: 0.5516 - val_mae: 0.5516\n",
      "Epoch 776/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6919 - mae: 0.6919 - val_loss: 0.3899 - val_mae: 0.3899\n",
      "Epoch 777/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6256 - mae: 0.6256 - val_loss: 0.6277 - val_mae: 0.6277\n",
      "Epoch 778/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.7138 - mae: 0.7138 - val_loss: 0.7703 - val_mae: 0.7703\n",
      "Epoch 779/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.7112 - mae: 0.7112 - val_loss: 0.3789 - val_mae: 0.3789\n",
      "Epoch 780/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6910 - mae: 0.6910 - val_loss: 0.4036 - val_mae: 0.4036\n",
      "Epoch 781/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6362 - mae: 0.6362 - val_loss: 0.7962 - val_mae: 0.7962\n",
      "Epoch 782/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.7007 - mae: 0.7007 - val_loss: 0.8461 - val_mae: 0.8461\n",
      "Epoch 783/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6467 - mae: 0.6467 - val_loss: 1.0335 - val_mae: 1.0335\n",
      "Epoch 784/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.7415 - mae: 0.7415 - val_loss: 0.6610 - val_mae: 0.6610\n",
      "Epoch 785/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6747 - mae: 0.6747 - val_loss: 0.5463 - val_mae: 0.5463\n",
      "Epoch 786/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6561 - mae: 0.6561 - val_loss: 0.9203 - val_mae: 0.9203\n",
      "Epoch 787/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6718 - mae: 0.6718 - val_loss: 0.9087 - val_mae: 0.9087\n",
      "Epoch 788/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6506 - mae: 0.6506 - val_loss: 0.7589 - val_mae: 0.7589\n",
      "Epoch 789/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6982 - mae: 0.6982 - val_loss: 0.6622 - val_mae: 0.6622\n",
      "Epoch 790/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5972 - mae: 0.5972 - val_loss: 0.4372 - val_mae: 0.4372\n",
      "Epoch 791/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.7267 - mae: 0.7267 - val_loss: 0.4137 - val_mae: 0.4137\n",
      "Epoch 792/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6909 - mae: 0.6909 - val_loss: 0.5372 - val_mae: 0.5372\n",
      "Epoch 793/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6648 - mae: 0.6648 - val_loss: 0.3751 - val_mae: 0.3751\n",
      "Epoch 794/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6503 - mae: 0.6503 - val_loss: 0.9285 - val_mae: 0.9285\n",
      "Epoch 795/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6788 - mae: 0.6788 - val_loss: 0.9345 - val_mae: 0.9345\n",
      "Epoch 796/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6971 - mae: 0.6971 - val_loss: 1.0556 - val_mae: 1.0556\n",
      "Epoch 797/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6948 - mae: 0.6948 - val_loss: 0.7417 - val_mae: 0.7417\n",
      "Epoch 798/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6550 - mae: 0.6550 - val_loss: 0.6548 - val_mae: 0.6548\n",
      "Epoch 799/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.7175 - mae: 0.7175 - val_loss: 0.3759 - val_mae: 0.3759\n",
      "Epoch 800/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.7192 - mae: 0.7192 - val_loss: 0.4937 - val_mae: 0.4937\n",
      "Epoch 801/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.5864 - mae: 0.5864 - val_loss: 0.9100 - val_mae: 0.9100\n",
      "Epoch 802/1500\n",
      "592/592 [==============================] - 4s 6ms/step - loss: 0.6324 - mae: 0.6324 - val_loss: 0.7084 - val_mae: 0.7084\n",
      "Epoch 803/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.7062 - mae: 0.7062 - val_loss: 0.4324 - val_mae: 0.4324\n",
      "Epoch 804/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.7291 - mae: 0.7291 - val_loss: 0.7492 - val_mae: 0.7492\n",
      "Epoch 805/1500\n",
      "592/592 [==============================] - 13s 23ms/step - loss: 0.7112 - mae: 0.7112 - val_loss: 0.5283 - val_mae: 0.5283\n",
      "Epoch 806/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.7888 - mae: 0.7888 - val_loss: 0.6951 - val_mae: 0.6951\n",
      "Epoch 807/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5895 - mae: 0.5895 - val_loss: 0.3831 - val_mae: 0.3831\n",
      "Epoch 808/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5632 - mae: 0.5632 - val_loss: 0.5659 - val_mae: 0.5659\n",
      "Epoch 809/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6899 - mae: 0.6899 - val_loss: 1.2845 - val_mae: 1.2845\n",
      "Epoch 810/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.6167 - mae: 0.6167 - val_loss: 0.4893 - val_mae: 0.4893\n",
      "Epoch 811/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6574 - mae: 0.6574 - val_loss: 0.5518 - val_mae: 0.5518\n",
      "Epoch 812/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6166 - mae: 0.6166 - val_loss: 0.8168 - val_mae: 0.8168\n",
      "Epoch 813/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.8044 - mae: 0.8044 - val_loss: 0.5529 - val_mae: 0.5529\n",
      "Epoch 814/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6454 - mae: 0.6454 - val_loss: 0.8073 - val_mae: 0.8073\n",
      "Epoch 815/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6240 - mae: 0.6240 - val_loss: 0.8825 - val_mae: 0.8825\n",
      "Epoch 816/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6717 - mae: 0.6717 - val_loss: 1.6746 - val_mae: 1.6746\n",
      "Epoch 817/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.7031 - mae: 0.7031 - val_loss: 0.5008 - val_mae: 0.5008\n",
      "Epoch 818/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6145 - mae: 0.6145 - val_loss: 0.4843 - val_mae: 0.4843\n",
      "Epoch 819/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6938 - mae: 0.6938 - val_loss: 0.6234 - val_mae: 0.6234\n",
      "Epoch 820/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.6256 - mae: 0.6256 - val_loss: 0.6717 - val_mae: 0.6717\n",
      "Epoch 821/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6589 - mae: 0.6589 - val_loss: 0.7521 - val_mae: 0.7521\n",
      "Epoch 822/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.6462 - mae: 0.6462 - val_loss: 0.5036 - val_mae: 0.5036\n",
      "Epoch 823/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6743 - mae: 0.6743 - val_loss: 0.8688 - val_mae: 0.8688\n",
      "Epoch 824/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.7100 - mae: 0.7100 - val_loss: 0.6225 - val_mae: 0.6225\n",
      "Epoch 825/1500\n",
      "592/592 [==============================] - 40s 67ms/step - loss: 0.6309 - mae: 0.6309 - val_loss: 0.5963 - val_mae: 0.5963\n",
      "Epoch 826/1500\n",
      "592/592 [==============================] - 25s 42ms/step - loss: 0.5890 - mae: 0.5890 - val_loss: 0.4929 - val_mae: 0.4929\n",
      "Epoch 827/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.6594 - mae: 0.6594 - val_loss: 0.5191 - val_mae: 0.5191\n",
      "Epoch 828/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.6565 - mae: 0.6565 - val_loss: 0.6845 - val_mae: 0.6845\n",
      "Epoch 829/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.6394 - mae: 0.6394 - val_loss: 0.6826 - val_mae: 0.6826\n",
      "Epoch 830/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6375 - mae: 0.6375 - val_loss: 0.4549 - val_mae: 0.4549\n",
      "Epoch 831/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6905 - mae: 0.6905 - val_loss: 0.5945 - val_mae: 0.5945\n",
      "Epoch 832/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5946 - mae: 0.5946 - val_loss: 0.3646 - val_mae: 0.3646\n",
      "Epoch 833/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.6435 - mae: 0.6435 - val_loss: 0.5827 - val_mae: 0.5827\n",
      "Epoch 834/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.6333 - mae: 0.6333 - val_loss: 0.3530 - val_mae: 0.3530\n",
      "Epoch 835/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5940 - mae: 0.5940 - val_loss: 0.3974 - val_mae: 0.3974\n",
      "Epoch 836/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.6522 - mae: 0.6522 - val_loss: 0.6324 - val_mae: 0.6324\n",
      "Epoch 837/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.6372 - mae: 0.6372 - val_loss: 0.3973 - val_mae: 0.3973\n",
      "Epoch 838/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.5819 - mae: 0.5819 - val_loss: 0.8928 - val_mae: 0.8928\n",
      "Epoch 839/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.6064 - mae: 0.6064 - val_loss: 0.8348 - val_mae: 0.8348\n",
      "Epoch 840/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.6904 - mae: 0.6904 - val_loss: 0.6278 - val_mae: 0.6278\n",
      "Epoch 841/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.7248 - mae: 0.7248 - val_loss: 1.2180 - val_mae: 1.2180\n",
      "Epoch 842/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.6817 - mae: 0.6817 - val_loss: 0.7121 - val_mae: 0.7121\n",
      "Epoch 843/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.5877 - mae: 0.5877 - val_loss: 0.7206 - val_mae: 0.7206\n",
      "Epoch 844/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.7076 - mae: 0.7076 - val_loss: 0.3275 - val_mae: 0.3275\n",
      "Epoch 845/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.6084 - mae: 0.6084 - val_loss: 0.4411 - val_mae: 0.4411\n",
      "Epoch 846/1500\n",
      "592/592 [==============================] - 10s 16ms/step - loss: 0.6690 - mae: 0.6690 - val_loss: 1.2567 - val_mae: 1.2567\n",
      "Epoch 847/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5750 - mae: 0.5750 - val_loss: 0.5127 - val_mae: 0.5127\n",
      "Epoch 848/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.7502 - mae: 0.7502 - val_loss: 0.9320 - val_mae: 0.9320\n",
      "Epoch 849/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5889 - mae: 0.5889 - val_loss: 0.5661 - val_mae: 0.5661\n",
      "Epoch 850/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.6546 - mae: 0.6546 - val_loss: 1.0233 - val_mae: 1.0233\n",
      "Epoch 851/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5831 - mae: 0.5831 - val_loss: 0.7805 - val_mae: 0.7805\n",
      "Epoch 852/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6254 - mae: 0.6254 - val_loss: 0.3544 - val_mae: 0.3544\n",
      "Epoch 853/1500\n",
      "592/592 [==============================] - 12s 20ms/step - loss: 0.5734 - mae: 0.5734 - val_loss: 0.3587 - val_mae: 0.3587\n",
      "Epoch 854/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.6065 - mae: 0.6065 - val_loss: 0.3810 - val_mae: 0.3810\n",
      "Epoch 855/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.7266 - mae: 0.7266 - val_loss: 0.5477 - val_mae: 0.5477\n",
      "Epoch 856/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6438 - mae: 0.6438 - val_loss: 0.4569 - val_mae: 0.4569\n",
      "Epoch 857/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.6586 - mae: 0.6586 - val_loss: 0.9111 - val_mae: 0.9111\n",
      "Epoch 858/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.6536 - mae: 0.6536 - val_loss: 0.8016 - val_mae: 0.8016\n",
      "Epoch 859/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5509 - mae: 0.5509 - val_loss: 1.0161 - val_mae: 1.0161\n",
      "Epoch 860/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5633 - mae: 0.5633 - val_loss: 1.2113 - val_mae: 1.2113\n",
      "Epoch 861/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6670 - mae: 0.6670 - val_loss: 0.5486 - val_mae: 0.5486\n",
      "Epoch 862/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5847 - mae: 0.5847 - val_loss: 0.4122 - val_mae: 0.4122\n",
      "Epoch 863/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.6003 - mae: 0.6003 - val_loss: 0.5402 - val_mae: 0.5402\n",
      "Epoch 864/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5799 - mae: 0.5799 - val_loss: 0.5791 - val_mae: 0.5791\n",
      "Epoch 865/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.6005 - mae: 0.6005 - val_loss: 0.6259 - val_mae: 0.6259\n",
      "Epoch 866/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5936 - mae: 0.5936 - val_loss: 0.4163 - val_mae: 0.4163\n",
      "Epoch 867/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.6436 - mae: 0.6436 - val_loss: 1.5107 - val_mae: 1.5107\n",
      "Epoch 868/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6459 - mae: 0.6459 - val_loss: 0.5386 - val_mae: 0.5386\n",
      "Epoch 869/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5892 - mae: 0.5892 - val_loss: 0.5348 - val_mae: 0.5348\n",
      "Epoch 870/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.7917 - mae: 0.7917 - val_loss: 0.4808 - val_mae: 0.4808\n",
      "Epoch 871/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6032 - mae: 0.6032 - val_loss: 0.7100 - val_mae: 0.7100\n",
      "Epoch 872/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6456 - mae: 0.6456 - val_loss: 0.3467 - val_mae: 0.3467\n",
      "Epoch 873/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6070 - mae: 0.6070 - val_loss: 0.5513 - val_mae: 0.5513\n",
      "Epoch 874/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5349 - mae: 0.5349 - val_loss: 0.4897 - val_mae: 0.4897\n",
      "Epoch 875/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.6111 - mae: 0.6111 - val_loss: 1.2914 - val_mae: 1.2914\n",
      "Epoch 876/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.6202 - mae: 0.6202 - val_loss: 0.8437 - val_mae: 0.8437\n",
      "Epoch 877/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.5830 - mae: 0.5830 - val_loss: 0.5063 - val_mae: 0.5063\n",
      "Epoch 878/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6696 - mae: 0.6696 - val_loss: 0.4383 - val_mae: 0.4383\n",
      "Epoch 879/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.5566 - mae: 0.5566 - val_loss: 0.3914 - val_mae: 0.3914\n",
      "Epoch 880/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6571 - mae: 0.6571 - val_loss: 0.9482 - val_mae: 0.9482\n",
      "Epoch 881/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5841 - mae: 0.5841 - val_loss: 0.4995 - val_mae: 0.4995\n",
      "Epoch 882/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6153 - mae: 0.6153 - val_loss: 0.5006 - val_mae: 0.5006\n",
      "Epoch 883/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6350 - mae: 0.6350 - val_loss: 0.3482 - val_mae: 0.3482\n",
      "Epoch 884/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6115 - mae: 0.6115 - val_loss: 0.5184 - val_mae: 0.5184\n",
      "Epoch 885/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5769 - mae: 0.5769 - val_loss: 0.7953 - val_mae: 0.7953\n",
      "Epoch 886/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6334 - mae: 0.6334 - val_loss: 0.3476 - val_mae: 0.3476\n",
      "Epoch 887/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.6657 - mae: 0.6657 - val_loss: 0.9424 - val_mae: 0.9424\n",
      "Epoch 888/1500\n",
      "592/592 [==============================] - 12s 21ms/step - loss: 0.6129 - mae: 0.6129 - val_loss: 0.3913 - val_mae: 0.3913\n",
      "Epoch 889/1500\n",
      "592/592 [==============================] - 15s 25ms/step - loss: 0.5693 - mae: 0.5693 - val_loss: 0.4070 - val_mae: 0.4070\n",
      "Epoch 890/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.6329 - mae: 0.6329 - val_loss: 1.0727 - val_mae: 1.0727\n",
      "Epoch 891/1500\n",
      "592/592 [==============================] - 18s 31ms/step - loss: 0.6143 - mae: 0.6143 - val_loss: 0.5144 - val_mae: 0.5144\n",
      "Epoch 892/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5995 - mae: 0.5995 - val_loss: 0.4126 - val_mae: 0.4126\n",
      "Epoch 893/1500\n",
      "592/592 [==============================] - 19s 32ms/step - loss: 0.6143 - mae: 0.6143 - val_loss: 0.7232 - val_mae: 0.7232\n",
      "Epoch 894/1500\n",
      "592/592 [==============================] - 18s 30ms/step - loss: 0.5894 - mae: 0.5894 - val_loss: 0.4432 - val_mae: 0.4432\n",
      "Epoch 895/1500\n",
      "592/592 [==============================] - 20s 34ms/step - loss: 0.6787 - mae: 0.6787 - val_loss: 0.5414 - val_mae: 0.5414\n",
      "Epoch 896/1500\n",
      "592/592 [==============================] - 21s 35ms/step - loss: 0.5887 - mae: 0.5887 - val_loss: 0.4755 - val_mae: 0.4755\n",
      "Epoch 897/1500\n",
      "592/592 [==============================] - 20s 34ms/step - loss: 0.6071 - mae: 0.6071 - val_loss: 0.4069 - val_mae: 0.4069\n",
      "Epoch 898/1500\n",
      "592/592 [==============================] - 19s 32ms/step - loss: 0.6184 - mae: 0.6184 - val_loss: 0.8244 - val_mae: 0.8244\n",
      "Epoch 899/1500\n",
      "592/592 [==============================] - 15s 26ms/step - loss: 0.6147 - mae: 0.6147 - val_loss: 0.4630 - val_mae: 0.4630\n",
      "Epoch 900/1500\n",
      "592/592 [==============================] - 23s 38ms/step - loss: 0.5873 - mae: 0.5873 - val_loss: 0.5184 - val_mae: 0.5184\n",
      "Epoch 901/1500\n",
      "592/592 [==============================] - 22s 37ms/step - loss: 0.5849 - mae: 0.5849 - val_loss: 0.3055 - val_mae: 0.3055\n",
      "Epoch 902/1500\n",
      "592/592 [==============================] - 14s 23ms/step - loss: 0.5775 - mae: 0.5775 - val_loss: 0.4439 - val_mae: 0.4439\n",
      "Epoch 903/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.6002 - mae: 0.6002 - val_loss: 0.3692 - val_mae: 0.3692\n",
      "Epoch 904/1500\n",
      "592/592 [==============================] - 24s 40ms/step - loss: 0.6039 - mae: 0.6039 - val_loss: 0.5827 - val_mae: 0.5827\n",
      "Epoch 905/1500\n",
      "592/592 [==============================] - 22s 36ms/step - loss: 0.5786 - mae: 0.5786 - val_loss: 0.4916 - val_mae: 0.4916\n",
      "Epoch 906/1500\n",
      "592/592 [==============================] - 12s 21ms/step - loss: 0.5909 - mae: 0.5909 - val_loss: 0.5087 - val_mae: 0.5087\n",
      "Epoch 907/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.5407 - mae: 0.5407 - val_loss: 0.7942 - val_mae: 0.7942\n",
      "Epoch 908/1500\n",
      "592/592 [==============================] - 12s 21ms/step - loss: 0.6100 - mae: 0.6100 - val_loss: 0.4502 - val_mae: 0.4502\n",
      "Epoch 909/1500\n",
      "592/592 [==============================] - 15s 25ms/step - loss: 0.5512 - mae: 0.5512 - val_loss: 0.3360 - val_mae: 0.3360\n",
      "Epoch 910/1500\n",
      "592/592 [==============================] - 14s 24ms/step - loss: 0.5988 - mae: 0.5988 - val_loss: 0.4213 - val_mae: 0.4213\n",
      "Epoch 911/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.6623 - mae: 0.6623 - val_loss: 0.4718 - val_mae: 0.4718\n",
      "Epoch 912/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.5292 - mae: 0.5292 - val_loss: 0.3224 - val_mae: 0.3224\n",
      "Epoch 913/1500\n",
      "592/592 [==============================] - 12s 21ms/step - loss: 0.5710 - mae: 0.5710 - val_loss: 1.1553 - val_mae: 1.1553\n",
      "Epoch 914/1500\n",
      "592/592 [==============================] - 12s 21ms/step - loss: 0.6234 - mae: 0.6234 - val_loss: 0.5197 - val_mae: 0.5197\n",
      "Epoch 915/1500\n",
      "592/592 [==============================] - 20s 33ms/step - loss: 0.5935 - mae: 0.5935 - val_loss: 0.4635 - val_mae: 0.4635\n",
      "Epoch 916/1500\n",
      "592/592 [==============================] - 14s 24ms/step - loss: 0.5566 - mae: 0.5566 - val_loss: 0.3623 - val_mae: 0.3623\n",
      "Epoch 917/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.5590 - mae: 0.5590 - val_loss: 1.0092 - val_mae: 1.0092\n",
      "Epoch 918/1500\n",
      "592/592 [==============================] - 12s 20ms/step - loss: 0.6679 - mae: 0.6679 - val_loss: 0.3408 - val_mae: 0.3408\n",
      "Epoch 919/1500\n",
      "592/592 [==============================] - 10s 16ms/step - loss: 0.5500 - mae: 0.5500 - val_loss: 0.8048 - val_mae: 0.8048\n",
      "Epoch 920/1500\n",
      "592/592 [==============================] - 13s 23ms/step - loss: 0.6410 - mae: 0.6410 - val_loss: 0.4728 - val_mae: 0.4728\n",
      "Epoch 921/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.5525 - mae: 0.5525 - val_loss: 0.4080 - val_mae: 0.4080\n",
      "Epoch 922/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.7748 - mae: 0.7748 - val_loss: 0.3813 - val_mae: 0.3813\n",
      "Epoch 923/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5376 - mae: 0.5376 - val_loss: 0.3566 - val_mae: 0.3566\n",
      "Epoch 924/1500\n",
      "592/592 [==============================] - 19s 32ms/step - loss: 0.6024 - mae: 0.6024 - val_loss: 0.4797 - val_mae: 0.4797\n",
      "Epoch 925/1500\n",
      "592/592 [==============================] - 18s 31ms/step - loss: 0.5527 - mae: 0.5527 - val_loss: 0.9974 - val_mae: 0.9974\n",
      "Epoch 926/1500\n",
      "592/592 [==============================] - 13s 21ms/step - loss: 0.5794 - mae: 0.5794 - val_loss: 0.8015 - val_mae: 0.8015\n",
      "Epoch 927/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.6487 - mae: 0.6487 - val_loss: 0.4763 - val_mae: 0.4763\n",
      "Epoch 928/1500\n",
      "592/592 [==============================] - 23s 38ms/step - loss: 0.6320 - mae: 0.6320 - val_loss: 0.9134 - val_mae: 0.9134\n",
      "Epoch 929/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.6256 - mae: 0.6256 - val_loss: 0.7462 - val_mae: 0.7462\n",
      "Epoch 930/1500\n",
      "592/592 [==============================] - 15s 25ms/step - loss: 0.5361 - mae: 0.5361 - val_loss: 0.8805 - val_mae: 0.8805\n",
      "Epoch 931/1500\n",
      "592/592 [==============================] - 18s 30ms/step - loss: 0.6780 - mae: 0.6780 - val_loss: 0.4260 - val_mae: 0.4260\n",
      "Epoch 932/1500\n",
      "592/592 [==============================] - 14s 24ms/step - loss: 0.5622 - mae: 0.5622 - val_loss: 0.4884 - val_mae: 0.4884\n",
      "Epoch 933/1500\n",
      "592/592 [==============================] - 20s 33ms/step - loss: 0.6264 - mae: 0.6264 - val_loss: 1.5217 - val_mae: 1.5217\n",
      "Epoch 934/1500\n",
      "592/592 [==============================] - 20s 33ms/step - loss: 0.6137 - mae: 0.6137 - val_loss: 0.3653 - val_mae: 0.3653\n",
      "Epoch 935/1500\n",
      "592/592 [==============================] - 12s 20ms/step - loss: 0.5406 - mae: 0.5406 - val_loss: 0.4830 - val_mae: 0.4830\n",
      "Epoch 936/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.6091 - mae: 0.6091 - val_loss: 0.5771 - val_mae: 0.5771\n",
      "Epoch 937/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5578 - mae: 0.5578 - val_loss: 0.8264 - val_mae: 0.8264\n",
      "Epoch 938/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5818 - mae: 0.5818 - val_loss: 0.5502 - val_mae: 0.5502\n",
      "Epoch 939/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.6429 - mae: 0.6429 - val_loss: 0.7830 - val_mae: 0.7830\n",
      "Epoch 940/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.6159 - mae: 0.6159 - val_loss: 0.7070 - val_mae: 0.7070\n",
      "Epoch 941/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6027 - mae: 0.6027 - val_loss: 0.5889 - val_mae: 0.5889\n",
      "Epoch 942/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5892 - mae: 0.5892 - val_loss: 0.9361 - val_mae: 0.9361\n",
      "Epoch 943/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5568 - mae: 0.5568 - val_loss: 0.3305 - val_mae: 0.3305\n",
      "Epoch 944/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.5358 - mae: 0.5358 - val_loss: 1.0666 - val_mae: 1.0666\n",
      "Epoch 945/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6602 - mae: 0.6602 - val_loss: 0.3187 - val_mae: 0.3187\n",
      "Epoch 946/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6074 - mae: 0.6074 - val_loss: 0.3366 - val_mae: 0.3366\n",
      "Epoch 947/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.6010 - mae: 0.6010 - val_loss: 0.3822 - val_mae: 0.3822\n",
      "Epoch 948/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5837 - mae: 0.5837 - val_loss: 0.5976 - val_mae: 0.5976\n",
      "Epoch 949/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.5940 - mae: 0.5940 - val_loss: 1.0972 - val_mae: 1.0972\n",
      "Epoch 950/1500\n",
      "592/592 [==============================] - 12s 21ms/step - loss: 0.6729 - mae: 0.6729 - val_loss: 0.8271 - val_mae: 0.8271\n",
      "Epoch 951/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.6439 - mae: 0.6439 - val_loss: 0.4959 - val_mae: 0.4959\n",
      "Epoch 952/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.5828 - mae: 0.5828 - val_loss: 0.3699 - val_mae: 0.3699\n",
      "Epoch 953/1500\n",
      "592/592 [==============================] - 12s 19ms/step - loss: 0.5263 - mae: 0.5263 - val_loss: 0.7646 - val_mae: 0.7646\n",
      "Epoch 954/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.6603 - mae: 0.6603 - val_loss: 0.9694 - val_mae: 0.9694\n",
      "Epoch 955/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5588 - mae: 0.5588 - val_loss: 0.2936 - val_mae: 0.2936\n",
      "Epoch 956/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5622 - mae: 0.5622 - val_loss: 0.4962 - val_mae: 0.4962\n",
      "Epoch 957/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6064 - mae: 0.6064 - val_loss: 0.3047 - val_mae: 0.3047\n",
      "Epoch 958/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5757 - mae: 0.5757 - val_loss: 0.4434 - val_mae: 0.4434\n",
      "Epoch 959/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5499 - mae: 0.5499 - val_loss: 0.3629 - val_mae: 0.3629\n",
      "Epoch 960/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5724 - mae: 0.5724 - val_loss: 0.8178 - val_mae: 0.8178\n",
      "Epoch 961/1500\n",
      "592/592 [==============================] - 12s 21ms/step - loss: 0.5547 - mae: 0.5547 - val_loss: 1.2482 - val_mae: 1.2482\n",
      "Epoch 962/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5553 - mae: 0.5553 - val_loss: 0.2892 - val_mae: 0.2892\n",
      "Epoch 963/1500\n",
      "592/592 [==============================] - 18s 31ms/step - loss: 0.6404 - mae: 0.6404 - val_loss: 0.4613 - val_mae: 0.4613\n",
      "Epoch 964/1500\n",
      "592/592 [==============================] - 20s 34ms/step - loss: 0.6035 - mae: 0.6035 - val_loss: 0.2792 - val_mae: 0.2792\n",
      "Epoch 965/1500\n",
      "592/592 [==============================] - 19s 33ms/step - loss: 0.5522 - mae: 0.5522 - val_loss: 0.6330 - val_mae: 0.6330\n",
      "Epoch 966/1500\n",
      "592/592 [==============================] - 20s 34ms/step - loss: 0.5469 - mae: 0.5469 - val_loss: 0.5977 - val_mae: 0.5977\n",
      "Epoch 967/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.6511 - mae: 0.6511 - val_loss: 0.5010 - val_mae: 0.5010\n",
      "Epoch 968/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.6661 - mae: 0.6661 - val_loss: 0.7799 - val_mae: 0.7799\n",
      "Epoch 969/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.6131 - mae: 0.6131 - val_loss: 0.5333 - val_mae: 0.5333\n",
      "Epoch 970/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5806 - mae: 0.5806 - val_loss: 0.5366 - val_mae: 0.5366\n",
      "Epoch 971/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.6403 - mae: 0.6403 - val_loss: 0.5382 - val_mae: 0.5382\n",
      "Epoch 972/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4586 - mae: 0.4586 - val_loss: 0.3417 - val_mae: 0.3417\n",
      "Epoch 973/1500\n",
      "592/592 [==============================] - 19s 32ms/step - loss: 0.6349 - mae: 0.6349 - val_loss: 0.6460 - val_mae: 0.6460\n",
      "Epoch 974/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.6326 - mae: 0.6326 - val_loss: 0.5959 - val_mae: 0.5959\n",
      "Epoch 975/1500\n",
      "592/592 [==============================] - 15s 26ms/step - loss: 0.5959 - mae: 0.5959 - val_loss: 0.6331 - val_mae: 0.6331\n",
      "Epoch 976/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.6045 - mae: 0.6045 - val_loss: 0.7679 - val_mae: 0.7679\n",
      "Epoch 977/1500\n",
      "592/592 [==============================] - 18s 31ms/step - loss: 0.5232 - mae: 0.5232 - val_loss: 0.9322 - val_mae: 0.9322\n",
      "Epoch 978/1500\n",
      "592/592 [==============================] - 19s 31ms/step - loss: 0.6215 - mae: 0.6215 - val_loss: 0.8311 - val_mae: 0.8311\n",
      "Epoch 979/1500\n",
      "592/592 [==============================] - 18s 31ms/step - loss: 0.5818 - mae: 0.5818 - val_loss: 0.4350 - val_mae: 0.4350\n",
      "Epoch 980/1500\n",
      "592/592 [==============================] - 35s 59ms/step - loss: 0.5440 - mae: 0.5440 - val_loss: 0.4101 - val_mae: 0.4101\n",
      "Epoch 981/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.6445 - mae: 0.6445 - val_loss: 0.4679 - val_mae: 0.4679\n",
      "Epoch 982/1500\n",
      "592/592 [==============================] - 19s 32ms/step - loss: 0.4844 - mae: 0.4844 - val_loss: 1.2377 - val_mae: 1.2377\n",
      "Epoch 983/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.6304 - mae: 0.6304 - val_loss: 0.3626 - val_mae: 0.3626\n",
      "Epoch 984/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.5627 - mae: 0.5627 - val_loss: 0.5460 - val_mae: 0.5460\n",
      "Epoch 985/1500\n",
      "592/592 [==============================] - 16s 26ms/step - loss: 0.6192 - mae: 0.6192 - val_loss: 0.2705 - val_mae: 0.2705\n",
      "Epoch 986/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5974 - mae: 0.5974 - val_loss: 0.4253 - val_mae: 0.4253\n",
      "Epoch 987/1500\n",
      "592/592 [==============================] - 15s 26ms/step - loss: 0.6276 - mae: 0.6276 - val_loss: 0.4748 - val_mae: 0.4748\n",
      "Epoch 988/1500\n",
      "592/592 [==============================] - 15s 26ms/step - loss: 0.5791 - mae: 0.5791 - val_loss: 0.3600 - val_mae: 0.3600\n",
      "Epoch 989/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.5034 - mae: 0.5034 - val_loss: 0.3554 - val_mae: 0.3554\n",
      "Epoch 990/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.6223 - mae: 0.6223 - val_loss: 1.2405 - val_mae: 1.2405\n",
      "Epoch 991/1500\n",
      "592/592 [==============================] - 15s 26ms/step - loss: 0.6544 - mae: 0.6544 - val_loss: 0.9017 - val_mae: 0.9017\n",
      "Epoch 992/1500\n",
      "592/592 [==============================] - 22s 38ms/step - loss: 0.5227 - mae: 0.5227 - val_loss: 0.4759 - val_mae: 0.4759\n",
      "Epoch 993/1500\n",
      "592/592 [==============================] - 21s 36ms/step - loss: 0.6051 - mae: 0.6051 - val_loss: 0.3017 - val_mae: 0.3017\n",
      "Epoch 994/1500\n",
      "592/592 [==============================] - 16s 28ms/step - loss: 0.5321 - mae: 0.5321 - val_loss: 0.7419 - val_mae: 0.7419\n",
      "Epoch 995/1500\n",
      "592/592 [==============================] - 14s 24ms/step - loss: 0.5861 - mae: 0.5861 - val_loss: 0.4308 - val_mae: 0.4308\n",
      "Epoch 996/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.5672 - mae: 0.5672 - val_loss: 0.7079 - val_mae: 0.7079\n",
      "Epoch 997/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.6284 - mae: 0.6284 - val_loss: 0.9432 - val_mae: 0.9432\n",
      "Epoch 998/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5413 - mae: 0.5413 - val_loss: 0.3065 - val_mae: 0.3065\n",
      "Epoch 999/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5490 - mae: 0.5490 - val_loss: 0.4602 - val_mae: 0.4602\n",
      "Epoch 1000/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4938 - mae: 0.4938 - val_loss: 0.3403 - val_mae: 0.3403\n",
      "Epoch 1001/1500\n",
      "592/592 [==============================] - 12s 20ms/step - loss: 0.5583 - mae: 0.5583 - val_loss: 0.4152 - val_mae: 0.4152\n",
      "Epoch 1002/1500\n",
      "592/592 [==============================] - 19s 32ms/step - loss: 0.5745 - mae: 0.5745 - val_loss: 0.6594 - val_mae: 0.6594\n",
      "Epoch 1003/1500\n",
      "592/592 [==============================] - 23s 39ms/step - loss: 0.5529 - mae: 0.5529 - val_loss: 0.6709 - val_mae: 0.6709\n",
      "Epoch 1004/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.5910 - mae: 0.5910 - val_loss: 0.4392 - val_mae: 0.4392\n",
      "Epoch 1005/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.4962 - mae: 0.4962 - val_loss: 0.9900 - val_mae: 0.9900\n",
      "Epoch 1006/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6687 - mae: 0.6687 - val_loss: 0.7718 - val_mae: 0.7718\n",
      "Epoch 1007/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.5608 - mae: 0.5608 - val_loss: 0.2884 - val_mae: 0.2884\n",
      "Epoch 1008/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5656 - mae: 0.5656 - val_loss: 0.3586 - val_mae: 0.3586\n",
      "Epoch 1009/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.5724 - mae: 0.5724 - val_loss: 0.4174 - val_mae: 0.4174\n",
      "Epoch 1010/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6008 - mae: 0.6008 - val_loss: 0.7501 - val_mae: 0.7501\n",
      "Epoch 1011/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.6455 - mae: 0.6455 - val_loss: 0.3193 - val_mae: 0.3193\n",
      "Epoch 1012/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6910 - mae: 0.6910 - val_loss: 0.4329 - val_mae: 0.4329\n",
      "Epoch 1013/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.5319 - mae: 0.5319 - val_loss: 0.6665 - val_mae: 0.6665\n",
      "Epoch 1014/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5963 - mae: 0.5963 - val_loss: 0.7033 - val_mae: 0.7033\n",
      "Epoch 1015/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5419 - mae: 0.5419 - val_loss: 0.4418 - val_mae: 0.4418\n",
      "Epoch 1016/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5373 - mae: 0.5373 - val_loss: 0.4160 - val_mae: 0.4160\n",
      "Epoch 1017/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.6141 - mae: 0.6141 - val_loss: 0.7641 - val_mae: 0.7641\n",
      "Epoch 1018/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5404 - mae: 0.5404 - val_loss: 0.4824 - val_mae: 0.4824\n",
      "Epoch 1019/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5483 - mae: 0.5483 - val_loss: 0.4393 - val_mae: 0.4393\n",
      "Epoch 1020/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.6009 - mae: 0.6009 - val_loss: 0.3127 - val_mae: 0.3127\n",
      "Epoch 1021/1500\n",
      "592/592 [==============================] - 10s 16ms/step - loss: 0.5297 - mae: 0.5297 - val_loss: 0.6243 - val_mae: 0.6243\n",
      "Epoch 1022/1500\n",
      "592/592 [==============================] - 14s 23ms/step - loss: 0.5805 - mae: 0.5805 - val_loss: 0.6449 - val_mae: 0.6449\n",
      "Epoch 1023/1500\n",
      "592/592 [==============================] - 23s 38ms/step - loss: 0.7533 - mae: 0.7533 - val_loss: 1.1921 - val_mae: 1.1921\n",
      "Epoch 1024/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5795 - mae: 0.5795 - val_loss: 0.9195 - val_mae: 0.9195\n",
      "Epoch 1025/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.5155 - mae: 0.5155 - val_loss: 0.4331 - val_mae: 0.4331\n",
      "Epoch 1026/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5669 - mae: 0.5669 - val_loss: 0.8344 - val_mae: 0.8344\n",
      "Epoch 1027/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5328 - mae: 0.5328 - val_loss: 0.5126 - val_mae: 0.5126\n",
      "Epoch 1028/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.5329 - mae: 0.5329 - val_loss: 0.2983 - val_mae: 0.2983\n",
      "Epoch 1029/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6611 - mae: 0.6611 - val_loss: 0.4431 - val_mae: 0.4431\n",
      "Epoch 1030/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.4998 - mae: 0.4998 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 1031/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5257 - mae: 0.5257 - val_loss: 0.3276 - val_mae: 0.3276\n",
      "Epoch 1032/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5957 - mae: 0.5957 - val_loss: 0.5618 - val_mae: 0.5618\n",
      "Epoch 1033/1500\n",
      "592/592 [==============================] - 4s 8ms/step - loss: 0.5665 - mae: 0.5665 - val_loss: 0.4172 - val_mae: 0.4172\n",
      "Epoch 1034/1500\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5687 - mae: 0.5687 - val_loss: 0.6662 - val_mae: 0.6662\n",
      "Epoch 1035/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5443 - mae: 0.5443 - val_loss: 0.6013 - val_mae: 0.6013\n",
      "Epoch 1036/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5844 - mae: 0.5844 - val_loss: 0.3312 - val_mae: 0.3312\n",
      "Epoch 1037/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5560 - mae: 0.5560 - val_loss: 0.3115 - val_mae: 0.3115\n",
      "Epoch 1038/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.5975 - mae: 0.5975 - val_loss: 0.3058 - val_mae: 0.3058\n",
      "Epoch 1039/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.5925 - mae: 0.5925 - val_loss: 0.8032 - val_mae: 0.8032\n",
      "Epoch 1040/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5487 - mae: 0.5487 - val_loss: 0.7698 - val_mae: 0.7698\n",
      "Epoch 1041/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5697 - mae: 0.5697 - val_loss: 0.6502 - val_mae: 0.6502\n",
      "Epoch 1042/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6882 - mae: 0.6882 - val_loss: 1.4847 - val_mae: 1.4847\n",
      "Epoch 1043/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.5875 - mae: 0.5875 - val_loss: 0.6084 - val_mae: 0.6084\n",
      "Epoch 1044/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4994 - mae: 0.4994 - val_loss: 0.3840 - val_mae: 0.3840\n",
      "Epoch 1045/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.6115 - mae: 0.6115 - val_loss: 0.4490 - val_mae: 0.4490\n",
      "Epoch 1046/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5061 - mae: 0.5061 - val_loss: 0.3182 - val_mae: 0.3182\n",
      "Epoch 1047/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4964 - mae: 0.4964 - val_loss: 0.7103 - val_mae: 0.7103\n",
      "Epoch 1048/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5719 - mae: 0.5719 - val_loss: 0.9536 - val_mae: 0.9536\n",
      "Epoch 1049/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.5847 - mae: 0.5847 - val_loss: 0.4187 - val_mae: 0.4187\n",
      "Epoch 1050/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.6520 - mae: 0.6520 - val_loss: 0.2863 - val_mae: 0.2863\n",
      "Epoch 1051/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5448 - mae: 0.5448 - val_loss: 0.4497 - val_mae: 0.4497\n",
      "Epoch 1052/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5006 - mae: 0.5006 - val_loss: 0.3630 - val_mae: 0.3630\n",
      "Epoch 1053/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5523 - mae: 0.5523 - val_loss: 0.4473 - val_mae: 0.4473\n",
      "Epoch 1054/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.5472 - mae: 0.5472 - val_loss: 0.4308 - val_mae: 0.4308\n",
      "Epoch 1055/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.5298 - mae: 0.5298 - val_loss: 0.2931 - val_mae: 0.2931\n",
      "Epoch 1056/1500\n",
      "592/592 [==============================] - 26s 44ms/step - loss: 0.5730 - mae: 0.5730 - val_loss: 0.5864 - val_mae: 0.5864\n",
      "Epoch 1057/1500\n",
      "592/592 [==============================] - 30s 50ms/step - loss: 0.6041 - mae: 0.6041 - val_loss: 0.4072 - val_mae: 0.4072\n",
      "Epoch 1058/1500\n",
      "592/592 [==============================] - 34s 58ms/step - loss: 0.5917 - mae: 0.5917 - val_loss: 0.3681 - val_mae: 0.3681\n",
      "Epoch 1059/1500\n",
      "592/592 [==============================] - 31s 52ms/step - loss: 0.5028 - mae: 0.5028 - val_loss: 0.2758 - val_mae: 0.2758\n",
      "Epoch 1060/1500\n",
      "592/592 [==============================] - 20s 34ms/step - loss: 0.5667 - mae: 0.5667 - val_loss: 0.3261 - val_mae: 0.3261\n",
      "Epoch 1061/1500\n",
      "592/592 [==============================] - 19s 32ms/step - loss: 0.5717 - mae: 0.5717 - val_loss: 0.3632 - val_mae: 0.3632\n",
      "Epoch 1062/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5637 - mae: 0.5637 - val_loss: 1.0940 - val_mae: 1.0940\n",
      "Epoch 1063/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.5146 - mae: 0.5146 - val_loss: 0.6174 - val_mae: 0.6174\n",
      "Epoch 1064/1500\n",
      "592/592 [==============================] - 18s 31ms/step - loss: 0.4885 - mae: 0.4885 - val_loss: 0.7047 - val_mae: 0.7047\n",
      "Epoch 1065/1500\n",
      "592/592 [==============================] - 19s 31ms/step - loss: 0.5850 - mae: 0.5850 - val_loss: 0.4385 - val_mae: 0.4385\n",
      "Epoch 1066/1500\n",
      "592/592 [==============================] - 16s 28ms/step - loss: 0.5570 - mae: 0.5570 - val_loss: 0.4458 - val_mae: 0.4458\n",
      "Epoch 1067/1500\n",
      "592/592 [==============================] - 20s 34ms/step - loss: 0.6620 - mae: 0.6620 - val_loss: 0.4405 - val_mae: 0.4405\n",
      "Epoch 1068/1500\n",
      "592/592 [==============================] - 19s 33ms/step - loss: 0.5373 - mae: 0.5373 - val_loss: 0.3482 - val_mae: 0.3482\n",
      "Epoch 1069/1500\n",
      "592/592 [==============================] - 23s 39ms/step - loss: 0.5268 - mae: 0.5268 - val_loss: 0.4969 - val_mae: 0.4969\n",
      "Epoch 1070/1500\n",
      "592/592 [==============================] - 23s 40ms/step - loss: 0.5301 - mae: 0.5301 - val_loss: 0.6261 - val_mae: 0.6261\n",
      "Epoch 1071/1500\n",
      "592/592 [==============================] - 23s 39ms/step - loss: 0.4972 - mae: 0.4972 - val_loss: 0.6830 - val_mae: 0.6830\n",
      "Epoch 1072/1500\n",
      "592/592 [==============================] - 31s 53ms/step - loss: 0.5737 - mae: 0.5737 - val_loss: 0.4700 - val_mae: 0.4700\n",
      "Epoch 1073/1500\n",
      "592/592 [==============================] - 33s 55ms/step - loss: 0.5377 - mae: 0.5377 - val_loss: 0.7147 - val_mae: 0.7147\n",
      "Epoch 1074/1500\n",
      "592/592 [==============================] - 26s 44ms/step - loss: 0.5301 - mae: 0.5301 - val_loss: 0.2605 - val_mae: 0.2605\n",
      "Epoch 1075/1500\n",
      "592/592 [==============================] - 22s 37ms/step - loss: 0.4870 - mae: 0.4870 - val_loss: 1.1220 - val_mae: 1.1220\n",
      "Epoch 1076/1500\n",
      "592/592 [==============================] - 22s 37ms/step - loss: 0.5188 - mae: 0.5188 - val_loss: 0.3266 - val_mae: 0.3266\n",
      "Epoch 1077/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.6295 - mae: 0.6295 - val_loss: 0.2713 - val_mae: 0.2713\n",
      "Epoch 1078/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5889 - mae: 0.5889 - val_loss: 0.7477 - val_mae: 0.7477\n",
      "Epoch 1079/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5841 - mae: 0.5841 - val_loss: 1.1358 - val_mae: 1.1358\n",
      "Epoch 1080/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.5356 - mae: 0.5356 - val_loss: 0.6280 - val_mae: 0.6280\n",
      "Epoch 1081/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5410 - mae: 0.5410 - val_loss: 0.3011 - val_mae: 0.3011\n",
      "Epoch 1082/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.5159 - mae: 0.5159 - val_loss: 0.5975 - val_mae: 0.5975\n",
      "Epoch 1083/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4853 - mae: 0.4853 - val_loss: 0.2982 - val_mae: 0.2982\n",
      "Epoch 1084/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5881 - mae: 0.5881 - val_loss: 0.3697 - val_mae: 0.3697\n",
      "Epoch 1085/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5270 - mae: 0.5270 - val_loss: 0.5147 - val_mae: 0.5147\n",
      "Epoch 1086/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5692 - mae: 0.5692 - val_loss: 0.6737 - val_mae: 0.6737\n",
      "Epoch 1087/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4963 - mae: 0.4963 - val_loss: 0.6075 - val_mae: 0.6075\n",
      "Epoch 1088/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5037 - mae: 0.5037 - val_loss: 0.5693 - val_mae: 0.5693\n",
      "Epoch 1089/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5602 - mae: 0.5602 - val_loss: 0.5085 - val_mae: 0.5085\n",
      "Epoch 1090/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5266 - mae: 0.5266 - val_loss: 0.8753 - val_mae: 0.8753\n",
      "Epoch 1091/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5972 - mae: 0.5972 - val_loss: 0.4323 - val_mae: 0.4323\n",
      "Epoch 1092/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.6028 - mae: 0.6028 - val_loss: 0.7898 - val_mae: 0.7898\n",
      "Epoch 1093/1500\n",
      "592/592 [==============================] - 6s 9ms/step - loss: 0.6352 - mae: 0.6352 - val_loss: 0.6596 - val_mae: 0.6596\n",
      "Epoch 1094/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5549 - mae: 0.5549 - val_loss: 0.6371 - val_mae: 0.6371\n",
      "Epoch 1095/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.5153 - mae: 0.5153 - val_loss: 0.7715 - val_mae: 0.7715\n",
      "Epoch 1096/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5064 - mae: 0.5064 - val_loss: 0.3082 - val_mae: 0.3082\n",
      "Epoch 1097/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5548 - mae: 0.5548 - val_loss: 0.6083 - val_mae: 0.6083\n",
      "Epoch 1098/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5280 - mae: 0.5280 - val_loss: 0.2524 - val_mae: 0.2524\n",
      "Epoch 1099/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5741 - mae: 0.5741 - val_loss: 0.3974 - val_mae: 0.3974\n",
      "Epoch 1100/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5123 - mae: 0.5123 - val_loss: 0.2561 - val_mae: 0.2561\n",
      "Epoch 1101/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5457 - mae: 0.5457 - val_loss: 0.8733 - val_mae: 0.8733\n",
      "Epoch 1102/1500\n",
      "592/592 [==============================] - 10s 16ms/step - loss: 0.5182 - mae: 0.5182 - val_loss: 0.2777 - val_mae: 0.2777\n",
      "Epoch 1103/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.5668 - mae: 0.5668 - val_loss: 0.2772 - val_mae: 0.2772\n",
      "Epoch 1104/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4839 - mae: 0.4839 - val_loss: 1.1132 - val_mae: 1.1132\n",
      "Epoch 1105/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5808 - mae: 0.5808 - val_loss: 0.3321 - val_mae: 0.3321\n",
      "Epoch 1106/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5090 - mae: 0.5090 - val_loss: 0.4947 - val_mae: 0.4947\n",
      "Epoch 1107/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4612 - mae: 0.4612 - val_loss: 0.3113 - val_mae: 0.3113\n",
      "Epoch 1108/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.5626 - mae: 0.5626 - val_loss: 1.2665 - val_mae: 1.2665\n",
      "Epoch 1109/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5400 - mae: 0.5400 - val_loss: 0.6982 - val_mae: 0.6982\n",
      "Epoch 1110/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.5234 - mae: 0.5234 - val_loss: 0.3323 - val_mae: 0.3323\n",
      "Epoch 1111/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.5278 - mae: 0.5278 - val_loss: 0.3441 - val_mae: 0.3441\n",
      "Epoch 1112/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4810 - mae: 0.4810 - val_loss: 0.8396 - val_mae: 0.8396\n",
      "Epoch 1113/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5662 - mae: 0.5662 - val_loss: 0.2941 - val_mae: 0.2941\n",
      "Epoch 1114/1500\n",
      "592/592 [==============================] - 9s 14ms/step - loss: 0.4940 - mae: 0.4940 - val_loss: 0.4917 - val_mae: 0.4917\n",
      "Epoch 1115/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5177 - mae: 0.5177 - val_loss: 0.6890 - val_mae: 0.6890\n",
      "Epoch 1116/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.6353 - mae: 0.6353 - val_loss: 0.6199 - val_mae: 0.6199\n",
      "Epoch 1117/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.5931 - mae: 0.5931 - val_loss: 0.3301 - val_mae: 0.3301\n",
      "Epoch 1118/1500\n",
      "592/592 [==============================] - 10s 16ms/step - loss: 0.4574 - mae: 0.4574 - val_loss: 0.2747 - val_mae: 0.2747\n",
      "Epoch 1119/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.5430 - mae: 0.5430 - val_loss: 0.5267 - val_mae: 0.5267\n",
      "Epoch 1120/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.6939 - mae: 0.6939 - val_loss: 0.7115 - val_mae: 0.7115\n",
      "Epoch 1121/1500\n",
      "592/592 [==============================] - 10s 16ms/step - loss: 0.5568 - mae: 0.5568 - val_loss: 0.8784 - val_mae: 0.8784\n",
      "Epoch 1122/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.5658 - mae: 0.5658 - val_loss: 0.9110 - val_mae: 0.9110\n",
      "Epoch 1123/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.5257 - mae: 0.5257 - val_loss: 0.8907 - val_mae: 0.8907\n",
      "Epoch 1124/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.5043 - mae: 0.5043 - val_loss: 0.4330 - val_mae: 0.4330\n",
      "Epoch 1125/1500\n",
      "592/592 [==============================] - 10s 16ms/step - loss: 0.5102 - mae: 0.5102 - val_loss: 0.6417 - val_mae: 0.6417\n",
      "Epoch 1126/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.4761 - mae: 0.4761 - val_loss: 0.5168 - val_mae: 0.5168\n",
      "Epoch 1127/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.5255 - mae: 0.5255 - val_loss: 0.7942 - val_mae: 0.7942\n",
      "Epoch 1128/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.5040 - mae: 0.5040 - val_loss: 0.2653 - val_mae: 0.2653\n",
      "Epoch 1129/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.4823 - mae: 0.4823 - val_loss: 0.5695 - val_mae: 0.5695\n",
      "Epoch 1130/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.4650 - mae: 0.4650 - val_loss: 0.2709 - val_mae: 0.2709\n",
      "Epoch 1131/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.6892 - mae: 0.6892 - val_loss: 0.4192 - val_mae: 0.4192\n",
      "Epoch 1132/1500\n",
      "592/592 [==============================] - 12s 21ms/step - loss: 0.5822 - mae: 0.5822 - val_loss: 0.7329 - val_mae: 0.7329\n",
      "Epoch 1133/1500\n",
      "592/592 [==============================] - 12s 21ms/step - loss: 0.5851 - mae: 0.5851 - val_loss: 0.6346 - val_mae: 0.6346\n",
      "Epoch 1134/1500\n",
      "592/592 [==============================] - 14s 24ms/step - loss: 0.5401 - mae: 0.5401 - val_loss: 0.3654 - val_mae: 0.3654\n",
      "Epoch 1135/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5342 - mae: 0.5342 - val_loss: 1.2362 - val_mae: 1.2362\n",
      "Epoch 1136/1500\n",
      "592/592 [==============================] - 15s 26ms/step - loss: 0.5852 - mae: 0.5852 - val_loss: 1.2485 - val_mae: 1.2485\n",
      "Epoch 1137/1500\n",
      "592/592 [==============================] - 15s 25ms/step - loss: 0.5482 - mae: 0.5482 - val_loss: 0.3719 - val_mae: 0.3719\n",
      "Epoch 1138/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5475 - mae: 0.5475 - val_loss: 0.2715 - val_mae: 0.2715\n",
      "Epoch 1139/1500\n",
      "592/592 [==============================] - 16s 26ms/step - loss: 0.5269 - mae: 0.5269 - val_loss: 1.5294 - val_mae: 1.5294\n",
      "Epoch 1140/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5957 - mae: 0.5957 - val_loss: 0.6096 - val_mae: 0.6096\n",
      "Epoch 1141/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5147 - mae: 0.5147 - val_loss: 0.2391 - val_mae: 0.2391\n",
      "Epoch 1142/1500\n",
      "592/592 [==============================] - 19s 33ms/step - loss: 0.5281 - mae: 0.5281 - val_loss: 0.5724 - val_mae: 0.5724\n",
      "Epoch 1143/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.5844 - mae: 0.5844 - val_loss: 0.5748 - val_mae: 0.5748\n",
      "Epoch 1144/1500\n",
      "592/592 [==============================] - 16s 26ms/step - loss: 0.5079 - mae: 0.5079 - val_loss: 0.7887 - val_mae: 0.7887\n",
      "Epoch 1145/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.4945 - mae: 0.4945 - val_loss: 0.5594 - val_mae: 0.5594\n",
      "Epoch 1146/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6837 - mae: 0.6837 - val_loss: 0.2616 - val_mae: 0.2616\n",
      "Epoch 1147/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5178 - mae: 0.5178 - val_loss: 0.4382 - val_mae: 0.4382\n",
      "Epoch 1148/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5201 - mae: 0.5201 - val_loss: 0.5141 - val_mae: 0.5141\n",
      "Epoch 1149/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5333 - mae: 0.5333 - val_loss: 0.2897 - val_mae: 0.2897\n",
      "Epoch 1150/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5641 - mae: 0.5641 - val_loss: 0.4072 - val_mae: 0.4072\n",
      "Epoch 1151/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.6287 - mae: 0.6287 - val_loss: 0.2569 - val_mae: 0.2569\n",
      "Epoch 1152/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4968 - mae: 0.4968 - val_loss: 0.5759 - val_mae: 0.5759\n",
      "Epoch 1153/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.5484 - mae: 0.5484 - val_loss: 0.2732 - val_mae: 0.2732\n",
      "Epoch 1154/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.4769 - mae: 0.4769 - val_loss: 0.4588 - val_mae: 0.4588\n",
      "Epoch 1155/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.4804 - mae: 0.4804 - val_loss: 0.5685 - val_mae: 0.5685\n",
      "Epoch 1156/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5581 - mae: 0.5581 - val_loss: 0.8718 - val_mae: 0.8718\n",
      "Epoch 1157/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5169 - mae: 0.5169 - val_loss: 0.6427 - val_mae: 0.6427\n",
      "Epoch 1158/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5613 - mae: 0.5613 - val_loss: 0.7108 - val_mae: 0.7108\n",
      "Epoch 1159/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.4889 - mae: 0.4889 - val_loss: 0.6353 - val_mae: 0.6353\n",
      "Epoch 1160/1500\n",
      "592/592 [==============================] - 12s 20ms/step - loss: 0.5235 - mae: 0.5235 - val_loss: 0.7751 - val_mae: 0.7751\n",
      "Epoch 1161/1500\n",
      "592/592 [==============================] - 14s 24ms/step - loss: 0.5567 - mae: 0.5567 - val_loss: 0.6176 - val_mae: 0.6176\n",
      "Epoch 1162/1500\n",
      "592/592 [==============================] - 12s 21ms/step - loss: 0.4723 - mae: 0.4723 - val_loss: 0.4513 - val_mae: 0.4513\n",
      "Epoch 1163/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.4997 - mae: 0.4997 - val_loss: 0.7041 - val_mae: 0.7041\n",
      "Epoch 1164/1500\n",
      "592/592 [==============================] - 12s 20ms/step - loss: 0.5385 - mae: 0.5385 - val_loss: 0.7241 - val_mae: 0.7241\n",
      "Epoch 1165/1500\n",
      "592/592 [==============================] - 17s 30ms/step - loss: 0.4848 - mae: 0.4848 - val_loss: 0.2437 - val_mae: 0.2437\n",
      "Epoch 1166/1500\n",
      "592/592 [==============================] - 16s 28ms/step - loss: 0.5309 - mae: 0.5309 - val_loss: 0.2650 - val_mae: 0.2650\n",
      "Epoch 1167/1500\n",
      "592/592 [==============================] - 15s 25ms/step - loss: 0.5259 - mae: 0.5259 - val_loss: 1.2671 - val_mae: 1.2671\n",
      "Epoch 1168/1500\n",
      "592/592 [==============================] - 13s 21ms/step - loss: 0.5169 - mae: 0.5169 - val_loss: 0.6362 - val_mae: 0.6362\n",
      "Epoch 1169/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.5253 - mae: 0.5253 - val_loss: 0.3767 - val_mae: 0.3767\n",
      "Epoch 1170/1500\n",
      "592/592 [==============================] - 13s 22ms/step - loss: 0.5099 - mae: 0.5099 - val_loss: 0.9564 - val_mae: 0.9564\n",
      "Epoch 1171/1500\n",
      "592/592 [==============================] - 13s 21ms/step - loss: 0.5258 - mae: 0.5258 - val_loss: 0.2291 - val_mae: 0.2291\n",
      "Epoch 1172/1500\n",
      "592/592 [==============================] - 12s 20ms/step - loss: 0.4748 - mae: 0.4748 - val_loss: 0.3398 - val_mae: 0.3398\n",
      "Epoch 1173/1500\n",
      "592/592 [==============================] - 17s 28ms/step - loss: 0.6385 - mae: 0.6385 - val_loss: 0.4469 - val_mae: 0.4469\n",
      "Epoch 1174/1500\n",
      "592/592 [==============================] - 18s 30ms/step - loss: 0.5060 - mae: 0.5060 - val_loss: 0.2870 - val_mae: 0.2870\n",
      "Epoch 1175/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.4893 - mae: 0.4893 - val_loss: 0.5612 - val_mae: 0.5612\n",
      "Epoch 1176/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4869 - mae: 0.4869 - val_loss: 0.2670 - val_mae: 0.2670\n",
      "Epoch 1177/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5263 - mae: 0.5263 - val_loss: 0.2426 - val_mae: 0.2426\n",
      "Epoch 1178/1500\n",
      "592/592 [==============================] - 19s 32ms/step - loss: 0.5091 - mae: 0.5091 - val_loss: 1.4511 - val_mae: 1.4511\n",
      "Epoch 1179/1500\n",
      "592/592 [==============================] - 23s 39ms/step - loss: 0.5440 - mae: 0.5440 - val_loss: 0.3909 - val_mae: 0.3909\n",
      "Epoch 1180/1500\n",
      "592/592 [==============================] - 17s 28ms/step - loss: 0.5361 - mae: 0.5361 - val_loss: 0.5557 - val_mae: 0.5557\n",
      "Epoch 1181/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.5369 - mae: 0.5369 - val_loss: 0.6160 - val_mae: 0.6160\n",
      "Epoch 1182/1500\n",
      "592/592 [==============================] - 19s 32ms/step - loss: 0.5209 - mae: 0.5209 - val_loss: 0.3599 - val_mae: 0.3599\n",
      "Epoch 1183/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5421 - mae: 0.5421 - val_loss: 0.5252 - val_mae: 0.5252\n",
      "Epoch 1184/1500\n",
      "592/592 [==============================] - 16s 28ms/step - loss: 0.5026 - mae: 0.5026 - val_loss: 0.8601 - val_mae: 0.8601\n",
      "Epoch 1185/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.4601 - mae: 0.4601 - val_loss: 0.5376 - val_mae: 0.5376\n",
      "Epoch 1186/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.4742 - mae: 0.4742 - val_loss: 0.3768 - val_mae: 0.3768\n",
      "Epoch 1187/1500\n",
      "592/592 [==============================] - 15s 25ms/step - loss: 0.5306 - mae: 0.5306 - val_loss: 0.4979 - val_mae: 0.4979\n",
      "Epoch 1188/1500\n",
      "592/592 [==============================] - 17s 28ms/step - loss: 0.4794 - mae: 0.4794 - val_loss: 1.1061 - val_mae: 1.1061\n",
      "Epoch 1189/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.5339 - mae: 0.5339 - val_loss: 0.6507 - val_mae: 0.6507\n",
      "Epoch 1190/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.5642 - mae: 0.5642 - val_loss: 0.8974 - val_mae: 0.8974\n",
      "Epoch 1191/1500\n",
      "592/592 [==============================] - 18s 31ms/step - loss: 0.5542 - mae: 0.5542 - val_loss: 0.5155 - val_mae: 0.5155\n",
      "Epoch 1192/1500\n",
      "592/592 [==============================] - 16s 27ms/step - loss: 0.5505 - mae: 0.5505 - val_loss: 0.6719 - val_mae: 0.6719\n",
      "Epoch 1193/1500\n",
      "592/592 [==============================] - 18s 30ms/step - loss: 0.4966 - mae: 0.4966 - val_loss: 0.3854 - val_mae: 0.3854\n",
      "Epoch 1194/1500\n",
      "592/592 [==============================] - 17s 29ms/step - loss: 0.5711 - mae: 0.5711 - val_loss: 0.7631 - val_mae: 0.7631\n",
      "Epoch 1195/1500\n",
      "592/592 [==============================] - 17s 28ms/step - loss: 0.5960 - mae: 0.5960 - val_loss: 0.4386 - val_mae: 0.4386\n",
      "Epoch 1196/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.5367 - mae: 0.5367 - val_loss: 0.3001 - val_mae: 0.3001\n",
      "Epoch 1197/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.5594 - mae: 0.5594 - val_loss: 0.3449 - val_mae: 0.3449\n",
      "Epoch 1198/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.5679 - mae: 0.5679 - val_loss: 0.7363 - val_mae: 0.7363\n",
      "Epoch 1199/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5014 - mae: 0.5014 - val_loss: 0.3535 - val_mae: 0.3535\n",
      "Epoch 1200/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4883 - mae: 0.4883 - val_loss: 0.7692 - val_mae: 0.7692\n",
      "Epoch 1201/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5597 - mae: 0.5597 - val_loss: 0.5443 - val_mae: 0.5443\n",
      "Epoch 1202/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5086 - mae: 0.5086 - val_loss: 0.7346 - val_mae: 0.7346\n",
      "Epoch 1203/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5263 - mae: 0.5263 - val_loss: 0.5764 - val_mae: 0.5764\n",
      "Epoch 1204/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.4898 - mae: 0.4898 - val_loss: 0.4347 - val_mae: 0.4347\n",
      "Epoch 1205/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5376 - val_mae: 0.5376\n",
      "Epoch 1206/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5172 - mae: 0.5172 - val_loss: 0.3206 - val_mae: 0.3206\n",
      "Epoch 1207/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5235 - mae: 0.5235 - val_loss: 0.6179 - val_mae: 0.6179\n",
      "Epoch 1208/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5052 - mae: 0.5052 - val_loss: 0.5791 - val_mae: 0.5791\n",
      "Epoch 1209/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5138 - mae: 0.5138 - val_loss: 0.5879 - val_mae: 0.5879\n",
      "Epoch 1210/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5201 - mae: 0.5201 - val_loss: 0.8093 - val_mae: 0.8093\n",
      "Epoch 1211/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.6228 - mae: 0.6228 - val_loss: 0.2544 - val_mae: 0.2544\n",
      "Epoch 1212/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4702 - mae: 0.4702 - val_loss: 0.4826 - val_mae: 0.4826\n",
      "Epoch 1213/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5204 - mae: 0.5204 - val_loss: 0.3135 - val_mae: 0.3135\n",
      "Epoch 1214/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5552 - mae: 0.5552 - val_loss: 0.3070 - val_mae: 0.3070\n",
      "Epoch 1215/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.6564 - mae: 0.6564 - val_loss: 0.8142 - val_mae: 0.8142\n",
      "Epoch 1216/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4755 - mae: 0.4755 - val_loss: 0.7273 - val_mae: 0.7273\n",
      "Epoch 1217/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5194 - mae: 0.5194 - val_loss: 0.8189 - val_mae: 0.8189\n",
      "Epoch 1218/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5635 - mae: 0.5635 - val_loss: 0.3919 - val_mae: 0.3919\n",
      "Epoch 1219/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5342 - mae: 0.5342 - val_loss: 0.4849 - val_mae: 0.4849\n",
      "Epoch 1220/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4983 - mae: 0.4983 - val_loss: 0.2792 - val_mae: 0.2792\n",
      "Epoch 1221/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4793 - mae: 0.4793 - val_loss: 0.4450 - val_mae: 0.4450\n",
      "Epoch 1222/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4754 - mae: 0.4754 - val_loss: 0.2926 - val_mae: 0.2926\n",
      "Epoch 1223/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5399 - mae: 0.5399 - val_loss: 0.3935 - val_mae: 0.3935\n",
      "Epoch 1224/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5413 - mae: 0.5413 - val_loss: 0.2799 - val_mae: 0.2799\n",
      "Epoch 1225/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5014 - mae: 0.5014 - val_loss: 0.2652 - val_mae: 0.2652\n",
      "Epoch 1226/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5073 - mae: 0.5073 - val_loss: 0.3174 - val_mae: 0.3174\n",
      "Epoch 1227/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5799 - mae: 0.5799 - val_loss: 0.3784 - val_mae: 0.3784\n",
      "Epoch 1228/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4950 - mae: 0.4950 - val_loss: 0.6479 - val_mae: 0.6479\n",
      "Epoch 1229/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5253 - mae: 0.5253 - val_loss: 0.3612 - val_mae: 0.3612\n",
      "Epoch 1230/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5371 - mae: 0.5371 - val_loss: 0.7052 - val_mae: 0.7052\n",
      "Epoch 1231/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4510 - mae: 0.4510 - val_loss: 0.5830 - val_mae: 0.5830\n",
      "Epoch 1232/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4768 - mae: 0.4768 - val_loss: 0.2600 - val_mae: 0.2600\n",
      "Epoch 1233/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4658 - mae: 0.4658 - val_loss: 0.4188 - val_mae: 0.4188\n",
      "Epoch 1234/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4872 - mae: 0.4872 - val_loss: 0.8454 - val_mae: 0.8454\n",
      "Epoch 1235/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.5063 - mae: 0.5063 - val_loss: 0.6478 - val_mae: 0.6478\n",
      "Epoch 1236/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5268 - mae: 0.5268 - val_loss: 0.7035 - val_mae: 0.7035\n",
      "Epoch 1237/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4643 - mae: 0.4643 - val_loss: 0.4605 - val_mae: 0.4605\n",
      "Epoch 1238/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5215 - mae: 0.5215 - val_loss: 0.5232 - val_mae: 0.5232\n",
      "Epoch 1239/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5564 - mae: 0.5564 - val_loss: 0.2954 - val_mae: 0.2954\n",
      "Epoch 1240/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6041 - mae: 0.6041 - val_loss: 0.2604 - val_mae: 0.2604\n",
      "Epoch 1241/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5560 - mae: 0.5560 - val_loss: 0.3340 - val_mae: 0.3340\n",
      "Epoch 1242/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5399 - mae: 0.5399 - val_loss: 0.3933 - val_mae: 0.3933\n",
      "Epoch 1243/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5328 - mae: 0.5328 - val_loss: 0.6387 - val_mae: 0.6387\n",
      "Epoch 1244/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5564 - mae: 0.5564 - val_loss: 0.4026 - val_mae: 0.4026\n",
      "Epoch 1245/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5071 - mae: 0.5071 - val_loss: 0.6603 - val_mae: 0.6603\n",
      "Epoch 1246/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5988 - mae: 0.5988 - val_loss: 0.4800 - val_mae: 0.4800\n",
      "Epoch 1247/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4679 - mae: 0.4679 - val_loss: 0.2036 - val_mae: 0.2036\n",
      "Epoch 1248/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4736 - mae: 0.4736 - val_loss: 0.8034 - val_mae: 0.8034\n",
      "Epoch 1249/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.6616 - mae: 0.6616 - val_loss: 0.9687 - val_mae: 0.9687\n",
      "Epoch 1250/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5363 - mae: 0.5363 - val_loss: 0.4994 - val_mae: 0.4994\n",
      "Epoch 1251/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4638 - mae: 0.4638 - val_loss: 0.8370 - val_mae: 0.8370\n",
      "Epoch 1252/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5190 - mae: 0.5190 - val_loss: 0.5016 - val_mae: 0.5016\n",
      "Epoch 1253/1500\n",
      "592/592 [==============================] - 5s 8ms/step - loss: 0.5104 - mae: 0.5104 - val_loss: 0.4071 - val_mae: 0.4071\n",
      "Epoch 1254/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.4879 - mae: 0.4879 - val_loss: 0.7955 - val_mae: 0.7955\n",
      "Epoch 1255/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5647 - mae: 0.5647 - val_loss: 0.5631 - val_mae: 0.5631\n",
      "Epoch 1256/1500\n",
      "592/592 [==============================] - 5s 9ms/step - loss: 0.5308 - mae: 0.5308 - val_loss: 0.2287 - val_mae: 0.2287\n",
      "Epoch 1257/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4651 - mae: 0.4651 - val_loss: 0.6537 - val_mae: 0.6537\n",
      "Epoch 1258/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5092 - mae: 0.5092 - val_loss: 0.2441 - val_mae: 0.2441\n",
      "Epoch 1259/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5482 - mae: 0.5482 - val_loss: 0.4278 - val_mae: 0.4278\n",
      "Epoch 1260/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4163 - mae: 0.4163 - val_loss: 0.7490 - val_mae: 0.7490\n",
      "Epoch 1261/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5057 - mae: 0.5057 - val_loss: 0.3039 - val_mae: 0.3039\n",
      "Epoch 1262/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5549 - mae: 0.5549 - val_loss: 0.4928 - val_mae: 0.4928\n",
      "Epoch 1263/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4706 - mae: 0.4706 - val_loss: 0.5593 - val_mae: 0.5593\n",
      "Epoch 1264/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5445 - mae: 0.5445 - val_loss: 0.3010 - val_mae: 0.3010\n",
      "Epoch 1265/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4975 - mae: 0.4975 - val_loss: 0.2488 - val_mae: 0.2488\n",
      "Epoch 1266/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5325 - mae: 0.5325 - val_loss: 0.5717 - val_mae: 0.5717\n",
      "Epoch 1267/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4576 - mae: 0.4576 - val_loss: 0.3720 - val_mae: 0.3720\n",
      "Epoch 1268/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5086 - mae: 0.5086 - val_loss: 0.7309 - val_mae: 0.7309\n",
      "Epoch 1269/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4727 - mae: 0.4727 - val_loss: 0.5898 - val_mae: 0.5898\n",
      "Epoch 1270/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5294 - mae: 0.5294 - val_loss: 0.2428 - val_mae: 0.2428\n",
      "Epoch 1271/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4978 - mae: 0.4978 - val_loss: 0.3675 - val_mae: 0.3675\n",
      "Epoch 1272/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5972 - mae: 0.5972 - val_loss: 0.5115 - val_mae: 0.5115\n",
      "Epoch 1273/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4805 - mae: 0.4805 - val_loss: 0.3168 - val_mae: 0.3168\n",
      "Epoch 1274/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4963 - mae: 0.4963 - val_loss: 0.5509 - val_mae: 0.5509\n",
      "Epoch 1275/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4852 - mae: 0.4852 - val_loss: 0.7053 - val_mae: 0.7053\n",
      "Epoch 1276/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5012 - mae: 0.5012 - val_loss: 0.2951 - val_mae: 0.2951\n",
      "Epoch 1277/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5298 - mae: 0.5298 - val_loss: 0.2705 - val_mae: 0.2705\n",
      "Epoch 1278/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4872 - mae: 0.4872 - val_loss: 0.5928 - val_mae: 0.5928\n",
      "Epoch 1279/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4816 - mae: 0.4816 - val_loss: 0.5308 - val_mae: 0.5308\n",
      "Epoch 1280/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4903 - mae: 0.4903 - val_loss: 0.5723 - val_mae: 0.5723\n",
      "Epoch 1281/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5088 - mae: 0.5088 - val_loss: 0.5430 - val_mae: 0.5430\n",
      "Epoch 1282/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4961 - mae: 0.4961 - val_loss: 1.4661 - val_mae: 1.4661\n",
      "Epoch 1283/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5438 - mae: 0.5438 - val_loss: 0.3536 - val_mae: 0.3536\n",
      "Epoch 1284/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4624 - mae: 0.4624 - val_loss: 0.5543 - val_mae: 0.5543\n",
      "Epoch 1285/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5089 - mae: 0.5089 - val_loss: 0.2178 - val_mae: 0.2178\n",
      "Epoch 1286/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4799 - mae: 0.4799 - val_loss: 0.2337 - val_mae: 0.2337\n",
      "Epoch 1287/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5185 - mae: 0.5185 - val_loss: 0.6713 - val_mae: 0.6713\n",
      "Epoch 1288/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4635 - mae: 0.4635 - val_loss: 0.2620 - val_mae: 0.2620\n",
      "Epoch 1289/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5131 - mae: 0.5131 - val_loss: 1.0276 - val_mae: 1.0276\n",
      "Epoch 1290/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5126 - mae: 0.5126 - val_loss: 0.4619 - val_mae: 0.4619\n",
      "Epoch 1291/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5180 - mae: 0.5180 - val_loss: 0.5135 - val_mae: 0.5135\n",
      "Epoch 1292/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5247 - mae: 0.5247 - val_loss: 0.5317 - val_mae: 0.5317\n",
      "Epoch 1293/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5091 - mae: 0.5091 - val_loss: 0.7301 - val_mae: 0.7301\n",
      "Epoch 1294/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5758 - mae: 0.5758 - val_loss: 0.3661 - val_mae: 0.3661\n",
      "Epoch 1295/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4487 - mae: 0.4487 - val_loss: 0.4512 - val_mae: 0.4512\n",
      "Epoch 1296/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5125 - mae: 0.5125 - val_loss: 0.3101 - val_mae: 0.3101\n",
      "Epoch 1297/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4602 - mae: 0.4602 - val_loss: 0.2880 - val_mae: 0.2880\n",
      "Epoch 1298/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4883 - mae: 0.4883 - val_loss: 0.6934 - val_mae: 0.6934\n",
      "Epoch 1299/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5136 - mae: 0.5136 - val_loss: 0.4885 - val_mae: 0.4885\n",
      "Epoch 1300/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6392 - mae: 0.6392 - val_loss: 0.5030 - val_mae: 0.5030\n",
      "Epoch 1301/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.6302 - mae: 0.6302 - val_loss: 0.2165 - val_mae: 0.2165\n",
      "Epoch 1302/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5136 - mae: 0.5136 - val_loss: 0.4852 - val_mae: 0.4852\n",
      "Epoch 1303/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5217 - mae: 0.5217 - val_loss: 0.1979 - val_mae: 0.1979\n",
      "Epoch 1304/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4930 - mae: 0.4930 - val_loss: 0.7941 - val_mae: 0.7941\n",
      "Epoch 1305/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4077 - mae: 0.4077 - val_loss: 0.9848 - val_mae: 0.9848\n",
      "Epoch 1306/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5024 - mae: 0.5024 - val_loss: 0.6935 - val_mae: 0.6935\n",
      "Epoch 1307/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5803 - mae: 0.5803 - val_loss: 0.6282 - val_mae: 0.6282\n",
      "Epoch 1308/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5096 - mae: 0.5096 - val_loss: 0.2169 - val_mae: 0.2169\n",
      "Epoch 1309/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4811 - mae: 0.4811 - val_loss: 0.3532 - val_mae: 0.3532\n",
      "Epoch 1310/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5112 - mae: 0.5112 - val_loss: 0.5132 - val_mae: 0.5132\n",
      "Epoch 1311/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5025 - mae: 0.5025 - val_loss: 0.3658 - val_mae: 0.3658\n",
      "Epoch 1312/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4621 - mae: 0.4621 - val_loss: 0.4323 - val_mae: 0.4323\n",
      "Epoch 1313/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4607 - mae: 0.4607 - val_loss: 0.1817 - val_mae: 0.1817\n",
      "Epoch 1314/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5069 - mae: 0.5069 - val_loss: 0.8418 - val_mae: 0.8418\n",
      "Epoch 1315/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4896 - mae: 0.4896 - val_loss: 0.3966 - val_mae: 0.3966\n",
      "Epoch 1316/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4724 - mae: 0.4724 - val_loss: 0.2151 - val_mae: 0.2151\n",
      "Epoch 1317/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5110 - mae: 0.5110 - val_loss: 0.5621 - val_mae: 0.5621\n",
      "Epoch 1318/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5194 - mae: 0.5194 - val_loss: 0.2881 - val_mae: 0.2881\n",
      "Epoch 1319/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5130 - mae: 0.5130 - val_loss: 0.3894 - val_mae: 0.3894\n",
      "Epoch 1320/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4661 - mae: 0.4661 - val_loss: 0.8298 - val_mae: 0.8298\n",
      "Epoch 1321/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4453 - mae: 0.4453 - val_loss: 0.3693 - val_mae: 0.3693\n",
      "Epoch 1322/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4681 - mae: 0.4681 - val_loss: 0.4435 - val_mae: 0.4435\n",
      "Epoch 1323/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4360 - mae: 0.4360 - val_loss: 0.3253 - val_mae: 0.3253\n",
      "Epoch 1324/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5559 - mae: 0.5559 - val_loss: 0.4757 - val_mae: 0.4757\n",
      "Epoch 1325/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4313 - mae: 0.4313 - val_loss: 0.6824 - val_mae: 0.6824\n",
      "Epoch 1326/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5168 - mae: 0.5168 - val_loss: 0.6570 - val_mae: 0.6570\n",
      "Epoch 1327/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5006 - mae: 0.5006 - val_loss: 0.5754 - val_mae: 0.5754\n",
      "Epoch 1328/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5019 - mae: 0.5019 - val_loss: 0.4550 - val_mae: 0.4550\n",
      "Epoch 1329/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4359 - mae: 0.4359 - val_loss: 0.9245 - val_mae: 0.9245\n",
      "Epoch 1330/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5844 - mae: 0.5844 - val_loss: 0.4973 - val_mae: 0.4973\n",
      "Epoch 1331/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4640 - mae: 0.4640 - val_loss: 0.3266 - val_mae: 0.3266\n",
      "Epoch 1332/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5344 - mae: 0.5344 - val_loss: 0.7966 - val_mae: 0.7966\n",
      "Epoch 1333/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4723 - mae: 0.4723 - val_loss: 0.6661 - val_mae: 0.6661\n",
      "Epoch 1334/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4509 - mae: 0.4509 - val_loss: 0.5744 - val_mae: 0.5744\n",
      "Epoch 1335/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4812 - mae: 0.4812 - val_loss: 0.3452 - val_mae: 0.3452\n",
      "Epoch 1336/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5191 - mae: 0.5191 - val_loss: 0.4083 - val_mae: 0.4083\n",
      "Epoch 1337/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4757 - mae: 0.4757 - val_loss: 0.5686 - val_mae: 0.5686\n",
      "Epoch 1338/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5313 - mae: 0.5313 - val_loss: 0.1958 - val_mae: 0.1958\n",
      "Epoch 1339/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4467 - mae: 0.4467 - val_loss: 0.4754 - val_mae: 0.4754\n",
      "Epoch 1340/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5160 - mae: 0.5160 - val_loss: 0.2719 - val_mae: 0.2719\n",
      "Epoch 1341/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5307 - mae: 0.5307 - val_loss: 0.4435 - val_mae: 0.4435\n",
      "Epoch 1342/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5019 - mae: 0.5019 - val_loss: 0.7332 - val_mae: 0.7332\n",
      "Epoch 1343/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4845 - mae: 0.4845 - val_loss: 0.4442 - val_mae: 0.4442\n",
      "Epoch 1344/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5080 - mae: 0.5080 - val_loss: 0.3543 - val_mae: 0.3543\n",
      "Epoch 1345/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4063 - mae: 0.4063 - val_loss: 0.2718 - val_mae: 0.2718\n",
      "Epoch 1346/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4815 - mae: 0.4815 - val_loss: 0.2868 - val_mae: 0.2868\n",
      "Epoch 1347/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5969 - mae: 0.5969 - val_loss: 0.4950 - val_mae: 0.4950\n",
      "Epoch 1348/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4751 - mae: 0.4751 - val_loss: 0.6717 - val_mae: 0.6717\n",
      "Epoch 1349/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5225 - mae: 0.5225 - val_loss: 0.4784 - val_mae: 0.4784\n",
      "Epoch 1350/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4540 - mae: 0.4540 - val_loss: 0.2455 - val_mae: 0.2455\n",
      "Epoch 1351/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5747 - mae: 0.5747 - val_loss: 1.0052 - val_mae: 1.0052\n",
      "Epoch 1352/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5215 - mae: 0.5215 - val_loss: 0.4638 - val_mae: 0.4638\n",
      "Epoch 1353/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5564 - mae: 0.5564 - val_loss: 0.3156 - val_mae: 0.3156\n",
      "Epoch 1354/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4417 - mae: 0.4417 - val_loss: 0.4010 - val_mae: 0.4010\n",
      "Epoch 1355/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4841 - mae: 0.4841 - val_loss: 0.8043 - val_mae: 0.8043\n",
      "Epoch 1356/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5420 - mae: 0.5420 - val_loss: 0.3189 - val_mae: 0.3189\n",
      "Epoch 1357/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5363 - mae: 0.5363 - val_loss: 0.6785 - val_mae: 0.6785\n",
      "Epoch 1358/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6060 - mae: 0.6060 - val_loss: 0.4480 - val_mae: 0.4480\n",
      "Epoch 1359/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5302 - mae: 0.5302 - val_loss: 0.6303 - val_mae: 0.6303\n",
      "Epoch 1360/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.4485 - mae: 0.4485 - val_loss: 1.3256 - val_mae: 1.3256\n",
      "Epoch 1361/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5735 - mae: 0.5735 - val_loss: 0.3756 - val_mae: 0.3756\n",
      "Epoch 1362/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5390 - mae: 0.5390 - val_loss: 0.3445 - val_mae: 0.3445\n",
      "Epoch 1363/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.5757 - mae: 0.5757 - val_loss: 0.5686 - val_mae: 0.5686\n",
      "Epoch 1364/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4960 - mae: 0.4960 - val_loss: 0.1980 - val_mae: 0.1980\n",
      "Epoch 1365/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4523 - mae: 0.4523 - val_loss: 0.9420 - val_mae: 0.9420\n",
      "Epoch 1366/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5026 - mae: 0.5026 - val_loss: 0.2607 - val_mae: 0.2607\n",
      "Epoch 1367/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5037 - mae: 0.5037 - val_loss: 0.5499 - val_mae: 0.5499\n",
      "Epoch 1368/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5036 - mae: 0.5036 - val_loss: 0.2094 - val_mae: 0.2094\n",
      "Epoch 1369/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4384 - mae: 0.4384 - val_loss: 0.3884 - val_mae: 0.3884\n",
      "Epoch 1370/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4515 - mae: 0.4515 - val_loss: 0.3524 - val_mae: 0.3524\n",
      "Epoch 1371/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4938 - mae: 0.4938 - val_loss: 0.2705 - val_mae: 0.2705\n",
      "Epoch 1372/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.4218 - mae: 0.4218 - val_loss: 0.5101 - val_mae: 0.5101\n",
      "Epoch 1373/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4847 - mae: 0.4847 - val_loss: 0.2438 - val_mae: 0.2438\n",
      "Epoch 1374/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4948 - mae: 0.4948 - val_loss: 0.5893 - val_mae: 0.5893\n",
      "Epoch 1375/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5247 - mae: 0.5247 - val_loss: 0.3203 - val_mae: 0.3203\n",
      "Epoch 1376/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4741 - mae: 0.4741 - val_loss: 0.2559 - val_mae: 0.2559\n",
      "Epoch 1377/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.5229 - mae: 0.5229 - val_loss: 0.2460 - val_mae: 0.2460\n",
      "Epoch 1378/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5058 - mae: 0.5058 - val_loss: 0.6550 - val_mae: 0.6550\n",
      "Epoch 1379/1500\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.6170 - mae: 0.6170 - val_loss: 0.2081 - val_mae: 0.2081\n",
      "Epoch 1380/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4747 - mae: 0.4747 - val_loss: 0.4611 - val_mae: 0.4611\n",
      "Epoch 1381/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.4707 - mae: 0.4707 - val_loss: 0.4441 - val_mae: 0.4441\n",
      "Epoch 1382/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4616 - mae: 0.4616 - val_loss: 0.2797 - val_mae: 0.2797\n",
      "Epoch 1383/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5163 - mae: 0.5163 - val_loss: 0.4365 - val_mae: 0.4365\n",
      "Epoch 1384/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5034 - mae: 0.5034 - val_loss: 0.8293 - val_mae: 0.8293\n",
      "Epoch 1385/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4790 - mae: 0.4790 - val_loss: 0.4073 - val_mae: 0.4073\n",
      "Epoch 1386/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4548 - mae: 0.4548 - val_loss: 0.2575 - val_mae: 0.2575\n",
      "Epoch 1387/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4498 - mae: 0.4498 - val_loss: 0.6164 - val_mae: 0.6164\n",
      "Epoch 1388/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4811 - mae: 0.4811 - val_loss: 0.2901 - val_mae: 0.2901\n",
      "Epoch 1389/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4793 - mae: 0.4793 - val_loss: 0.6493 - val_mae: 0.6493\n",
      "Epoch 1390/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5731 - mae: 0.5731 - val_loss: 0.6200 - val_mae: 0.6200\n",
      "Epoch 1391/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4401 - mae: 0.4401 - val_loss: 0.7644 - val_mae: 0.7644\n",
      "Epoch 1392/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4839 - mae: 0.4839 - val_loss: 0.3968 - val_mae: 0.3968\n",
      "Epoch 1393/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5160 - mae: 0.5160 - val_loss: 0.2901 - val_mae: 0.2901\n",
      "Epoch 1394/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4635 - mae: 0.4635 - val_loss: 1.4457 - val_mae: 1.4457\n",
      "Epoch 1395/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5055 - mae: 0.5055 - val_loss: 0.2142 - val_mae: 0.2142\n",
      "Epoch 1396/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4822 - mae: 0.4822 - val_loss: 0.3538 - val_mae: 0.3538\n",
      "Epoch 1397/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5818 - mae: 0.5818 - val_loss: 0.7257 - val_mae: 0.7257\n",
      "Epoch 1398/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5289 - mae: 0.5289 - val_loss: 0.4017 - val_mae: 0.4017\n",
      "Epoch 1399/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.4494 - mae: 0.4494 - val_loss: 0.3869 - val_mae: 0.3869\n",
      "Epoch 1400/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4851 - mae: 0.4851 - val_loss: 1.1277 - val_mae: 1.1277\n",
      "Epoch 1401/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5644 - mae: 0.5644 - val_loss: 0.3840 - val_mae: 0.3840\n",
      "Epoch 1402/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.6207 - mae: 0.6207 - val_loss: 0.2679 - val_mae: 0.2679\n",
      "Epoch 1403/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4783 - mae: 0.4783 - val_loss: 0.8026 - val_mae: 0.8026\n",
      "Epoch 1404/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.5949 - mae: 0.5949 - val_loss: 0.5358 - val_mae: 0.5358\n",
      "Epoch 1405/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4602 - mae: 0.4602 - val_loss: 0.7835 - val_mae: 0.7835\n",
      "Epoch 1406/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4235 - mae: 0.4235 - val_loss: 0.4820 - val_mae: 0.4820\n",
      "Epoch 1407/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4546 - mae: 0.4546 - val_loss: 0.3868 - val_mae: 0.3868\n",
      "Epoch 1408/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.3753 - mae: 0.3753 - val_loss: 0.3676 - val_mae: 0.3676\n",
      "Epoch 1409/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4757 - mae: 0.4757 - val_loss: 0.4727 - val_mae: 0.4727\n",
      "Epoch 1410/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5469 - mae: 0.5469 - val_loss: 0.9445 - val_mae: 0.9445\n",
      "Epoch 1411/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5114 - mae: 0.5114 - val_loss: 0.4428 - val_mae: 0.4428\n",
      "Epoch 1412/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.4697 - mae: 0.4697 - val_loss: 0.3948 - val_mae: 0.3948\n",
      "Epoch 1413/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5480 - mae: 0.5480 - val_loss: 0.4934 - val_mae: 0.4934\n",
      "Epoch 1414/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.4444 - mae: 0.4444 - val_loss: 0.3926 - val_mae: 0.3926\n",
      "Epoch 1415/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5307 - mae: 0.5307 - val_loss: 0.2879 - val_mae: 0.2879\n",
      "Epoch 1416/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.4772 - mae: 0.4772 - val_loss: 0.5898 - val_mae: 0.5898\n",
      "Epoch 1417/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.4147 - mae: 0.4147 - val_loss: 0.2728 - val_mae: 0.2728\n",
      "Epoch 1418/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.5651 - mae: 0.5651 - val_loss: 0.2271 - val_mae: 0.2271\n",
      "Epoch 1419/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.4669 - mae: 0.4669 - val_loss: 0.3184 - val_mae: 0.3184\n",
      "Epoch 1420/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.4719 - mae: 0.4719 - val_loss: 0.6283 - val_mae: 0.6283\n",
      "Epoch 1421/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4217 - mae: 0.4217 - val_loss: 0.4094 - val_mae: 0.4094\n",
      "Epoch 1422/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4842 - mae: 0.4842 - val_loss: 0.7687 - val_mae: 0.7687\n",
      "Epoch 1423/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4297 - mae: 0.4297 - val_loss: 0.2431 - val_mae: 0.2431\n",
      "Epoch 1424/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4913 - mae: 0.4913 - val_loss: 0.3270 - val_mae: 0.3270\n",
      "Epoch 1425/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4601 - mae: 0.4601 - val_loss: 0.2159 - val_mae: 0.2159\n",
      "Epoch 1426/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5617 - mae: 0.5617 - val_loss: 0.4042 - val_mae: 0.4042\n",
      "Epoch 1427/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5691 - mae: 0.5691 - val_loss: 1.0719 - val_mae: 1.0719\n",
      "Epoch 1428/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4649 - mae: 0.4649 - val_loss: 1.3881 - val_mae: 1.3881\n",
      "Epoch 1429/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5046 - mae: 0.5046 - val_loss: 0.4878 - val_mae: 0.4878\n",
      "Epoch 1430/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4314 - mae: 0.4314 - val_loss: 0.6349 - val_mae: 0.6349\n",
      "Epoch 1431/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5809 - mae: 0.5809 - val_loss: 0.2398 - val_mae: 0.2398\n",
      "Epoch 1432/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4680 - mae: 0.4680 - val_loss: 0.2254 - val_mae: 0.2254\n",
      "Epoch 1433/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4509 - mae: 0.4509 - val_loss: 0.3620 - val_mae: 0.3620\n",
      "Epoch 1434/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4405 - mae: 0.4405 - val_loss: 1.1260 - val_mae: 1.1260\n",
      "Epoch 1435/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4895 - mae: 0.4895 - val_loss: 0.3013 - val_mae: 0.3013\n",
      "Epoch 1436/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5091 - mae: 0.5091 - val_loss: 0.8297 - val_mae: 0.8297\n",
      "Epoch 1437/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4875 - mae: 0.4875 - val_loss: 0.4301 - val_mae: 0.4301\n",
      "Epoch 1438/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5492 - mae: 0.5492 - val_loss: 0.7563 - val_mae: 0.7563\n",
      "Epoch 1439/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4720 - mae: 0.4720 - val_loss: 0.4624 - val_mae: 0.4624\n",
      "Epoch 1440/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4060 - mae: 0.4060 - val_loss: 0.1717 - val_mae: 0.1717\n",
      "Epoch 1441/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.4071 - mae: 0.4071 - val_loss: 0.3089 - val_mae: 0.3089\n",
      "Epoch 1442/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5174 - mae: 0.5174 - val_loss: 0.5274 - val_mae: 0.5274\n",
      "Epoch 1443/1500\n",
      "592/592 [==============================] - 6s 11ms/step - loss: 0.5003 - mae: 0.5003 - val_loss: 0.5810 - val_mae: 0.5810\n",
      "Epoch 1444/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4678 - mae: 0.4678 - val_loss: 0.1959 - val_mae: 0.1959\n",
      "Epoch 1445/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4831 - mae: 0.4831 - val_loss: 0.6114 - val_mae: 0.6114\n",
      "Epoch 1446/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4923 - mae: 0.4923 - val_loss: 0.5874 - val_mae: 0.5874\n",
      "Epoch 1447/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4829 - mae: 0.4829 - val_loss: 0.6726 - val_mae: 0.6726\n",
      "Epoch 1448/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4808 - mae: 0.4808 - val_loss: 0.7938 - val_mae: 0.7938\n",
      "Epoch 1449/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5211 - mae: 0.5211 - val_loss: 0.6893 - val_mae: 0.6893\n",
      "Epoch 1450/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4643 - mae: 0.4643 - val_loss: 0.8706 - val_mae: 0.8706\n",
      "Epoch 1451/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4538 - mae: 0.4538 - val_loss: 0.4259 - val_mae: 0.4259\n",
      "Epoch 1452/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5016 - mae: 0.5016 - val_loss: 0.6150 - val_mae: 0.6150\n",
      "Epoch 1453/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4702 - mae: 0.4702 - val_loss: 0.3602 - val_mae: 0.3602\n",
      "Epoch 1454/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.4793 - mae: 0.4793 - val_loss: 0.4067 - val_mae: 0.4067\n",
      "Epoch 1455/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5047 - mae: 0.5047 - val_loss: 0.6605 - val_mae: 0.6605\n",
      "Epoch 1456/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4198 - mae: 0.4198 - val_loss: 0.4424 - val_mae: 0.4424\n",
      "Epoch 1457/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4640 - mae: 0.4640 - val_loss: 0.3297 - val_mae: 0.3297\n",
      "Epoch 1458/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.4245 - mae: 0.4245 - val_loss: 0.3564 - val_mae: 0.3564\n",
      "Epoch 1459/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4233 - mae: 0.4233 - val_loss: 0.3660 - val_mae: 0.3660\n",
      "Epoch 1460/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.4931 - mae: 0.4931 - val_loss: 0.4912 - val_mae: 0.4912\n",
      "Epoch 1461/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5207 - mae: 0.5207 - val_loss: 0.5831 - val_mae: 0.5831\n",
      "Epoch 1462/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.5787 - mae: 0.5787 - val_loss: 0.2756 - val_mae: 0.2756\n",
      "Epoch 1463/1500\n",
      "592/592 [==============================] - 7s 13ms/step - loss: 0.4836 - mae: 0.4836 - val_loss: 0.2342 - val_mae: 0.2342\n",
      "Epoch 1464/1500\n",
      "592/592 [==============================] - 8s 13ms/step - loss: 0.4631 - mae: 0.4631 - val_loss: 0.7981 - val_mae: 0.7981\n",
      "Epoch 1465/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5062 - mae: 0.5062 - val_loss: 0.8231 - val_mae: 0.8231\n",
      "Epoch 1466/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.4462 - mae: 0.4462 - val_loss: 0.1837 - val_mae: 0.1837\n",
      "Epoch 1467/1500\n",
      "592/592 [==============================] - 10s 16ms/step - loss: 0.5224 - mae: 0.5224 - val_loss: 0.2645 - val_mae: 0.2645\n",
      "Epoch 1468/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.4890 - mae: 0.4890 - val_loss: 0.4391 - val_mae: 0.4391\n",
      "Epoch 1469/1500\n",
      "592/592 [==============================] - 8s 14ms/step - loss: 0.5176 - mae: 0.5176 - val_loss: 0.1972 - val_mae: 0.1972\n",
      "Epoch 1470/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.4576 - mae: 0.4576 - val_loss: 0.5446 - val_mae: 0.5446\n",
      "Epoch 1471/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.4456 - mae: 0.4456 - val_loss: 0.4110 - val_mae: 0.4110\n",
      "Epoch 1472/1500\n",
      "592/592 [==============================] - 9s 14ms/step - loss: 0.4202 - mae: 0.4202 - val_loss: 0.6382 - val_mae: 0.6382\n",
      "Epoch 1473/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.4823 - mae: 0.4823 - val_loss: 0.5064 - val_mae: 0.5064\n",
      "Epoch 1474/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.4833 - mae: 0.4833 - val_loss: 0.9957 - val_mae: 0.9957\n",
      "Epoch 1475/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.4109 - mae: 0.4109 - val_loss: 0.4671 - val_mae: 0.4671\n",
      "Epoch 1476/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.4995 - mae: 0.4995 - val_loss: 0.4887 - val_mae: 0.4887\n",
      "Epoch 1477/1500\n",
      "592/592 [==============================] - 10s 16ms/step - loss: 0.4326 - mae: 0.4326 - val_loss: 0.2067 - val_mae: 0.2067\n",
      "Epoch 1478/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.4211 - mae: 0.4211 - val_loss: 0.6735 - val_mae: 0.6735\n",
      "Epoch 1479/1500\n",
      "592/592 [==============================] - 9s 16ms/step - loss: 0.5084 - mae: 0.5084 - val_loss: 0.3301 - val_mae: 0.3301\n",
      "Epoch 1480/1500\n",
      "592/592 [==============================] - 9s 15ms/step - loss: 0.4679 - mae: 0.4679 - val_loss: 0.5168 - val_mae: 0.5168\n",
      "Epoch 1481/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5879 - mae: 0.5879 - val_loss: 0.1787 - val_mae: 0.1787\n",
      "Epoch 1482/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4454 - mae: 0.4454 - val_loss: 0.1936 - val_mae: 0.1936\n",
      "Epoch 1483/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5151 - mae: 0.5151 - val_loss: 0.2916 - val_mae: 0.2916\n",
      "Epoch 1484/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5071 - mae: 0.5071 - val_loss: 0.1871 - val_mae: 0.1871\n",
      "Epoch 1485/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4670 - mae: 0.4670 - val_loss: 0.6554 - val_mae: 0.6554\n",
      "Epoch 1486/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4421 - mae: 0.4421 - val_loss: 0.5465 - val_mae: 0.5465\n",
      "Epoch 1487/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4595 - mae: 0.4595 - val_loss: 0.6741 - val_mae: 0.6741\n",
      "Epoch 1488/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.5107 - mae: 0.5107 - val_loss: 0.1507 - val_mae: 0.1507\n",
      "Epoch 1489/1500\n",
      "592/592 [==============================] - 7s 11ms/step - loss: 0.4487 - mae: 0.4487 - val_loss: 0.4627 - val_mae: 0.4627\n",
      "Epoch 1490/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4492 - mae: 0.4492 - val_loss: 0.5972 - val_mae: 0.5972\n",
      "Epoch 1491/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.5105 - mae: 0.5105 - val_loss: 0.4499 - val_mae: 0.4499\n",
      "Epoch 1492/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4994 - mae: 0.4994 - val_loss: 0.1628 - val_mae: 0.1628\n",
      "Epoch 1493/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4873 - mae: 0.4873 - val_loss: 0.7782 - val_mae: 0.7782\n",
      "Epoch 1494/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4032 - mae: 0.4032 - val_loss: 0.4037 - val_mae: 0.4037\n",
      "Epoch 1495/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4206 - mae: 0.4206 - val_loss: 0.4865 - val_mae: 0.4865\n",
      "Epoch 1496/1500\n",
      "592/592 [==============================] - 7s 12ms/step - loss: 0.4196 - mae: 0.4196 - val_loss: 0.3689 - val_mae: 0.3689\n",
      "Epoch 1497/1500\n",
      "592/592 [==============================] - 10s 16ms/step - loss: 0.5574 - mae: 0.5574 - val_loss: 0.4910 - val_mae: 0.4910\n",
      "Epoch 1498/1500\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 0.4564 - mae: 0.4564 - val_loss: 0.6303 - val_mae: 0.6303\n",
      "Epoch 1499/1500\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 0.4551 - mae: 0.4551 - val_loss: 0.7920 - val_mae: 0.7920\n",
      "Epoch 1500/1500\n",
      "592/592 [==============================] - 10s 17ms/step - loss: 0.5194 - mae: 0.5194 - val_loss: 0.2396 - val_mae: 0.2396\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=1500,validation_data=(X_test, Y_test),batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80561b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184/1184 [==============================] - 4s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23960150813403677"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(model.predict(X_test), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ab72d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\anaconda3\\Anaconda\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[285.56335]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(scaler.transform(np.array([[285.695,306.202,1338,-0.62,-1.32,1.48]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65819409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_y_z = np.zeros((8, 20, 8))\n",
    "\n",
    "x = np.array([-1.8775,\n",
    " -1.5925,\n",
    " -1.3075,\n",
    " -1.0225,\n",
    " -0.7375,\n",
    " -0.4525,\n",
    " -0.16758,\n",
    " 0.1175])\n",
    "\n",
    "y = np.array([-4.54375,\n",
    " -4.27125,\n",
    " -3.99875,\n",
    " -3.72625,\n",
    " -3.45375,\n",
    " -3.18125,\n",
    " -2.90875,\n",
    " -2.63625,\n",
    " -2.36375,\n",
    " -2.09125,\n",
    " -1.81875,\n",
    " -1.54625,\n",
    " -1.27375,\n",
    " -1.00125,\n",
    " -0.72875,\n",
    " -0.45625,\n",
    " -0.18375,\n",
    " 0.08875,\n",
    " 0.36125,\n",
    " 0.63375])\n",
    "\n",
    "\n",
    "z = np.array([-0.057875,\n",
    " 0.226375,\n",
    " 0.510625,\n",
    " 0.794875,\n",
    " 1.079125,\n",
    " 1.363375,\n",
    " 1.647625,\n",
    " 1.931875])\n",
    "\n",
    "def get_boxes(x, y, z, t1, t2, ts, y_val):\n",
    "  boxes = []\n",
    "  differences = abs(y - y_val)\n",
    "  midpoint_index = np.argmin(differences)\n",
    "  if y_val >= np.min(y) and y_val <= np.max(y):\n",
    "    for i in range(len(x)):\n",
    "      for k in range(len(z)):\n",
    "        boxes.append([t1, t2, ts, x[i], y[midpoint_index], z[k]])\n",
    "  return boxes\n",
    "\n",
    "all_boxes = get_boxes(x, y, z, 285.695, 306.202, 3000, -3.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c975f12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.877500e+00,\n",
       "        -3.181250e+00, -5.787500e-02],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.877500e+00,\n",
       "        -3.181250e+00,  2.263750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.877500e+00,\n",
       "        -3.181250e+00,  5.106250e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.877500e+00,\n",
       "        -3.181250e+00,  7.948750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.877500e+00,\n",
       "        -3.181250e+00,  1.079125e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.877500e+00,\n",
       "        -3.181250e+00,  1.363375e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.877500e+00,\n",
       "        -3.181250e+00,  1.647625e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.877500e+00,\n",
       "        -3.181250e+00,  1.931875e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.592500e+00,\n",
       "        -3.181250e+00, -5.787500e-02],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.592500e+00,\n",
       "        -3.181250e+00,  2.263750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.592500e+00,\n",
       "        -3.181250e+00,  5.106250e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.592500e+00,\n",
       "        -3.181250e+00,  7.948750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.592500e+00,\n",
       "        -3.181250e+00,  1.079125e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.592500e+00,\n",
       "        -3.181250e+00,  1.363375e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.592500e+00,\n",
       "        -3.181250e+00,  1.647625e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.592500e+00,\n",
       "        -3.181250e+00,  1.931875e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.307500e+00,\n",
       "        -3.181250e+00, -5.787500e-02],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.307500e+00,\n",
       "        -3.181250e+00,  2.263750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.307500e+00,\n",
       "        -3.181250e+00,  5.106250e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.307500e+00,\n",
       "        -3.181250e+00,  7.948750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.307500e+00,\n",
       "        -3.181250e+00,  1.079125e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.307500e+00,\n",
       "        -3.181250e+00,  1.363375e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.307500e+00,\n",
       "        -3.181250e+00,  1.647625e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.307500e+00,\n",
       "        -3.181250e+00,  1.931875e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.022500e+00,\n",
       "        -3.181250e+00, -5.787500e-02],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.022500e+00,\n",
       "        -3.181250e+00,  2.263750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.022500e+00,\n",
       "        -3.181250e+00,  5.106250e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.022500e+00,\n",
       "        -3.181250e+00,  7.948750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.022500e+00,\n",
       "        -3.181250e+00,  1.079125e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.022500e+00,\n",
       "        -3.181250e+00,  1.363375e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.022500e+00,\n",
       "        -3.181250e+00,  1.647625e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.022500e+00,\n",
       "        -3.181250e+00,  1.931875e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -7.375000e-01,\n",
       "        -3.181250e+00, -5.787500e-02],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -7.375000e-01,\n",
       "        -3.181250e+00,  2.263750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -7.375000e-01,\n",
       "        -3.181250e+00,  5.106250e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -7.375000e-01,\n",
       "        -3.181250e+00,  7.948750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -7.375000e-01,\n",
       "        -3.181250e+00,  1.079125e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -7.375000e-01,\n",
       "        -3.181250e+00,  1.363375e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -7.375000e-01,\n",
       "        -3.181250e+00,  1.647625e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -7.375000e-01,\n",
       "        -3.181250e+00,  1.931875e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -4.525000e-01,\n",
       "        -3.181250e+00, -5.787500e-02],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -4.525000e-01,\n",
       "        -3.181250e+00,  2.263750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -4.525000e-01,\n",
       "        -3.181250e+00,  5.106250e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -4.525000e-01,\n",
       "        -3.181250e+00,  7.948750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -4.525000e-01,\n",
       "        -3.181250e+00,  1.079125e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -4.525000e-01,\n",
       "        -3.181250e+00,  1.363375e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -4.525000e-01,\n",
       "        -3.181250e+00,  1.647625e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -4.525000e-01,\n",
       "        -3.181250e+00,  1.931875e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.675800e-01,\n",
       "        -3.181250e+00, -5.787500e-02],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.675800e-01,\n",
       "        -3.181250e+00,  2.263750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.675800e-01,\n",
       "        -3.181250e+00,  5.106250e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.675800e-01,\n",
       "        -3.181250e+00,  7.948750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.675800e-01,\n",
       "        -3.181250e+00,  1.079125e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.675800e-01,\n",
       "        -3.181250e+00,  1.363375e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.675800e-01,\n",
       "        -3.181250e+00,  1.647625e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03, -1.675800e-01,\n",
       "        -3.181250e+00,  1.931875e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03,  1.175000e-01,\n",
       "        -3.181250e+00, -5.787500e-02],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03,  1.175000e-01,\n",
       "        -3.181250e+00,  2.263750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03,  1.175000e-01,\n",
       "        -3.181250e+00,  5.106250e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03,  1.175000e-01,\n",
       "        -3.181250e+00,  7.948750e-01],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03,  1.175000e-01,\n",
       "        -3.181250e+00,  1.079125e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03,  1.175000e-01,\n",
       "        -3.181250e+00,  1.363375e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03,  1.175000e-01,\n",
       "        -3.181250e+00,  1.647625e+00],\n",
       "       [ 2.856950e+02,  3.062020e+02,  3.000000e+03,  1.175000e-01,\n",
       "        -3.181250e+00,  1.931875e+00]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b1fb514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\anaconda3\\Anaconda\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "all_boxes = np.array(all_boxes)\n",
    "all_boxes\n",
    "temps = model.predict(scaler.transform(all_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fae75e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[92.860016]\n",
      " [92.85439 ]\n",
      " [92.84517 ]\n",
      " [92.84992 ]\n",
      " [92.86351 ]\n",
      " [92.8766  ]\n",
      " [92.89335 ]\n",
      " [92.91392 ]\n",
      " [92.88779 ]\n",
      " [92.85675 ]\n",
      " [92.85103 ]\n",
      " [92.85716 ]\n",
      " [92.87163 ]\n",
      " [92.884186]\n",
      " [92.91481 ]\n",
      " [92.92722 ]\n",
      " [92.908966]\n",
      " [92.87243 ]\n",
      " [92.8552  ]\n",
      " [92.86091 ]\n",
      " [92.87186 ]\n",
      " [92.88968 ]\n",
      " [92.917175]\n",
      " [92.9361  ]\n",
      " [92.93341 ]\n",
      " [92.89351 ]\n",
      " [92.850716]\n",
      " [92.84984 ]\n",
      " [92.86623 ]\n",
      " [92.88165 ]\n",
      " [92.90445 ]\n",
      " [92.9252  ]\n",
      " [92.915535]\n",
      " [92.89297 ]\n",
      " [92.85361 ]\n",
      " [92.86089 ]\n",
      " [92.871635]\n",
      " [92.88574 ]\n",
      " [92.90425 ]\n",
      " [92.91252 ]\n",
      " [92.9155  ]\n",
      " [92.892334]\n",
      " [92.86986 ]\n",
      " [92.88089 ]\n",
      " [92.88688 ]\n",
      " [92.90519 ]\n",
      " [92.91143 ]\n",
      " [92.91504 ]\n",
      " [92.91655 ]\n",
      " [92.900314]\n",
      " [92.894684]\n",
      " [92.89948 ]\n",
      " [92.91683 ]\n",
      " [92.92866 ]\n",
      " [92.93037 ]\n",
      " [92.92695 ]\n",
      " [92.918724]\n",
      " [92.905334]\n",
      " [92.911514]\n",
      " [92.91372 ]\n",
      " [92.93232 ]\n",
      " [92.94151 ]\n",
      " [92.939705]\n",
      " [92.934105]]\n"
     ]
    }
   ],
   "source": [
    "def quality(t, temp, a0 = 100):\n",
    "  q = a0 * (1 - 0.041 * t * np.exp(-42.35/temp))\n",
    "  return q\n",
    "\n",
    "quality = quality(2, temps)\n",
    "print(quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39acb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = quality.reshape((8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c866693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dede73b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 12)\n",
    "newcolors = viridis(np.linspace(0, 1, 256))\n",
    "pink = np.array([248/256, 24/256, 148/256, 1])\n",
    "newcolors[:25, :] = pink\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "q1 = (quality - np.min(quality))/(np.max(quality) - np.min(quality))\n",
    "\n",
    "def plot_examples(cms, data):\n",
    "    \"\"\"\n",
    "    helper function to plot two colormaps\n",
    "    \"\"\"\n",
    "    np.random.seed(19680801)\n",
    "    data = data\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 3), constrained_layout=True)\n",
    "    for [ax, cmap] in zip(axs, cms):\n",
    "        psm = ax.pcolormesh(data, cmap=cmap, rasterized=True, vmin=0, vmax=1)\n",
    "        fig.colorbar(psm, ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "plot_examples([viridis, newcmp], q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5803d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "rel_model = keras.models.load_model('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12c446b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 161ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[459.66608]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_model.predict(scaler.transform(np.array([[285.695,306.202,1,-0.62,-1.32,1.48]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8663558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[459.64832,\n",
       " 461.45731,\n",
       " 463.62653,\n",
       " 466.04755,\n",
       " 468.97449,\n",
       " 472.07697,\n",
       " 475.33478,\n",
       " 478.60278,\n",
       " 482.01663,\n",
       " 485.47845,\n",
       " 489.09485,\n",
       " 492.85016,\n",
       " 496.76401,\n",
       " 501.07883,\n",
       " 505.42914,\n",
       " 510.53412,\n",
       " 515.75079,\n",
       " 521.03937,\n",
       " 526.43597,\n",
       " 532.1759,\n",
       " 537.9137,\n",
       " 543.48853,\n",
       " 549.35358,\n",
       " 555.22644,\n",
       " 561.13867,\n",
       " 567.0509,\n",
       " 572.96332,\n",
       " 578.87567,\n",
       " 585.12616,\n",
       " 591.58051,\n",
       " 598.29767,\n",
       " 605.05927,\n",
       " 611.8703,\n",
       " 619.08405,\n",
       " 626.34161,\n",
       " 633.60303,\n",
       " 640.86993,\n",
       " 648.13495,\n",
       " 655.45898,\n",
       " 662.9942,\n",
       " 670.5296,\n",
       " 678.06476,\n",
       " 685.64264,\n",
       " 693.29938,\n",
       " 700.9566,\n",
       " 708.64105,\n",
       " 716.36041,\n",
       " 724.10919,\n",
       " 731.86334,\n",
       " 739.63104,\n",
       " 747.39862,\n",
       " 755.1662,\n",
       " 763.13263,\n",
       " 771.09351,\n",
       " 779.05756,\n",
       " 787.05273,\n",
       " 795.04602,\n",
       " 803.03918,\n",
       " 811.08569,\n",
       " 819.1579,\n",
       " 827.22992,\n",
       " 835.30206,\n",
       " 843.37415,\n",
       " 851.44623,\n",
       " 859.51837,\n",
       " 867.59558,\n",
       " 875.68433,\n",
       " 883.80524,\n",
       " 891.92609,\n",
       " 900.0722,\n",
       " 908.24231,\n",
       " 916.41199,\n",
       " 924.58197,\n",
       " 932.75189,\n",
       " 940.92175,\n",
       " 949.09485,\n",
       " 957.37775,\n",
       " 965.73151,\n",
       " 974.0849,\n",
       " 982.43866,\n",
       " 990.79218,\n",
       " 999.14575,\n",
       " 1007.49939,\n",
       " 1015.85291,\n",
       " 1024.20654,\n",
       " 1032.56006,\n",
       " 1040.9137,\n",
       " 1049.26709,\n",
       " 1057.62073,\n",
       " 1065.97449,\n",
       " 1074.328,\n",
       " 1082.68164,\n",
       " 1091.03503,\n",
       " 1099.38855,\n",
       " 1107.74231,\n",
       " 1116.09583,\n",
       " 1124.44934,\n",
       " 1132.8031,\n",
       " 1141.15662,\n",
       " 1149.51013,\n",
       " 1157.86389]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar=[]\n",
    "graph=[]\n",
    "for i in range(101):\n",
    "    ar.append(i*100)\n",
    "for j in ar:\n",
    "    brr=scaler.transform(np.array([[285.695,306.202,j,-0.62,-1.32,1.48]]))\n",
    "    graph.append(round(float(rel_model.predict(brr)),5))\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2285402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32310e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3cc61ddd3867d9883461812607a57d39d3b07f08bb1e76bb7104713f5f40038f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
